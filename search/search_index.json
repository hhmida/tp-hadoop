{"config": {"lang": ["en", "fr"], "separator": "[\\s\\-\\.]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "index.html", "title": "Introduction", "text": ""}, {"location": "index.html#debuter_avec_lecosysteme_hadoop", "title": "D\u00e9buter avec l'\u00e9cosyst\u00e8me Hadoop", "text": ""}, {"location": "index.html#objectifs", "title": "Objectifs", "text": "<ul> <li>G\u00e9rer et v\u00e9rifier les services de Hadoop</li> <li>Manipuler le syst\u00e8me de fichiers HDFS</li> <li>Comprendre et cr\u00e9er des programmes MapReduce simples</li> <li>Cr\u00e9er des programmes MapReduce \u00e0 plusieurs phases</li> <li>Transformer et Interroger les donn\u00e9es avec Pig et Hive</li> <li>D\u00e9ployer un cluster Hadoop</li> </ul>"}, {"location": "index.html#ressources", "title": "Ressources", "text": "<p>Outils</p> <ul> <li>Oracle Virtual Box V6 ou  plus (ou VMware)</li> <li>Image de machine virtuelle  avec les outils pr\u00e9-install\u00e9s. C'est une image cr\u00e9\u00e9e par Pierre Nezric \u00e0 laquelle sont ajout\u00e9s jupyter et mrjob. Elle contient les outils suivants :<ul> <li>Hadoop 2.7.3</li> <li>Spark 2.1.1</li> <li>Pig 0.15.0</li> <li>Hive 1.2.1</li> <li>HBase 1.1.9</li> <li>Cassandra </li> <li>Elasticsearch et Kibana</li> <li>Zookeeper 3.4.6</li> </ul> </li> </ul> <p>Sources et r\u00e9f\u00e9rence</p> <ul> <li>Documentation Hadoop</li> <li>Big Data Analytics with Hadoop 3, Sridhar Alla, Packt Publishing, 2018.</li> </ul> <p></p>"}, {"location": "index.html#presentation_de_hadoop", "title": "Pr\u00e9sentation de Hadoop", "text": ""}, {"location": "index.html#role", "title": "R\u00f4le", "text": "<p>Hadoop est un framework pour le stockage et le traitement distribu\u00e9 de grands volumes de donn\u00e9es sur des clusters d'ordinateurs \u00e0 l'aide de mod\u00e8les de programmation simples. Il permet de passer de n\u0153uds uniques \u00e0 des milliers de machines, chacune offrant un calcul et un stockage locaux. Hadoop garantit une haute disponibilit\u00e9 en d\u00e9tectant et traitant les pannes au niveau de la couche application.</p> <p>Hadoop supporte la scalabilit\u00e9 horizontale et verticale. Il est la plateforme Big Data de r\u00e9f\u00e9rence.</p>"}, {"location": "index.html#historique_et_versions", "title": "Historique et versions", "text": "<p>Les principales nouveaut\u00e9s des versions majeures par rapport aux versions pr\u00e9c\u00e9dentes sont :</p> <p>Version 2 :</p> <ul> <li>YARN : la gestion des ressources du cluster.</li> </ul> <p>Version 3 :</p> <ul> <li>Erasure Coding : tol\u00e9rance aux fautes par bloc de parit\u00e9.</li> <li>Haute disponibilit\u00e9 am\u00e9lior\u00e9e avec plusieurs NameNode secondaires (en standby)</li> </ul> <p>Comment obtenir Hadoop ?</p> <p>Hadoop est projet Open Source avec la licence Apache V2. Mais il existe aussi des distributions commerciales offertes par des fournisseurs avec des outils suppl\u00e9mentaires pour former une plateforme de Big Data Analytics. Historiquement, les leaders sont Cloudera, Hortonworks et MapR. Mais ces derni\u00e8res ann\u00e9es, plusieurs acquisitions et fusions ont \u00e9t\u00e9 effectu\u00e9es.</p> <p>Les principales alternatives actuelles pour obtenir Hadoop sont :</p> <ol> <li>Apache Hadoop : C'est la version Open Source. Plus difficile \u00e0 g\u00e9rer pour composer un \u00e9cosyst\u00e8me complet malgr\u00e9 que la majorit\u00e9 des composants sont Open Source aussi (Spark, Flink, Hive, Hbase, ...).</li> <li>Distribution Hadoop : o\u00f9 une pile de composants sont pr\u00e9-install\u00e9 avec des outils de gestion et d'administration int\u00e9gr\u00e9s (Ambari, Clouder Manager, ...). Parmi ces distributions on trouve :<ol> <li>Cloudera : apr\u00e8s la fusion, en 2018, avec son concurrent direct Hortonworks il a h\u00e9rit\u00e9 de ces produits HDP (Hortonworks Data Platform)  et HDF (Hortonworks Data Flow) sous forme de machines virtuelles (VMware, VirtualBox et Docker) et CDP (Cloudera Data Platform) en Cloud.</li> <li>Hewlett Packard Enterprise : qui h\u00e9rite de la plateforme MapR Data Platform apr\u00e8s son acquisition en 2018. Elle est rebaptis\u00e9e sous le nom HPE Ezmeral Data Fabric V6.2 (voir comment l'installer ici). Elle est compatible avec Ubuntu, RedHat/CentOS et SUSE. Un version l\u00e9g\u00e8re pour le d\u00e9veloppement et test est aussi offerte sous la forme d'un container Docker (Development Environment for HPE Ezmeral Data Fabric).</li> <li>IBM Open Platform: IOP ou IBM BigInsights est la distribution d'IBM disponible en version de production ou de test (Quick Start Edition).</li> </ol> </li> <li>\u00c9quipement d\u00e9di\u00e9 : Mat\u00e9riel optimis\u00e9 pour Hadoop comme : Dell, EMC, Teradata Appliance for Hadoop, HP, Oracle, ...</li> <li>Hadoop sur le Cloud: comme PaaS (Platform as a Service) tel que Amazon EMR, Microsoft HDInsight, Google Cloud Platform, Qubole, IBM BigInsights, ...</li> </ol> Cloudera Data Platform <p>Pour tester une liste plus \u00e9tendue d'outils de l'\u00e9cosyst\u00e8me Hadoop, je vous recommance la distribution Cloudera Data Platform 3.0 ou 2.6. Il faut lui pr\u00e9voir plus de ressources RAM et disque.</p>"}, {"location": "index.html#composants_du_noyau_hadoop", "title": "Composants  du noyau Hadoop", "text": "<p>3 composants principaux sont au c\u0153ur de Hadoop :</p> <p></p>"}, {"location": "index.html#hdfs_hadoop_distributed_file_system", "title": "HDFS (Hadoop Distributed File System)", "text": "<p>C'est le syst\u00e8me de fichier primaire de Hadoop. Il permet le stockage de larges volumes de donn\u00e9es sur des unit\u00e9s de stockage assez basique et abordable. Les donn\u00e9es sont partitionn\u00e9es et r\u00e9pliqu\u00e9es pour garantir la fiabilit\u00e9 et un acc\u00e8s parall\u00e8le. Il op\u00e8re selon le mod\u00e8le ma\u00eetre-esclave form\u00e9 respectivement par les n\u0153uds NameNode et DataNode.</p>"}, {"location": "index.html#mapreduce", "title": "MapReduce", "text": "<p>C'est la couche de traitement dans Hadoop. Elle traite des volumes importants de donn\u00e9es structur\u00e9es et non structur\u00e9es stock\u00e9es dans HDFS. MapReduce traite \u00e9galement une \u00e9norme quantit\u00e9 de donn\u00e9es en parall\u00e8le. Pour ce faire, il divise le travail en un ensemble de t\u00e2ches ind\u00e9pendantes selon le principe divisier et conqu\u00e9rir. MapReduce fonctionne en divisant le traitement en phases: Map et Reduce.</p>"}, {"location": "index.html#yarn_yet_another_resource_negotiator", "title": "YARN (Yet Another Resource Negotiator)", "text": "<p>Il s'occupe de la gestion et la surveillance des travaux. YARN permet plusieurs moteurs de traitement de donn\u00e9es tels que le streaming en temps r\u00e9el, le traitement par lots, etc. Le Resource Manager est le composant au niveau de la machine ma\u00eetre. Il g\u00e8re les ressources et planifie les applications s'ex\u00e9cutant sur YARN. Il a deux composants: Scheduler &amp; Application Manager. Tandis que le Node Manager, au niveau du n\u0153ud, communique en permanence avec Resource Manager et assure l'ex\u00e9cution des t\u00e2ches.</p>"}, {"location": "cluster.html", "title": "Hadoop en mode cluster", "text": ""}, {"location": "cluster.html#hadoop_en_mode_cluster", "title": "Hadoop en mode cluster", "text": ""}, {"location": "cluster.html#virtualisation_avec_docker", "title": "Virtualisation avec Docker", "text": "<p>Utiliser plusieurs machines virtuelles requiert plus des ressources. Pour cela, nous allons utiliser des containers Docker.</p> <p>Le sc\u00e9nario consiste \u00e0 d\u00e9ployer 4 n\u0153uds hadoop \u00e0 partir d'une image Docker fournie ici. Cette image est similaire \u00e0 celle utilis\u00e9e avec VirtualBox : elle contient la verion 3.2.1 de hadoop, mrjob, python3 et jupyter.</p>"}, {"location": "cluster.html#deploiement_avec_docker", "title": "D\u00e9ploiement avec Docker", "text": "<p> Charger l'image docker</p> <p>Soit \u00e0 partir du fichier fourni :</p> <pre><code>docker load &lt; hadoop-3.2.1-docker.tar.gz\n</code></pre> <p>Ou \u00e0 partir du Docker Hub :</p> <pre><code>docker pull hhmida/hadoop:3.2.1\n</code></pre> <p> Cr\u00e9er un r\u00e9seau virtuel :</p> <pre><code>docker network create --driver bridge hadoopnet\n</code></pre> <p> Ex\u00e9cuter 4 instance de l'image avec les noms nodemaster node2, node3, et node4</p> <pre><code>docker run -d --network hadoopnet --name nodemaster -it -h nodemaster -p 8088:8088 -p 9870:9870 -p 9864:9864 -p 19888:19888 -p 8042:8042 -p 8888:8888 hadoop:3.2.1\ndocker run -dP --network hadoopnet --name node2 -it -h node2 hadoop:3.2.1\ndocker run -dP --network hadoopnet --name node3 -it -h node3 hadoop:3.2.1\ndocker run -dP --network hadoopnet --name node4 -it -h node4 hadoop:3.2.1\n</code></pre> Image t\u00e9l\u00e9charg\u00e9e depuis Docker Hub <p>Si vous avez t\u00e9l\u00e9charger l'image avec la commande <code>docker pull</code> alors remplacer <code>hadoop:3.2.1</code> par <code>hhmida/hadoop:3.2.1</code>.</p> <p> Formater le nodemaster :</p> <pre><code>docker exec -u hadoop -it nodemaster hadoop/bin/hdfs namenode -format\n</code></pre>"}, {"location": "cluster.html#demarrage_du_cluster", "title": "D\u00e9marrage du cluster", "text": "<p> D\u00e9marrer les containers</p> <pre><code>docker start nodemaster node2 node3 node4\n</code></pre> <p> D\u00e9marrer les services hadoop</p> <p>Ex\u00e9cuter ces 2 commandes pour d\u00e9marrer les services \u00e0 partir du nodemaster :</p> <pre><code>docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-dfs.sh\ndocker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-yarn.sh\n</code></pre> <p> cr\u00e9er le dossier racine sur HDFS</p> <pre><code>docker exec -u hadoop -it nodemaster /home/hadoop/hadoop/bin/hadoop fs -mkdir -p .\n</code></pre> <p> D\u00e9marrer jupyter</p> <pre><code>docker exec -u hadoop -d nodemaster jupyter notebook --ip=0.0.0.0 --port=8888 --notebook-dir='/home/hadoop' --NotebookApp.token='' --NotebookApp.password=''\n</code></pre>"}, {"location": "cluster.html#arret_des_nuds", "title": "Arr\u00eat des n\u0153uds", "text": "<pre><code>docker stop nodemaster node2 node3 node4\n</code></pre> Red\u00e9marrage du cluster <p>Pour red\u00e9marrer le cluster ex\u00e9cuter les \u00e9tapes de la section D\u00e9marrage du cluster.</p>"}, {"location": "cluster.html#verification_de_letat_du_cluster", "title": "V\u00e9rification de l'\u00e9tat du cluster", "text": "<p>Avec la commande <code>jps</code>:</p> <p>Au niveau du nodemaster</p> <p></p> <p>Au niveau de node2 et node3</p> <p></p> <p>Acc\u00e9der aux URLs suivantes pour v\u00e9rifier l'\u00e9tat et la configuration du cluster</p> <ul> <li>Cluster Hadoop : http://localhost:8088</li> <li>HDFS :  http://localhost:9870</li> <li>Jupyter notebook :  http://localhost:8888</li> </ul> <p>L'interface du Resource Manager montre les n&amp;339;uds :</p> <p></p> <p>L'interface HDFS montre les n\u0153uds DataNodes actifs :</p> <p></p> <p>Remarquer la colonne Last contact qui refl\u00e8te le dernier heartbeaz re\u00e7u (inf\u00e9rieur au timeout par d\u00e9faut : 3 Secondes).</p> <p>Maintenant, arr\u00eater le n\u0153ud node3 :</p> <pre><code>docker stop node4\n</code></pre> <p></p> <p>Apr\u00e8s 1000 Secondes le n\u0153ud est consid\u00e9r\u00e9 comme indisponible :</p> <p></p>"}, {"location": "cluster.html#virtualisation_avec_machines_virtuelles", "title": "Virtualisation avec machines virtuelles", "text": "<p>Cr\u00e9er un cluster avec la machine virtuelle est plus simple mais n\u00e9cessite plus de ressources. En effet, dans notre exemple et pour cr\u00e9er un cluster compos\u00e9 d'un ma\u00eetre et 2 workers, vous devez disposer de 8GO de RAM au minimum dont 6GO pour les machines virtuelles.</p>"}, {"location": "cluster.html#modifications_sur_la_machine_virtuelle_originale", "title": "Modifications sur la machine virtuelle originale", "text": "<p> Changer le nom de la machine dans le fichier <code>/etc/hostname</code> en <code>nodemaster</code></p> <pre><code>sudo echo \"nodemaster\" &gt; /etc/hostname\n</code></pre> <p> Modifier le fichier <code>/etc/hosts</code> :</p> <pre><code>sudo nano /etc/hosts\n</code></pre> <p>Puis \u00e9crire les lignes suivantes et enregistrer :</p> <pre><code>192.168.100.10 nodemaster\n192.168.100.20 node2\n192.168.100.30 mode3\n</code></pre> <p> Modifier les fichiers de configuration de Hadoop</p> <p>Modifier le fichier <code>/ur/local/hadoop/etc/hadoop/core-site.xml</code> ainsi :</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://nodemaster:9000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Changer la replication dans <code>/usr/local/hadoop/etc/hadoop/hdfs-site.xml</code></p> <pre><code>&lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Ajouter cette propri\u00e9t\u00e9 dans <code>/usr/local/hadoop/etc/hadoop/yarn-site.xml</code></p> <pre><code>&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;nodemaster&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Le fichier <code>/usr/local/hadoop/etc/hadoop/workers</code> contient les noms des machines du cluster, il sera utilis\u00e9 pour d\u00e9marrer les services depuis le n\u0153ud ma\u00eetre.</p> <pre><code>nodemaster\nnode2\nnode3\n</code></pre> Workers <p>nodemaster dans cette configuration est consid\u00e9r\u00e9 comme namenode et datanode \u00e0 la fois.</p>"}, {"location": "cluster.html#clonage_de_la_machine_virtuelle", "title": "Clonage de la machine virtuelle", "text": "<p> Cr\u00e9er 2 clones de la machine virtuelle originale qui vont \u00eatre respectivement node2 et node3.</p> <p> </p> <p> Chager leurs noms dans <code>/etc/hostname</code> respectivement en node2 et node3</p> <p>Sur la machine node2 :</p> <pre><code>sudo echo \"node2\" &gt; /etc/hostname\n</code></pre> <p>Sur la machine node3 :</p> <pre><code>sudo echo \"node3\" &gt; /etc/hostname\n</code></pre> <p> Changer les adresses IP en <code>192.168.100.20</code> et <code>192.168.100.30</code> en suivant les \u00e9tapes suivantes (refaire les m\u00eames \u00e9tapes pour node3) :</p> <pre><code>sudo nano /etc/netplan/00-installer-config.yaml\n</code></pre> <p></p> <p>Puis appliquer les modifications :</p> <pre><code>sudo netplan apply\n</code></pre> <p> Red\u00e9marrer les 3 machines et d\u00e9marrer les services depuis le nodemaster :</p> <pre><code>start-dfs.sh\nstart-yarn.sh\n</code></pre> <p>V\u00e9rifier avec <code>jps</code> ou les URLs http://192.168.100.10:9870 et http://192.168.100.10:8088</p> <p>Test de l'arr\u00eat d'un n\u0153ud : Voir la section pour l'exemple r\u00e9alis\u00e9 avec les containers Docker (ici).</p>"}, {"location": "cluster.html#test_de_map_reduce_sur_le_cluster", "title": "Test de Map Reduce sur le cluster", "text": "<p>Refaire l'exemple wordcout (voir ici):</p> <p>Mettre le fichier <code>shakespeare.txt</code> sur HDFS :</p> <pre><code>hadoop fs -put shakespeare.txt\n</code></pre> <p>Lancer Jupyter </p> <pre><code>jupyter notebook --ip=0.0.0.0 --port=8888 --notebook-dir='/home/hadoop' --NotebookApp.token='' --NotebookApp.password=''\n</code></pre> <p>Se connecter \u00e0 http://192.168.100.10:8888, cr\u00e9er le notebook et ajouter le code de l'exemple wordcount :</p> <p>Pendant l'ex\u00e9cution, vous pouvez visualiser l'\u00e9tat des ressources allou\u00e9es en cliquant sur <code>Scheduler</code> sur http://192.168.100.10:8088</p> <p>Apr\u00e8s l'ex\u00e9cution r\u00e9ussie sur le cluster, cliquer sur <code>Applications</code> pour v\u00e9rifier l'\u00e9tat de l'application.</p> <p>Pour afficher le d\u00e9tail des t\u00e2ches, il est possible de voir le Job History. D\u00e9marrer le serveur job History et acc\u00e9der \u00e0 son interface web.</p> <p>D\u00e9marrer le serveur</p> <pre><code>mapred jobhistoryserver start\n</code></pre> <p>Acc\u00e8s \u00e0 l'interface web : http://192.168.100.10:19888</p> <p></p> <p>En cliquant sur le Job ID, le nombre de t\u00e2ches Map et Reduce est affich\u00e9 (dans ce cas 2 Maps et 1 Reduce) </p> <p>Et puis chacune des t\u00e2ches :</p> <p>La premi\u00e8re t\u00e2che Map </p> <p>La seconde t\u00e2che Map </p> <p>La t\u00e2che Reduce </p>"}, {"location": "environnement.html", "title": "Pr\u00e9paration de l'environnement", "text": ""}, {"location": "environnement.html#preparation_de_lenvironnement", "title": "Pr\u00e9paration de l'environnement", "text": ""}, {"location": "environnement.html#installation_de_virtual_box", "title": "Installation de Virtual Box", "text": "<p> T\u00e9l\u00e9charger Oracle Virtual Box (ou VMware) selon votre syst\u00e8me (Windows, Linux ou Mac)</p> <p></p> <p> D\u00e9marrer l'installation de Virtual Box</p> <p>Suivre les \u00e9tapes ci-dessous</p> <p> </p> <p> </p> <p> </p> <p></p>"}, {"location": "environnement.html#creer_la_machine_virtuelle", "title": "Cr\u00e9er la machine virtuelle", "text": "<p> T\u00e9l\u00e9charger l'image de la machine virtuelle  </p> <p> En utilisant VirtualBox et \u00e0 partir du menu Fichier -&gt; Importer un appareil virtuel, s\u00e9lectionner le fichier <code>BDTools.ova</code> fourni et choisissez l'emplacement de destination.</p> <p></p> <p></p> <p> Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te. </p>"}, {"location": "environnement.html#demarrer_et_tester_la_machine_virtuelle", "title": "D\u00e9marrer et tester la machine virtuelle", "text": "<p> D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur <code>uti</code>. Le mot de passe <code>=uti=</code>.</p> <p></p> <p> Lancer le navigateur web, la page http://127.0.0.1 sera charg\u00e9e automatiquement o\u00f9 l'\u00e9tat des services est affich\u00e9 :</p> <p></p> <p>Par d\u00e9faut, les services actifs sont HDFS, YARN et HBase. La commande suivante permet de s\u00e9lectionner les services et activer/d\u00e9sactiver pour optimiser la gestion de la m\u00e9moire :</p> <pre><code>sudo SelectService hadoop | spark | cassandra | elasticsearch\n</code></pre>"}, {"location": "hdfs.html", "title": "Manipulation de HDFS", "text": ""}, {"location": "hdfs.html#manipulation_de_hdfs", "title": "Manipulation de HDFS", "text": ""}, {"location": "hdfs.html#service_hdfs", "title": "Service HDFS", "text": ""}, {"location": "hdfs.html#demarrerarreter", "title": "D\u00e9marrer/Arr\u00eater", "text": "Service HDFS <p>Pour g\u00e9rer les services, il est recommand\u00e9 d'utiliser le script SelectService. Mais pour les g\u00e9rer individuellement il est possible d'utiliser les commandes suivantes :</p> D\u00e9marrageArr\u00eat <pre><code>sudo service hadoop-hdfs-namenode start\nsudo service hadoop-hdfs-datanode start\n</code></pre> <pre><code>sudo service hadoop-hdfs-namenode stop\nsudo service hadoop-hdfs-datanode stop\n</code></pre> <p>La commande <code>sudo jps</code> permet de v\u00e9rifier que deux processus sont lanc\u00e9s : NameNode et DataNode. Avec la version 3, un processus SecondaryNameNode est aussi lanc\u00e9.</p> <p></p> <p>HDFS est maintenant accessible via l'interface web : http://localhost:50070</p> <p></p> <p>Cette interface permet d'afficher l'\u00e9tat de HDFS et ses diffrents datanodes. Il est possible d'explorer le contenu du syst\u00e8me de fichiers \u00e0 partir du menu <code>Utilities</code> puis <code>Browse the file system</code>.</p> <p></p>"}, {"location": "hdfs.html#formatage", "title": "Formatage", "text": "<pre><code>hdfs namenode -format\n</code></pre> Attention <p>L'op\u00e9ration de formatage supprime tous les fichiers. Elle est effectu\u00e9e lors de l'installation de Hadoop ou pour r\u00e9initialiser HDFS.</p>"}, {"location": "hdfs.html#commandes_hdfs", "title": "Commandes HDFS", "text": "<p>Le syst\u00e8me HDFS est manipul\u00e9 \u00e0 travers des commandes inspir\u00e9es du syst\u00e8me Linux. La forme g\u00e9n\u00e9rale de ces commandes est comme suit :</p> Format des commandes HDFS <pre><code>hadoop fs -nomCommande -options param1 ...\n</code></pre> <p>Ou encore :</p> <pre><code>hdfs dfs -nomCommande -options param1 ...\n</code></pre>"}, {"location": "hdfs.html#aide_sur_une_commande", "title": "Aide sur une commande", "text": "Aide sur les commandes HDFS <p>Afficher les commandes disponibles :</p> <p><pre><code>hadoop fs -help\n</code></pre> <pre><code>Usage: hadoop fs [generic options]\n    [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]\n    [-cat [-ignoreCrc] &lt;src&gt; ...]\n    [-checksum &lt;src&gt; ...]\n    [-chgrp [-R] GROUP PATH...]\n    [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]\n    [-chown [-R] [OWNER][:[GROUP]] PATH...]\n    [-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]\n    [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n    [-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]\n    [-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]\n    [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]\n    [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]\n    [-df [-h] [&lt;path&gt; ...]]\n    [-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]\n    [-expunge [-immediate]]\n    [-find &lt;path&gt; ... &lt;expression&gt; ...]\n    [-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n    [-getfacl [-R] &lt;path&gt;]\n    [-getfattr [-R] {-n name | -d} [-e en] &lt;path&gt;]\n    [-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]\n    [-head &lt;file&gt;]\n    [-help [cmd ...]]\n    [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]\n    [-mkdir [-p] &lt;path&gt; ...]\n    [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]\n    [-moveToLocal &lt;src&gt; &lt;localdst&gt;]\n    [-mv &lt;src&gt; ... &lt;dst&gt;]\n    [-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]\n    [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]\n    [-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]\n    [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]\n    [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]\n    [-setfattr {-n name [-v value] | -x name} &lt;path&gt;]\n    [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]\n    [-stat [format] &lt;path&gt; ...]\n    [-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]\n    [-test -[defswrz] &lt;path&gt;]\n    [-text [-ignoreCrc] &lt;src&gt; ...]\n    [-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]\n    [-touchz &lt;path&gt; ...]\n    [-truncate [-w] &lt;length&gt; &lt;path&gt; ...]\n    [-usage [cmd ...]]\n</code></pre></p> Aide sur une commande HDFS <p>Afficher l'aide sur une commande particuli\u00e8re (ici la commande ls) :</p> <p><pre><code>hadoop fs -help ls\n</code></pre> <pre><code>-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...] :\nList the contents that match the specified file pattern. If path is not specified, the contents of /user/&lt;currentUser&gt; will be listed. \nFor a directory a list of its direct children is returned (unless -d option is specified).\n\nDirectory entries are of the form:\npermissions - userId groupId sizeOfDirectory(in bytes) modificationDate(yyyy-MM-dd HH:mm) directoryName\n\nand file entries are of the form:\npermissions numberOfReplicas userId groupId sizeOfFile(in bytes) modificationDate(yyyy-MM-dd HH:mm) fileName\n\n-C  Display the paths of files and directories only.\n-d  Directories are listed as plain files.\n-h  Formats the sizes of files in a human-readable fashion  rather than a number of bytes.\n-q  Print ? instead of non-printable characters.\n-R  Recursively list the contents of directories.\n-t  Sort files by modification time (most recent first).\n-S  Sort files by size.\n-r  Reverse the order of the sort.\n-u  Use time of last access instead of modification for display and sorting.\n-e  Display the erasure coding policy of files and directories.\n</code></pre></p>"}, {"location": "hdfs.html#exemples", "title": "Exemples :", "text": ""}, {"location": "hdfs.html#affichage_du_contenu_dun_dossier", "title": "Affichage du contenu d'un dossier", "text": "<pre><code>hadoop fs -ls\n</code></pre> Dossier par d\u00e9faut <p>Pour les chemins relatifs HDFS utilise <code>/user/nom_utilisateur</code> comme racine ou <code>nom_utilisateur</code> est l'utilisateur connect\u00e9. Si le dossier <code>/user/&lt;nom utilisateur&gt;</code> n'est pas cr\u00e9\u00e9, cette commande provoque une erreur. Dans notre cas, c'est le dossier <code>/user/uti</code> qui doit \u00eatre cr\u00e9\u00e9. <pre><code>hadoop fs -mkdir -p .\n</code></pre> ou <pre><code>hadoop fs -mkdir -p /user/uti\n</code></pre></p>"}, {"location": "hdfs.html#creer_des_fichiers", "title": "Cr\u00e9er des fichiers", "text": "Cr\u00e9er un fichier test.txt <pre><code>hadoop fs -touchz test.txt\n</code></pre>"}, {"location": "hdfs.html#upload_de_fichiers_ou_dossiers", "title": "Upload de fichiers ou dossiers", "text": "Upload de fichier <p>Le fichier exemple.txt doit \u00eatre dans le dossier en cours. (Utiliser Geany ou autre \u00e9diteur pour le cr\u00e9er)</p> <p><pre><code>hadoop fs -put exemple.txt\n</code></pre> ou</p> <pre><code>hadoop fs -copyFromLocal exemple.txt\n</code></pre> <p>Pour choisir une taille de bloc diff\u00e9rente de celle par d\u00e9faut :</p> <pre><code>hadoop fs -D dfs.blocksize=268435456 -put exemple.txt exemple_256M.txt\n</code></pre>"}, {"location": "hdfs.html#supprimer_des_fichiers", "title": "Supprimer des fichiers", "text": "Supprimer le fichier test.txt <pre><code>hadoop fs -rm test.txt\n</code></pre>"}, {"location": "hdfs.html#creer_des_dossier", "title": "Cr\u00e9er des dossier", "text": "Cr\u00e9er un dossier data/csv <pre><code>hadoop fs -mkdir -p data/csv\n</code></pre>"}, {"location": "hdfs.html#supprimer_des_dossiers", "title": "Supprimer des dossiers", "text": "Supprimer le dossier csv <pre><code>hadoop fs -rmdir csv\n</code></pre>"}, {"location": "hdfs.html#copierdeplacer_des_fichiers_ou_des_dossiers", "title": "Copier/d\u00e9placer des fichiers ou des dossiers", "text": "Copier le fichier test.txt <pre><code>hadoop fs -cp test.txt data/copie.txt\n</code></pre> D\u00e9placer le fichier copie.txt <pre><code>hadoop fs -mv data/copie.txt copie2.txt\n</code></pre>"}, {"location": "hdfs.html#creer_un_fichiers_avec_plusieurs_blocs", "title": "Cr\u00e9er un fichiers avec plusieurs blocs", "text": "Cr\u00e9er un fichier de taille 100MO sur HDFS <pre><code>dd if=/dev/zero of=test.img bs=1024 count=0 seek=100M\nhdfs dfs -put test.img\n</code></pre> <p>Maintenant v\u00e9rifier sur l'interface Web les blocs. (7 blocs pour un bloc de taille 16M)</p>"}, {"location": "hdfs.html#autres_commandes", "title": "Autres Commandes", "text": ""}, {"location": "hdfs.html#ajouter_dans_un_fichier", "title": "Ajouter dans un fichier", "text": "<p>\u00e0 partir du STDIN <pre><code>hadoop fs -appendToFile - copie2.txt \n</code></pre> \u00e0 partir d'un fichier</p> <pre><code>echo \"nouvelle ligne\" &gt;new.txt\nhadoop fs -appendToFile new.txt copie2.txt\n</code></pre>"}, {"location": "hdfs.html#changer_les_permissions", "title": "Changer les permissions", "text": "<pre><code>hadoop fs -chmod 644 copie2.txt\n</code></pre>"}, {"location": "hdfs.html#changer_le_proprietaire", "title": "Changer le propri\u00e9taire", "text": "<pre><code>hadoop fs -chown root:root copie2.txt\n</code></pre>"}, {"location": "hdfs.html#recuperer_le_contenu_dun_dossier_dans_un_fichier", "title": "R\u00e9cup\u00e9rer le contenu d'un dossier dans un fichier", "text": "<pre><code>hadoop fs -getmerge -nl data resultat.txt\n</code></pre>"}, {"location": "hdfs.html#afficher_des_statistiques", "title": "Afficher des statistiques", "text": "<pre><code>hadoop fs -stat \"type:%F perm:%a %u:%g size:%b mtime:%y atime:%x name:%n block:%o replication:%r\" exemple.txt\n</code></pre>"}, {"location": "hdfs.html#changer_le_facteur_de_replication", "title": "Changer le facteur de replication", "text": "<pre><code>hadoop fs -setrep 5 exemple.txt\n</code></pre>"}, {"location": "hdfs.html#administration", "title": "Administration", "text": "<pre><code>    hdfs getconf -confKey\n\n    # Exemples\n    hdfs getconf -namenodes\n    hdfs getconf -secondaryNameNodes\n    hdfs getconf -confKey [key]\n\n    hdfs dfsadmin [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance] [-slownodes]]\n    hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]\n    hdfs dfsadmin [-printTopology]\n    hdfs dfsadmin [-listOpenFiles [-blockingDecommission] [-path &lt;path&gt;]]\n    hdfs dfsadmin [-allowSnapshot &lt;snapshotDir&gt;]\n\n    hdfs fsck &lt;path&gt;\n        [-list-corruptfileblocks |\n        [-move | -delete | -openforwrite]\n        [-files [-blocks [-locations | -racks | -replicaDetails | -upgradedomains]]]\n        [-includeSnapshots] [-showprogress]\n        [-storagepolicies] [-maintenance]\n        [-blockId &lt;blk_Id&gt;] [-replicate]\n\n    hdfs namenode [-backup] |\n        [-checkpoint] |\n        [-format [-clusterid cid ] [-force] [-nonInteractive] ] |\n        [-upgrade [-clusterid cid] [-renameReserved&lt;k-v pairs&gt;] ] |\n        [-upgradeOnly [-clusterid cid] [-renameReserved&lt;k-v pairs&gt;] ] |\n        [-rollback] |\n        [-rollingUpgrade &lt;rollback |started&gt; ] |\n        [-importCheckpoint] |\n        [-initializeSharedEdits] |\n        [-bootstrapStandby [-force] [-nonInteractive] [-skipSharedEditsCheck] ] |\n        [-recover [-force] ] |\n        [-metadataVersion ]\n\n    # Administration Erasure Coding\n    hdfs ec [generic options]\n        [-setPolicy -path &lt;path&gt; [-policy &lt;policyName&gt;] [-replicate]]\n        [-getPolicy -path &lt;path&gt;]\n        [-unsetPolicy -path &lt;path&gt;]\n        [-listPolicies]\n        [-addPolicies -policyFile &lt;file&gt;]\n        [-listCodecs]\n        [-enablePolicy -policy &lt;policyName&gt;]\n        [-disablePolicy -policy &lt;policyName&gt;]\n        [-removePolicy -policy &lt;policyName&gt;]\n        [-verifyClusterSetup -policy &lt;policyName&gt;...&lt;policyName&gt;]\n        [-help [cmd ...]]\n</code></pre>"}, {"location": "hdfs.html#exercices", "title": "Exercices", "text": ""}, {"location": "hdfs.html#exercice_1", "title": "Exercice 1", "text": "<ul> <li>Sur HDFS, cr\u00e9er l'arborescence tunisie/petrole.</li> <li>Chercher et t\u00e9l\u00e9charger, sur http://data.industrie.gov.tn, les donn\u00e9es sur la production p\u00e9troli\u00e8re mensuelle par champ dans le format CSV.</li> <li>Placer le fichier dans le dossier petrole cr\u00e9\u00e9 dans la premi\u00e8re \u00e9tape.</li> <li>\u00c0 partir de l'interface web :<ul> <li>Chercher la taille du bloc par d\u00e9faut.</li> <li>Copier sur HDFS un fichier plus grand que la taille du bloc et v\u00e9rifier le nombre de blocs de ce fichier.</li> </ul> </li> </ul>"}, {"location": "hdfs.html#exercice_2", "title": "Exercice 2", "text": "<ol> <li>Afficher les fichiers des sous-dossiers, avec une taille arrondie en Ko, Mo ou Go.</li> <li>Cr\u00e9er un dossier dans le r\u00e9pertoire racine du HDFS.</li> <li>Cr\u00e9er un fichier appel\u00e9 hello-hadoop.txt dans le compte Linux et contenant la phrase \u00ab Hello Hadoop \u00bb.</li> <li>Copier ce fichier sur HDFS.</li> <li>V\u00e9rifier le r\u00e9sultat.</li> <li>Afficher le contenu du fichier.</li> <li>Afficher le dernier Ko du fichier.</li> <li>Supprimer ce fichier de HDFS.</li> <li>Remettre \u00e0 nouveau ce fichier et v\u00e9rifier le r\u00e9sultat.</li> <li>Transf\u00e9rer le fichier hello-hadoop de HDFS vers le compte Linux en lui changeant son nom.</li> <li>Positionner le facteur de r\u00e9plication \u00e0 2 pour le fichier hello-hadoop.txt. </li> <li>V\u00e9rifier avec la commande stat.</li> <li>Changer les permissions sur le fichier hello-hadoop.txt \u00e0 777.</li> <li>Ajouter du texte \"HDFS est un syst\u00e8me de fichiers distribu\u00e9.\" au fichier hello-hadoop.txt sur HDFS.</li> </ol>"}, {"location": "hive.html", "title": "Hive", "text": ""}, {"location": "hive.html#hive", "title": "Hive", "text": ""}, {"location": "hive.html#premier_exemple_wordcount", "title": "Premier exemple : Wordcount", "text": "<p>Nous reprenons l'exemple de comptage de mots utilis\u00e9 dans l'atelier MapReduce pour le r\u00e9soudre avec Hive.</p> wordcount en HiveQL<pre><code>CREATE EXTERNAL TABLE lines (line STRING); -- (1) \nLOAD DATA INPATH '/user/uti/shakespeare.txt' OVERWRITE INTO TABLE lines;\nCREATE TABLE words AS SELECT explode(split(line, ' ')) AS word FROM lines;\nCREATE TABLE word_counts AS SELECT word, count(1) AS nb FROM words\nGROUP BY word\nORDER BY nb DESC;\n</code></pre> <ol> <li> EXTERNAL permet d'emp\u00eacher de supprimer le fichier <code>shakespeare.txt</code> original apr\u00e8s son chargement avec Hive.</li> </ol> <p>Suivre les \u00e9tapes suivantes pour ex\u00e9cuter le code HiveQL :</p> <ol> <li>Lancer la machine virtuelle et acc\u00e9der \u00e0 Shell Hive (Beehive) \u00e0 partir du terminal <pre><code>hive\n</code></pre></li> <li>Vous aurez alors l'invite hive :     <pre><code>hive&gt;\n</code></pre></li> <li>Maintenant taper les commandes du code fourni.</li> </ol> Services requis pour Hive <p>Le shell Hive a besoin des services suivants :</p> <ol> <li>MySQL </li> <li>hive-metastore</li> <li>hive-server2</li> </ol> <p>Utiliser la commande <code>service nom_service start</code> pour les d\u00e9marrer en cas de besoin.</p> <p>Les lignes du code permettent respectivement de :</p> <p> Cr\u00e9er une table externe <code>lines</code>avec une colonne unique <code>line</code>.</p> <p> Charger le contenu du fichier <code>shakespeare.txt</code> \u00e0 partir de HDFS dans la table pr\u00e9c\u00e9dente. V\u00e9rifier, apr\u00e8s cette commande, la cr\u00e9ation du fichier <code>/user/hive/warehouse/lines/shakespeare.txt</code>.</p> <p> Cr\u00e9er une deuxi\u00e8me table <code>words</code> pour stocker le r\u00e9sultat d'une requ\u00eate pour d\u00e9composer chaque ligne (line) du texte dans la table lines en mot avec les fonctions <code>explode</code>et <code>split</code>.</p> <p> Cr\u00e9er une table <code>word_counts</code> structur\u00e9e en 2 colonnes : word pour le mot et nb pour le nombre d'occurences obtenu par le r\u00e9sultat de la fonction count \u00e0 partir de la table <code>words</code>.</p> <p> Grouper par mot.</p> <p> Trier par nombre d'occurences d\u00e9croissant</p> <p>V\u00e9rifier la cr\u00e9ation de Jobs MapReduce pour les diff\u00e9rentes requ\u00eates SELECT sur http://localhost:8088</p> <p>Un dossier <code>word_counts</code> est cr\u00e9\u00e9 dans <code>/user/hive/warehouse</code> o\u00f9 des fichiers (1 dans note cas 000000_0) contiennent les nombres d'occurences associ\u00e9s aux mots du fichier.</p> <p>Afficher le r\u00e9sultat :</p> <pre><code>hadoop fs -cat /user/hive/warehouse/word_counts/000000_0 | more\n</code></pre>"}, {"location": "hive.html#hiveql", "title": "HiveQL", "text": "Pr\u00e9parer les donn\u00e9es <p>Dans cette section, nous allons utiliser les m\u00eames donn\u00e9es de la section Pig. Si les fichiers csv ne sont pas d\u00e9j\u00e0 t\u00e9l\u00e9charg\u00e9s alors proc\u00e9der aux \u00e9tapes suivantes :</p> <ol> <li>Cr\u00e9er un dossier demo : <code>mkdir ~/demo</code></li> <li>T\u00e9l\u00e9charger les fichiers csv dans /home/uti/demo :<ul> <li>employee.csv </li> <li>department.csv </li> <li>salary.csv </li> </ul> </li> <li>Envoyer les donn\u00e9es sur HDFS <pre><code>hadoop fs -put /home/uti/demo\n</code></pre></li> </ol>"}, {"location": "hive.html#ldd", "title": "LDD", "text": "<p> Cr\u00e9er une base de donn\u00e9es <code>demo</code></p> <pre><code>CREATE DATABASE demo;\n</code></pre> <p> S\u00e9lectionner une base de donn\u00e9es</p> <pre><code>USE demo;\n-- La base de donn\u00e9es par d\u00e9faut est DEFAULT\n</code></pre> <p> Cr\u00e9er les tables</p> <pre><code>create table employee\n(id int,\n fname string,\n lname string,\n department_id int\n )\n ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n\n\ncreate table department\n(id int,\n dept_name string\n )\n ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   \"separatorChar\" = \",\",\n   \"quoteChar\"     = \"\\\"\"\n)  ;\n\ncreate external table salary\n(\nsalary_id int,\nemployee_id int,\npayment double,\npayment_date date\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ','\nlocation '/user/uti/salary';\n</code></pre> Contraintes d'int\u00e9grit\u00e9 <p>Les contraintes primary key et foreign key ne sont support\u00e9es qu'\u00e0 partir de la version 2. </p> <p>Les contraintes unique, not null et check sont ajout\u00e9es depuis la version3.</p> <p> Ajouter une colonne \u00e0 une table</p> <pre><code>CREATE TABLE test(id int);\nALTER TABLE test ADD COLUMNS (name string, dob date);\n</code></pre> <p> Supprimer une table</p> <pre><code>DROP TABLE test;\n</code></pre> <p> SHOW/DESCRIBE</p> <p>Afficher les objets disponibles </p> <pre><code>Show Databases;\nShow Tables;\nShow Partitions &lt;table&gt;;\nShow TblProperties &lt;table&gt;;\nShow Create Table &lt;table&gt;;\nShow Indexes on &lt;table&gt;;\nShow Columns in &lt;table&gt;;\nShow Functions;\nShow transactions;\n</code></pre> <p>Afficher la strucuture ou les propri\u00e9t\u00e9s d'un objet</p> <pre><code>Describe database &lt;db_name&gt;;\nDescribe &lt;table&gt;;\nDescribe &lt;table.column&gt;;\n</code></pre>"}, {"location": "hive.html#lmd", "title": "LMD", "text": "<p> Charger des donn\u00e9es \u00e0 partir des fichiers CSV depuis HDFS</p> <pre><code>LOAD DATA INPATH '/user/uti/demo/employee.csv' INTO TABlE employee;\nLOAD DATA INPATH '/user/uti/demo/department.csv' INTO TABlE department;\nLOAD DATA INPATH '/user/uti/demo/salary.csv' INTO TABlE salary;\n</code></pre> Remarque <p>V\u00e9rifier sur HDFS l'emplacement, le type et le format des fichiers de donn\u00e9es relatifs \u00e0 ces tables.</p> <p> Insertion de donn\u00e9es</p> <pre><code>CREATE TABLE jours(num int, nom string);\nINSERT INTO jours VALUES(1, 'Lundi');\nINSERT INTO jours VALUES(2, 'Mardi');\nINSERT INTO jours VALUES(3, 'Mercredi');\n</code></pre> MapReduce <p>La commande d'insertion g\u00e9n\u00e8re un job MapReduce que vous pouvez consulter sur YARN.</p> <p> Mise \u00e0 jour et suppression</p> <p>Pour ex\u00e9cuter ces op\u00e9rations, quelques conditions doivent \u00eatre v\u00e9rifi\u00e9es :</p> <ol> <li>La table utilise le format ORC </li> <li>La table supporte le Bucketing</li> <li>Les transactions ACID sont activ\u00e9es sur Hive</li> </ol> <p>Essayer de mettre \u00e0 jour la table jours :</p> <p><pre><code>UPDATE jours set nom = upper(nom) WHERE num = 1;\n</code></pre> Une erreur est alors affich\u00e9e. Pour pouvoir ex\u00e9cuter ces requ\u00eates, nous commen\u00e7ons par activer les transactions sous Hive. Ceci est possible avec l'une des deux m\u00e9thodes :</p> <ol> <li>Modifier le fichier hive-site.xml en ajoutant les lignes suivante et puis red\u00e9marrer Hive :</li> </ol> <p><pre><code>&lt;property&gt;\n    &lt;name&gt;hive.support.concurrency&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.txn.manager&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.enforce.bucketing&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;\n    &lt;value&gt;nostrict&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.compactor.worker.threads&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> 2. \u00c0 partir du shell hive :</p> <pre><code>SET hive.support.concurrency=true;\nSET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n# Pour Hive 2.0\nSET hive.enforce.bucketing=true;\nSET hive.exec.dynamic.partition.mode=nostrict;\n# Pour hive metastore\nSET hive.compactor.initiator.on=true;\nSET hive.compactor.worker.threads=1;\n</code></pre> <p>Ensuite cr\u00e9er une table supportant les transactions :</p> <pre><code>CREATE TABLE employee_trans (\n id int,\n name string,\n age int,\n gender string)\n clustered by (gender) into 2 buckets\n STORED AS ORC\n TBLPROPERTIES ('transactional'='true');\n\n-- ins\u00e9rer des donn\u00e9es\nINSERT INTO employee_trans VALUES(1,'James',30,'M');\nINSERT INTO employee_trans VALUES(2,'Ann',40,'F');\nINSERT INTO employee_trans VALUES(3,'Jeff',41,'M');\nINSERT INTO employee_trans VALUES(4,'Jennifer',20,'F');\n\n-- Mise \u00e0 jour\nUPDATE employee_trans\nSET age=45\nWHERE id=3;\n\n-- Suppression\nDELETE FROM employee_trans\nWHERE id=4;\n\n-- V\u00e9rifier les donn\u00e9es\nSELECT * FROM employee_trans;\n</code></pre>"}, {"location": "hive.html#select", "title": "SELECT", "text": "<p>Format de la requ\u00eate :</p> <pre><code>SELECT [ALL | DISTINCT] select_expr, select_expr, ...\nFROM table_reference\n[WHERE where_condition]\n[GROUP BY col_list]\n[HAVING having_condition]\n[ORDER BY col_list]\n[LIMIT [offset,] rows]\n</code></pre> <p>Reprenons les m\u00eames exemples de la section Pig.</p> <p> Filtrage pour avoir les employ\u00e9s (sans doublons) ayant re\u00e7u un salaire compris entre 5000 et 7000</p> <pre><code>SELECT distinct employee_id\nFROM salary\nWHERE payment &gt;= 5000 AND payment &lt;= 7000;\n</code></pre> <p> Nombre d'employ\u00e9s par d\u00e9partement et afficher les 3 plus grands d\u00e9partements</p> <pre><code>SELECT department_id, count(id) AS nb_emp\nFROM employee\nGROUP BY department_id\nORDER BY nb_emp DESC\nLIMIT 3;\n</code></pre> <p> Jointure : trouver le nom du d\u00e9partement de chaque employ\u00e9 (afficher les 4 premiers employ\u00e9s)</p> <pre><code>SELECT fname, lname, dept_name\nFROM employee JOIN department ON (employee.department_id = department.id) limit 4;\n</code></pre> <p> Requ\u00eate imbriqu\u00e9e : Trouver le nombre moyen d'employ\u00e9s par d\u00e9partement</p> <pre><code>SELECT AVG(nb_emp)\nFROM (SELECT department_id, count(id) AS nb_emp\nFROM employee\nGROUP BY department_id\n) q1;\n</code></pre>"}, {"location": "hive.html#references_hiveql", "title": "R\u00e9f\u00e9rences HiveQL", "text": "<p>Pour une r\u00e9f\u00e9rence compl\u00e8te du langage Pi Latin, aller sur la page de documentation (ici)</p> <p>Voici aussi, un m\u00e9mo du langage :</p> <p>Ouvrir dans un nouvel onglet</p>"}, {"location": "hive.html#exercices", "title": "Exercices", "text": ""}, {"location": "hive.html#exercice_1", "title": "Exercice 1", "text": "<p>Reprendre l'exercice de la section Pig en utilisant Hive.</p>"}, {"location": "hive.html#exercice_2", "title": "Exercice 2", "text": "<p>Le fichier de donn\u00e9es utilis\u00e9 dans cet examen est CrimeData_200K.csv qui comporte 200 milles lignes de 5 colonnes. </p> <p>Un aper\u00e7u des 6 premi\u00e8re lignes est donn\u00e9 dans la figure ci-apr\u00e8s :</p> <pre><code>Dc_Dist,Dispatch_Date_Time,Text_General_Code,Lon,Lat\n25,2010-03-03 11:57:00,Fraud,-75.117665,40.019968\n15,2008-02-05 15:10:00,All Other Offenses,-75.083062,40.027315\n24,2015-03-17 22:17:00,Prostitution and Commercialized Vice,-75.113389,39.996459\n12,2010-07-11 23:19:00,All Other Offenses,-75.223246,39.937435\n2,2006-12-05 22:59:00,All Other Offenses,-75.102684,40.044051\n</code></pre> <p>Le fichier contient des informations sur des actes criminels d\u00e9crits par\u00a0:</p> Colonne Description Dc_Dist Num\u00e9ro de district Dispatch_Date_Time Date et heure de prise en charge de l\u2019incident Text_General_code Cat\u00e9gories du crime Lon Longitude Lat Latitude <p>Le d\u00e9partement de Police vous a recrut\u00e9 pour l\u2019aider \u00e0 pr\u00e9parer les donn\u00e9es pour cr\u00e9er des tableaux de bord.</p> <ol> <li>T\u00e9l\u00e9charger le fichier CSV depuis l\u2019URL\u00a0: https://tinyurl.com/crimesdataset puis cr\u00e9er une base de donn\u00e9es contenant une table avec Hive et charger les donn\u00e9es du fichier pr\u00e9c\u00e9dent dans cette m\u00eame table.</li> <li>Donner les requ\u00eates Hive pour r\u00e9pondre aux questions suivantes\u00a0:<ol> <li>Combien y-a-t-il de districts\u00a0?</li> <li>Donner le pourcentage de crimes par district.</li> <li>Quels sont les 3 mois avec le plus grand nombre d\u2019incidents enregistr\u00e9s. (La fonction month permet d\u2019extraire le mois de la date)</li> </ol> </li> </ol>"}, {"location": "mr.html", "title": "MapReduce avec Python", "text": ""}, {"location": "mr.html#mapreduce_avec_python", "title": "MapReduce avec Python", "text": ""}, {"location": "mr.html#demarragearret_du_service", "title": "D\u00e9marrage/Arr\u00eat du service", "text": "Service YARN D\u00e9marrageArr\u00eat <pre><code>sudo service hadoop-yarn-resourcemanager start\nsudo service hadoop-yarn-nodemanager start\n</code></pre> <pre><code>sudo service hadoop-yarn-resourcemanager stop\nsudo service hadoop-yarn-nodemanager stop\n</code></pre> <p>La commande <code>jps</code> permet de v\u00e9rifier que deux processus sont lanc\u00e9s : ResourceManager et NodeManager.</p> <p>L'interface du Resource Manager est accessible depuis : http://localhost:8088  Cette interface permet de v\u00e9rifier l'\u00e9tat des ressources RAM et CPU du cluster, les applications (d\u00e9marr\u00e9es, termin\u00e9es, en cours ...) et les noeuds du cluster.</p> <p></p>"}, {"location": "mr.html#verification_des_processus_serveur", "title": "V\u00e9rification des processus serveur", "text": "<p>Pour voir les diff\u00e9rents processus serveurs, ex\u00e9cuter la commande <code>jps</code>. Le r\u00e9sultat est :</p> <p></p> <p>Processus HDFS : NameNode, DataNode, JournalNode Processus YARN : ResourceManager, NodeManager, JobHistoryServer, ApplicationHistoryServer</p>"}, {"location": "mr.html#preparer_lenvironnement", "title": "Pr\u00e9parer l'environnement", "text": "<p>Pour les exemples et les exercices de cette section, vous avez besoin de Python, Jupyter et MRJOB.</p> <p> Installer les paquets et modules suivants :</p> <p>Cette \u00e9tape est \u00e0 ignorer si vous utilisez la machine virtuelle fournie.</p> <pre><code>$ sudo apt-get install python python3-pip\n$ sudo pip3 install mrjob\n$ sudo pip3 install jupyter\n</code></pre> <p> Lancer Jupyter Notebook</p> <pre><code>$ cd dossier_travail\n$ jupyter notebook --no-browser --ip=0.0.0.0 --NotebookApp.token=''\n</code></pre> <p>Pour acc\u00e9der depuis la machine h\u00f4te aller \u00e0 l'adresse : http://localhost:8888 </p> <p>Les exemples peuvent \u00eatre \u00e9crits avec un simple \u00e9diteur comme geany, nano, vi (ou encore notepad sous windows). Et les commandes peuvent \u00eatre lanc\u00e9es \u00e0 partir du shell.</p>"}, {"location": "mr.html#premier_exemple_word_count", "title": "Premier Exemple : Word Count", "text": "<p>L'exemple classique pour illustrer le fonctionnement de MapReduce est celui de compter le nombre d'occurences d'un mot dans un fichier texte (le fichier shakespeare.txt .)</p>"}, {"location": "mr.html#etapes_a_suivre", "title": "\u00c9tapes \u00e0 suivre", "text": "<p>Pour ex\u00e9cuter un job MapReduce, il faut g\u00e9n\u00e9ralement suivre les \u00e9tapes suivantes :</p> <ol> <li>Placer le fichier de donn\u00e9es ou dataset dans le syst\u00e8me HDFS via la commande <code>hadoop fs -put ...</code></li> <li>\u00c9crire le code source .py</li> <li>Soumettre le job MapReduce :<ol> <li>Par Hadoop Streaming : Avec la commande <code>hadoop jar hadoop-streaming.jar -input &lt;input_file&gt; -output &lt;resultat&gt; -mapper &lt;mapper.py&gt; -reducer &lt;reducer.py&gt;</code></li> <li>Par MRJob : Avec la commande <code>python &lt;programme.py&gt; -r hadoop &lt;input_file&gt;</code></li> </ol> </li> </ol>"}, {"location": "mr.html#mapper_et_reducer", "title": "Mapper et Reducer", "text": "<p>L'objectif est de d\u00e9terminer le nombre d'occurences de chaque mot du fichier. Chaque ligne est compos\u00e9e de plusieurs mots (Le s\u00e9parateur est ' ').</p> <p>La transformation \u00e0 effectuer par chaque Mapper est de d\u00e9composer chaque ligne re\u00e7ue en mots est lui associer la valeur 1.</p> <p>Apr\u00e8s la phase de tri et de redistribution (sort and shuffle), chaque Reducer va recevoir des mots et une liste de '1' pour chaque occurence. Pour calculer le nombre d'occurences total, il suffit de faire la somme de ces '1'.</p> <p></p>"}, {"location": "mr.html#programmation_avec_mrjob", "title": "Programmation avec MRJob", "text": "<p>MapReduce est \u00e9crit en Java mais accepte les autres langages de programmation via la technique de streaming via op\u00e9ration de lecture \u00e9criture sur les flux d'entr\u00e9e-sortie standard (print, read).</p> <p>La biblioth\u00e8que <code>mrjob</code> simplifie l'\u00e9criture de programmes MapReduce.</p> <p>Dans cet exemple, deux classes de la librairie mrjob sont utilis\u00e9es : MRJob et MRStep.</p> <ul> <li>Pour cr\u00e9er un Job MapReduce, il suffit de cr\u00e9er une classe qui h\u00e9rite de MRJob (ici la classe WordCount).</li> <li>Cr\u00e9er une fonction qui effectue la transformation du Mapper : la m\u00e9thode <code>mapper_get_words</code>. Cette m\u00e9thode re\u00e7oit 3 param\u00e8tres self comme toutes les m\u00e9thodes d'une classe, le deuxi\u00e8me est ignor\u00e9 pour ce mapper et le dernier c'est une ligne du fichier de donn\u00e9es. La fonction split d\u00e9compose la ligne en une liste de mots.</li> <li>Cr\u00e9er une fonction qui r\u00e9alise l'agr\u00e9gation du Reducer : la m\u00e9thode <code>reducer_count_words</code>. Elle re\u00e7oit le param\u00e8tre self, la cl\u00e9 utilis\u00e9e par le Mapper (donc un mot) et la liste des valeurs r\u00e9cupr\u00e9es des diff\u00e9rents mappers (les '1'). L'agr\u00e9gation effectu\u00e9e est une somme par la fonction sum.</li> <li>D\u00e9finier les \u00e9tapes ou les diff\u00e9rentes t\u00e2che \u00e0 effectuer dans une m\u00e9thode appel\u00e9e <code>steps</code> qui doit retouner une liste de MRStep. Pour chaque instance MRStep cr\u00e9\u00e9e, on sp\u00e9cifie le nom de la fonction mapper et reducer d\u00e9j\u00e0 d\u00e9finie. Dans cet exemple, il y a une seule phase MapReduce.  </li> <li>Enfin l'appel de la m\u00e9thode <code>run</code> pour d\u00e9clencher L'ex\u00e9cution du Job MapReduce.</li> </ul>"}, {"location": "mr.html#executer_votre_premier_job_mapreduce", "title": "Ex\u00e9cuter votre premier Job MapReduce", "text": "<p> D\u00e9marrer la machine virtuelle.</p> <p> T\u00e9l\u00e9charger (dans la machine virtuelle) le notebook wordcount.ipynb </p> <p> Depuis un terminal, lancer jupyter dans le dossier pour ex\u00e9cuter le notebook :</p> <pre><code>jupyter notebook\n</code></pre> <p>Dans ce qui suit le contenu du Notebook Jupyter avec le code \u00e0 tester.</p> Streaming jar <p>En cas d'erreur <code>No hadoop streaming jar</code>, donner le chemin vers ce jar avec la commande :</p> <p><code>! python wordcount.py -r hadoop --hadoop-streaming-jar C:\\hadoop-env\\hadoop-3.2.1\\share\\hadoop\\tools\\lib\\hadoop-streaming-3.2.1.jar hdfs:///user/uti/shakespeare.txt</code></p>"}, {"location": "mr.html#surveiller_les_jobs", "title": "Surveiller les jobs", "text": "<p>Pour afficher les jobs MapReduce de la session en cours, visiter la page http://localhost:8088</p> <p>Quand le programme MapReduce de l'exercice pr\u00e9c\u00e9dent est termin\u00e9, il est indiqu\u00e9 dans la capture suivante :</p> <p></p> <p>Pour afficher l'historique complet c'est \u00e0 partir d'ici : http://localhost:8081</p> <p>Il est aussi possible d'utiliser les commandes <code>yarn</code> : - Pour afficher toutes les applications :  <pre><code>yarn application -list -appStates ALL\n</code></pre> - Pour arr\u00eater une application :  <pre><code>yarn application -kill &lt;id&gt;\n</code></pre></p>"}, {"location": "mr.html#exercices", "title": "Exercices", "text": ""}, {"location": "mr.html#exercice_1", "title": "Exercice 1", "text": "<ol> <li>Transformer l'exemple WordCount afin d'obtenir le r\u00e9sultat tri\u00e9 dans l'ordre croissant du nombre d'occurrences. (Hint : exploiter la phase sort and shuffle qui trie la cl\u00e9 selon l'ordre alphab\u00e9tique).</li> <li>Transformer le programme pour inverser l'ordre de tri.</li> </ol>"}, {"location": "mr.html#exercice_2", "title": "Exercice 2", "text": "<p>Sur les donn\u00e9es de la production p\u00e9troli\u00e8re de la section pr\u00e9c\u00e9dente et en utilisant MapReduce :</p> <ul> <li>Calculer la production annuelle de chaque champ.</li> <li>Trouver le champ avec la plus grande production par mois.</li> </ul>"}, {"location": "mr.html#exercice_3", "title": "Exercice 3", "text": "<p>On souhaite afficher pour les utilisateurs d\u2019un r\u00e9seau social le nombre d\u2019amis en commun avec un autre utilisateur quand il visite la page de ce dernier. </p> <p>\u00c9crire un programme Map Reduce qui calcule le nombre d\u2019amis en communs pour chaque paire d\u2019utilisateurs sachant qu\u2019on dispose d\u2019un fichier contenant les identifiants des utilisateurs suivis des identifiants de leurs amis.</p> <p>Le format de chaque ligne de ce fichier est\u00a0:</p> <pre><code>Id_utilisateur\u00a0: id_ami1, id_ami2, \u2026.\nExemple\u00a0: (r\u00e9seau social contenant 5 utilisateurs)\n10:20,30,40\n20:10,30,40\n30:10,20,40,50\n40:10,20,30,50\n50:30,40\n</code></pre> <p> Id\u00e9e :</p> <p>L\u2019id\u00e9e consiste \u00e0\u00a0:</p> <ol> <li>G\u00e9n\u00e9rer les couples d\u2019amis \u00e0 partir de chaque ligne (dans l\u2019ordre croissant des cl\u00e9s couple 10-30 au lieu du couple 30-10 par exemple)\u00a0: La ligne \u00ab\u00a010:20,30,40\u00a0\u00bb g\u00e9n\u00e8re ainsi\u00a0: 10-20:20,30,40 10-30:20,30,40 10-40:20,30,40</li> <li>Regrouper les couples (m\u00eame couleur)\u00a0puis garder les \u00e9l\u00e9ments communs des 2 listes (remarquer que chaque couple appara\u00eet exactement 2 fois). 10-20:20,30,40 10-20:10,30,40 10-20:30,40 </li> </ol>"}, {"location": "mr.html#exercice_4", "title": "Exercice 4", "text": "<p>En s'inspirant de l'exemple MapReduce et source json (books.json ) ci-apr\u00e8s qui affiche les titres des livres ayant 700 pages, \u00e9crire un programme MapReduce qui calcule le nombre de livres pour chaque auteur.     <pre><code>from mrjob.step import MRStep\nfrom mrjob.job import MRJob\nfrom mrjob.protocol import JSONValueProtocol\n\nclass Exemple(MRJob):\n    INPUT_PROTOCOL = JSONValueProtocol\n    def steps(self):\n        return [MRStep(mapper=self.fmap, reducer=self.freduce)]\n\n    def fmap(self, _, book):\n        if book['pageCount']&gt;700:\n            yield None,(book['title'], book['pageCount'])\n    def freduce(self, _ ,v):\n        for l in v:\n            yield l[0], l[1]\nif __name__ == '__main__':\n    Exemple.run()\n</code></pre></p>"}, {"location": "pig.html", "title": "Pig", "text": ""}, {"location": "pig.html#pig", "title": "Pig", "text": ""}, {"location": "pig.html#premier_exemple_wordcount", "title": "Premier exemple : Wordcount", "text": "<p>Nous reprenons l'exemple de comptage de mots utilis\u00e9 dans l'atelier MapReduce pour le r\u00e9soudre avec le langage Pig Latin.</p> Structure d'un script Pig <p>Un script Pig comporte 3 phases :</p> <pre><code>1. Le chargement des donn\u00e9es avec LOAD\n2. Une s\u00e9rie de transformations\n3. L'affichage ou la sauvegarde des r\u00e9sultats\n</code></pre> <p> Lancer la machine virtuelle et acc\u00e9der \u00e0 Pig en mode interactif \u00e0 partir du terminal</p> <p><pre><code>pig -x mapreduce\n</code></pre> Vous serez alors redirig\u00e9 vers le shell grunt :</p> <pre><code>grunt&gt;\n</code></pre> <p> Charger le fichier <code>shakespeare.txt</code></p> <p><pre><code>lines = LOAD './shakespeare.txt' AS (line: chararray);\n</code></pre> Ce qui charge le contenu du fichier \u00e0 partir de HDFS sous la forme de bag de chararray dans la variable lines.</p> <p> D\u00e9composer chaque ligne en mots</p> <p><pre><code>words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n</code></pre> TOKENIZE : D\u00e9couper la ligne en bag de mots FLATTEN : Transformer le bag en des mots individuels</p> <p> Cr\u00e9er un groupe pour chaque mot</p> <pre><code>word_groups = GROUP words BY word;\n</code></pre> <p> Compter les \u00e9l\u00e9ments de chaque groupe</p> <pre><code>word_count = FOREACH word_groups GENERATE COUNT(words) AS count, group;\n</code></pre> <p> Trier par le nombre d'occurences</p> <p><pre><code>ordered_word_count = ORDER word_count BY count DESC;\n</code></pre>  Sauvegarder sur HDFS</p> <p><pre><code>STORE ordered_word_count INTO './word_count_result';\n</code></pre> Apr\u00e8s l'envoi de cette instruction, Pig cr\u00e9e un job MapReduce. V\u00e9rifier la progression sur http://localhost:8088</p> <p></p> <p>Un dossier <code>word_count_result</code> est cr\u00e9\u00e9 o\u00f9 des fichiers part-r-???? contiennent les nombres d'occurences associ\u00e9s aux mots du fichier.</p> <p> Afficher le r\u00e9sultat</p> <pre><code>hadoop fs -cat word_count_result/part-r-* | more\n</code></pre> <p>Vous pouvez aussi utiliser l'interface web de HDFS pour afficher le r\u00e9sultat dans le dossier 'word_count_result'</p>"}, {"location": "pig.html#execution_dun_script", "title": "Ex\u00e9cution d'un script", "text": "<p>Pour ex\u00e9cuter l'exemple pr\u00e9c\u00e9dent :</p> <p> Cr\u00e9er un fichier <code>wordcount.pig</code> avec un \u00e9diteur de texte (geany) avec le code suivant :</p> <p><pre><code>lines = LOAD './shakespeare.txt' AS (line: chararray);\nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\nword_groups = GROUP words BY word;\nword_count = FOREACH word_groups GENERATE COUNT(words) AS count, group;\nordered_word_count = ORDER word_count BY count DESC;\nSTORE ordered_word_count INTO './word_count_result';\n</code></pre>  Ex\u00e9cuter le script avec la commande suivante</p> <pre><code>pig -x mapreduce wordcount.pig\n</code></pre> Mode local <p>Avant de lancer un script pig, il est recommand\u00e9 de le tester en local. Ceci est possible avec la commande suivante : <pre><code>pig -x local wordcount.pig\n</code></pre></p> <p>Attention : dans ce mode d'ex\u00e9cution les chemins sont interpr\u00e9t\u00e9s dans le syst\u00e8me local et non HDFS.</p>"}, {"location": "pig.html#instructions_pig_par_lexemple", "title": "Instructions Pig par l'exemple", "text": "Pr\u00e9parer les donn\u00e9es <ol> <li>Cr\u00e9er un dossier demo : <code>mkdir ~/demo</code></li> <li>T\u00e9l\u00e9charger les fichiers csv dans /home/uti/demo :<ul> <li>employee.csv </li> <li>department.csv </li> <li>salary.csv </li> </ul> </li> <li>Envoyer les donn\u00e9es sur HDFS <pre><code>hadoop fs -put /home/uti/demo\n</code></pre></li> </ol> <p> Charger des donn\u00e9es sans sch\u00e9ma \u00e0 partir d'un fichier CSV avec ',' comme s\u00e9parateur. Puis limiter le nombre de tuples \u00e0 10</p> <pre><code>e1 = LOAD '/user/uti/demo/employee.csv' USING PigStorage (',');\ne1_sample = limit e1 10;\ndump e1_sample;\n</code></pre> <p> Charger avec un sch\u00e9ma</p> <pre><code>emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage (',') \nas (eid:chararray,fname:chararray,lname:chararray,department:chararray);\nemp_sample = limit emp 10;\ndump emp_sample;\n</code></pre> <p> Projection</p> <pre><code>emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage (',') \nas (eid:chararray,fname:chararray,lname:chararray,department:chararray);\nemp_proj = FOREACH emp GENERATE $1, lname;\nemp_sample = limit emp_proj 10;\ndump emp_sample;\n</code></pre> <p> Filtrage pour avoir les employ\u00e9s (sans doublons) ayant re\u00e7u un salaire compris entre 5000 et 7000</p> <pre><code>sal = LOAD '/user/uti/demo/salary.csv' USING PigStorage (',') as (salary_id:chararray,employ_id:chararray,payment:double,p_date:datetime);\nabove_5k_7k = DISTINCT (FILTER sal BY payment &gt;= 5000 AND payment &lt;= 7000);\nemp_sample = limit above_5k_7k 10;\ndump emp_sample;\n</code></pre> <p> Nombre d'employ\u00e9s par d\u00e9partement et afficher les 3 plus grands d\u00e9partements</p> <pre><code>emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage (',') as (eid:chararray,fname:chararray,lname:chararray,department:chararray);\nemp_group_dep = group emp by department;\n-- ou emp_group_dep = group emp by $3;\ngroup_count = FOREACH emp_group_dep GENERATE group AS dep, COUNT(emp.eid) AS nb_emp;\nnbemp_dep = ORDER group_count BY nb_emp DESC;\ntop_dep = limit nbemp_dep 3;\ndump top_dep;\n</code></pre> <p> Jointure : trouver le nom du d\u00e9partement de chaque employ\u00e9</p> <pre><code>emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage (',') as (emp_id:chararray,fname:chararray,lname:chararray,dept_id:chararray);\n\ndep = LOAD '/user/uti/demo/department.csv' USING PigStorage (',') as (dept_id:chararray,dept_name:chararray);\n\ndep_emp = LIMIT (FOREACH (JOIN emp by (dept_id), dep by (dept_id)) GENERATE fname, lname, dept_name) 3;\nDUMP dep_emp;\n</code></pre>"}, {"location": "pig.html#test_et_performances", "title": "Test et performances", "text": "<p>DESCRIBE</p> <p>Elle permet d'afficher le sch\u00e9ma d'une relation.</p> <p>Exemple</p> <p><pre><code>grunt&gt; lines = LOAD './shakespeare.txt' AS (line: chararray);\ngrunt&gt; words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\ngrunt&gt; word_groups = GROUP words BY word;\ngrunt&gt; word_count = FOREACH word_groups GENERATE COUNT(words) AS count, group;\ngrunt&gt; DESCRIBE lines;\nlines: {line: chararray}\ngrunt&gt; DECRIBE word_groups;\nword_groups: {group: chararray,words: {word: chararray}}\ngrunt&gt; DESCRIBEvword_count;\nword_count: {count: long,group: chararray}\n</code></pre> ILLUSTRATE</p> <p>Cette commande Pig permet de montrer les diff\u00e9rentes transformations effectu\u00e9es sur les donn\u00e9es.</p> <pre><code>ILLUSTRATE relation|-script &lt;nom_script&gt;\n</code></pre> <p>Exemple : (source http://pig.apache.org)</p> <p><pre><code>grunt&gt; cat visits.txt\nAmy     yahoo.com       19990421\nFred    harvard.edu     19991104\nAmy     cnn.com 20070218\nFrank   nba.com 20070305\nFred    berkeley.edu    20071204\nFred    stanford.edu    20071206\n\ngrunt&gt; cat visits.pig\nvisits = LOAD 'visits.txt' AS (user, url, timestamp);\nrecent_visits = FILTER visits BY timestamp &gt;= '20071201';\nhistorical_visits = FILTER visits BY timestamp &lt;= '20000101';\nDUMP recent_visits;\nDUMP historical_visits;\nSTORE recent_visits INTO 'recent';\nSTORE historical_visits INTO 'historical';\n\ngrunt&gt; exec visits.pig\n\n(Fred,berkeley.edu,20071204)\n(Fred,stanford.edu,20071206)\n\n(Amy,yahoo.com,19990421)\n(Fred,harvard.edu,19991104)\n\n\ngrunt&gt; illustrate -script visits.pig\n\n------------------------------------------------------------------------\n| visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n------------------------------------------------------------------------\n|            | Amy             | yahoo.com      | 19990421             |\n|            | Fred            | stanford.edu   | 20071206             |\n------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n| recent_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n-------------------------------------------------------------------------------\n|                   | Fred            | stanford.edu   | 20071206             |\n-------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------\n| Store : recent_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n---------------------------------------------------------------------------------------\n|                           | Fred            | stanford.edu   | 20071206             |\n---------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------\n| historical_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n-----------------------------------------------------------------------------------\n|                       | Amy             | yahoo.com      | 19990421             |\n-----------------------------------------------------------------------------------\n-------------------------------------------------------------------------------------------\n| Store : historical_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n-------------------------------------------------------------------------------------------\n|                               | Amy             | yahoo.com      | 19990421             |\n-------------------------------------------------------------------------------------------\n</code></pre> EXPLAIN</p> <p>Affiche les plans d'ex\u00e9cution logique, physique et les phases MapReduce.</p> <pre><code>EXPLAIN relation|-script &lt;nom_script&gt; [-out chemin] [-dot|-xml]\n</code></pre> <p>Exemple</p> <pre><code>grunt&gt; EXPLAIN -script wordcount.pig -out explain_wordcount.txt\n</code></pre> <p>Le fichier <code>explain_wordcount.txt</code> obtenu contient :</p> <pre><code>#-----------------------------------------------\n# New Logical Plan:\n#-----------------------------------------------\nordered_word_count: (Name: LOStore Schema: count#112:long,group#109:chararray)\n|\n|---ordered_word_count: (Name: LOSort Schema: count#112:long,group#109:chararray)\n    |   |\n    |   count:(Name: Project Type: long Uid: 112 Input: 0 Column: 0)\n    |\n    |---word_count: (Name: LOForEach Schema: count#112:long,group#109:chararray)\n        |   |\n        |   (Name: LOGenerate[false,false] Schema: count#112:long,group#109:chararray)ColumnPrune:OutputUids=[112, 109]ColumnPrune:InputUids=[109, 110]\n        |   |   |\n        |   |   (Name: UserFunc(org.apache.pig.builtin.COUNT) Type: long Uid: 112)\n        |   |   |\n        |   |   |---words:(Name: Project Type: bag Uid: 110 Input: 0 Column: (*))\n        |   |   |\n        |   |   group:(Name: Project Type: chararray Uid: 109 Input: 1 Column: (*))\n        |   |\n        |   |---words: (Name: LOInnerLoad[1] Schema: word#109:chararray)\n        |   |\n        |   |---(Name: LOInnerLoad[0] Schema: group#109:chararray)\n        |\n        |---word_groups: (Name: LOCogroup Schema: group#109:chararray,words#110:bag{#114:tuple(word#109:chararray)})\n            |   |\n            |   word:(Name: Project Type: chararray Uid: 109 Input: 0 Column: 0)\n            |\n            |---words: (Name: LOForEach Schema: word#118:chararray)\n                |   |\n                |   (Name: LOGenerate[true] Schema: word#118:chararray)\n                |   |   |\n                |   |   (Name: UserFunc(org.apache.pig.builtin.TOKENIZE) Type: bag Uid: 116)\n                |   |   |\n                |   |   |---(Name: Cast Type: chararray Uid: 98)\n                |   |       |\n                |   |       |---line:(Name: Project Type: bytearray Uid: 98 Input: 0 Column: (*))\n                |   |\n                |   |---(Name: LOInnerLoad[0] Schema: line#98:bytearray)\n                |\n                |---lines: (Name: LOLoad Schema: line#98:bytearray)RequiredFields:null\n#-----------------------------------------------\n# Physical Plan:\n#-----------------------------------------------\nordered_word_count: Store(hdfs://master.cluster.virt:8020/user/uti/word_count_result:org.apache.pig.builtin.PigStorage) - scope-18\n|\n|---ordered_word_count: POSort[bag]() - scope-17\n    |   |\n    |   Project[long][0] - scope-16\n    |\n    |---word_count: New For Each(false,false)[bag] - scope-15\n        |   |\n        |   POUserFunc(org.apache.pig.builtin.COUNT)[long] - scope-11\n        |   |\n        |   |---Project[bag][1] - scope-10\n        |   |\n        |   Project[chararray][0] - scope-13\n        |\n        |---word_groups: Package(Packager)[tuple]{chararray} - scope-7\n            |\n            |---word_groups: Global Rearrange[tuple] - scope-6\n                |\n                |---word_groups: Local Rearrange[tuple]{chararray}(false) - scope-8\n                    |   |\n                    |   Project[chararray][0] - scope-9\n                    |\n                    |---words: New For Each(true)[bag] - scope-5\n                        |   |\n                        |   POUserFunc(org.apache.pig.builtin.TOKENIZE)[bag] - scope-3\n                        |   |\n                        |   |---Cast[chararray] - scope-2\n                        |       |\n                        |       |---Project[bytearray][0] - scope-1\n                        |\n                        |---lines: Load(hdfs://master.cluster.virt:8020/user/uti/shakespeare.txt:org.apache.pig.builtin.PigStorage) - scope-0\n\n#--------------------------------------------------\n# Map Reduce Plan                                  \n#--------------------------------------------------\nMapReduce node scope-19\nMap Plan\nword_groups: Local Rearrange[tuple]{chararray}(false) - scope-53\n|   |\n|   Project[chararray][0] - scope-55\n|\n|---word_count: New For Each(false,false)[bag] - scope-42\n    |   |\n    |   Project[chararray][0] - scope-43\n    |   |\n    |   POUserFunc(org.apache.pig.builtin.COUNT$Initial)[tuple] - scope-44\n    |   |\n    |   |---Project[bag][1] - scope-45\n    |\n    |---Pre Combiner Local Rearrange[tuple]{Unknown} - scope-56\n        |\n        |---words: New For Each(true)[bag] - scope-5\n            |   |\n            |   POUserFunc(org.apache.pig.builtin.TOKENIZE)[bag] - scope-3\n            |   |\n            |   |---Cast[chararray] - scope-2\n            |       |\n            |       |---Project[bytearray][0] - scope-1\n            |\n            |---lines: Load(hdfs://master.cluster.virt:8020/user/uti/shakespeare.txt:org.apache.pig.builtin.PigStorage) - scope-0--------\nCombine Plan\nword_groups: Local Rearrange[tuple]{chararray}(false) - scope-57\n|   |\n|   Project[chararray][0] - scope-59\n|\n|---word_count: New For Each(false,false)[bag] - scope-46\n    |   |\n    |   Project[chararray][0] - scope-47\n    |   |\n    |   POUserFunc(org.apache.pig.builtin.COUNT$Intermediate)[tuple] - scope-48\n    |   |\n    |   |---Project[bag][1] - scope-49\n    |\n    |---word_groups: Package(CombinerPackager)[tuple]{chararray} - scope-52--------\nReduce Plan\nStore(hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.io.InterStorage) - scope-20\n|\n|---word_count: New For Each(false,false)[bag] - scope-15\n    |   |\n    |   POUserFunc(org.apache.pig.builtin.COUNT$Final)[long] - scope-11\n    |   |\n    |   |---Project[bag][1] - scope-50\n    |   |\n    |   Project[chararray][0] - scope-13\n    |\n    |---word_groups: Package(CombinerPackager)[tuple]{chararray} - scope-7--------\nGlobal sort: false\n----------------\n\nMapReduce node scope-22\nMap Plan\nordered_word_count: Local Rearrange[tuple]{tuple}(false) - scope-26\n|   |\n|   Constant(all) - scope-25\n|\n|---New For Each(false)[tuple] - scope-24\n    |   |\n    |   Project[long][0] - scope-23\n    |\n    |---Load(hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.builtin.RandomSampleLoader('org.apache.pig.impl.io.InterStorage','100')) - scope-21--------\nReduce Plan\nStore(hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp569443691:org.apache.pig.impl.io.InterStorage) - scope-35\n|\n|---New For Each(false)[tuple] - scope-34\n    |   |\n    |   POUserFunc(org.apache.pig.impl.builtin.FindQuantiles)[tuple] - scope-33\n    |   |\n    |   |---Project[tuple][*] - scope-32\n    |\n    |---New For Each(false,false)[tuple] - scope-31\n        |   |\n        |   Constant(-1) - scope-30\n        |   |\n        |   Project[bag][1] - scope-28\n        |\n        |---Package(Packager)[tuple]{chararray} - scope-27--------\nGlobal sort: false\nSecondary sort: true\n----------------\n\nMapReduce node scope-37\nMap Plan\nordered_word_count: Local Rearrange[tuple]{long}(false) - scope-38\n|   |\n|   Project[long][0] - scope-16\n|\n|---Load(hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.io.InterStorage) - scope-36--------\nReduce Plan\nordered_word_count: Store(hdfs://master.cluster.virt:8020/user/uti/word_count_result:org.apache.pig.builtin.PigStorage) - scope-18\n|\n|---New For Each(true)[tuple] - scope-41\n    |   |\n    |   Project[bag][1] - scope-40\n    |\n    |---Package(LitePackager)[tuple]{long} - scope-39--------\nGlobal sort: true\nQuantile file: hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp569443691\n----------------\n</code></pre> <p>Pour avoir un graphe d'ex\u00e9cution :</p> <pre><code>grunt&gt; EXPLAIN -script wordcount.pig -out explain_wordcount.dot -dot\n</code></pre> <p>Puis convertir le fichier .dot en png :</p> <pre><code>apt install graphviz -y\ndot -Tpng -out explain_wordcount.png explain_wordcount.dot\n</code></pre> <p>Ce qui donne :</p> <p></p>"}, {"location": "pig.html#references_pig_latin", "title": "R\u00e9f\u00e9rences Pig Latin", "text": "<p>Pour une r\u00e9f\u00e9rence compl\u00e8te du langage Pig Latin, aller sur la page de documentation (ici)</p> <p>Voici aussi, un m\u00e9mo du langage :</p> <p>Ouvrir dans un nouvel onglet</p>"}, {"location": "pig.html#exercice", "title": "Exercice", "text": ""}, {"location": "pig.html#datasets", "title": "Datasets", "text": "<p>Ci-apr\u00e8s la description des donn\u00e9es sur les cr\u00e9dits offerts par une banque \u00e0 ses clients. (source : https://www.kaggle.com ) Le fichier credit_risk_dataset.csv  est structur\u00e9 ainsi :</p> <pre><code>person_age :                \u00e2ge du client\nperson_income :             revenu annuel du client\nperson_home_ownership :     propri\u00e9t\u00e9 de la maison : OWN, RENT, MORTGAGE\nperson_emp_length :         dur\u00e9e en mois du cr\u00e9dit\nloan_intent :               motif du cr\u00e9dit (MEDICAL, EDUCATION, PERSONAL, ...)\nloan_grade :                A, B, C, D\nloan_amnt :                 montant du cr\u00e9dit\nloan_int_rate :             taux d'int\u00e9r\u00eat\nloan_status :               \u00e9tat du cr\u00e9dit (1: non rembours\u00e9, 0: rembours\u00e9)\nloan_percent_income :       pourcentage du montant du cr\u00e9dit par rapport au revenu\ncb_person_default_on_file : Y/N\ncb_person_cred_hist_length :historique (nombre de cr\u00e9dits)\n</code></pre>"}, {"location": "pig.html#travail_a_faire", "title": "Travail \u00e0 faire", "text": "<p>R\u00e9pondre aux questions suivantes avec Pig.</p> <ol> <li>Calculer le montant total et la dur\u00e9e moyenne des cr\u00e9dits non rembours\u00e9s.</li> <li>Calculer le pourcentage des cr\u00e9dits rembours\u00e9s et non rembours\u00e9s.</li> <li>Quelle est la distribution des diff\u00e9rents \u00e9tats de propri\u00e9t\u00e9 de la maison.</li> <li>Quels sont les 3 motifs de cr\u00e9dits les plus demand\u00e9s.</li> <li>Calculer le pourcentage des cr\u00e9dits rembours\u00e9s et non rembours\u00e9s par motif puis par grade. </li> </ol>"}, {"location": "encryptcontent_plugin_1;rED1Owz6IPzKjzTUD+cwTA==;szoSSZbbIcG8lzAB1H1Chw==", "title": "cDlycB2XUCA01k1EWduSwg==;FmsJkShkUvSAvy+YhXdI8jkkls3vhHHNAZYiTqHnPd4=", "text": "WReyUR6lOxwWLBHtnHmlng==;T1LHgfLiGeOP/VSV1cx0lw=="}, {"location": "encryptcontent_plugin_1;uYodjqpVe+B82xqMO/bu+g==;6JovC5f322ADjYUgpqdnJ6ieg8bDeHI86xtgZnv4x5g=", "title": "5m1WAbzOUox0ZLhYQ370wQ==;HxfD/hYZCJnOXdnExzqN+w==", "text": "7i6R+oCqxBkZqCgBqfpbqA==;sY6rYEi2V7/Eg+YgMmy9WQ=="}, {"location": "encryptcontent_plugin_1;ehnacAuR/l7mzFOMjJ9omw==;IczKwFrav+j0yFAdzhEi5QRSSqxIvGgyrDINA2V5cuU=", "title": "hQgEcMmU4lX3vU8kMqWTRA==;B/YYTqOPMPkob4JpRxpZLA==", "text": "LDBcanDKjXClE7E4OjJAeg==;5wSPugf3P/uRhBNXllhSBTWrwwBIvqiIs/AKeVwW93rQ6nvXivm+kOqI63QJowuDYPpyvVTPGIP4ilzSQSBfUJraXnNu+v3q6XVm/c2j93WprmlrskedKAQ6G39apJ/sUbH1DnmdXeGylO7TuVvHBhPkC3/SsWL6ZqA/SnQvgk/45Pob6/7PJWbnGq/0ouhSnUIvCBdA0nnUXKYz+9WMlcPn2bD7YYnnWsYUg4mADlCA3e22piw3MAs7Gf0WYP1wLFFKnUx6RmKr+65Ur9NtuHWR62T8JnSDuajbpHlGVsqmMW3KH7IEfW6F5WGwtigbUxfbU2dKdDei1cR7Xa1LWimjUiPiDSxLi+njygLAfOAGmefZSIavxJDDLwzsvYJhaAUNn5tsmKddrBG7uhLdG0Cbt/0otqspU6ATOtjlWuhSC1eTUAnKD8JGXYYuE9UO"}, {"location": "encryptcontent_plugin_1;FGrUVHvZCIy33fuP005xwg==;5s8KFPk/e24MhrzFCe33QoAVRCRaVH0jEYxozP0vECE=", "title": "WZwLaPxctOiulWQThk9vJw==;MkROBohxj8tn2WM5YH0qfg==", "text": "uk1puHC8et464hf0OsZejA==;a4aq5BSUf3nGvbTFkULT+Q=="}, {"location": "encryptcontent_plugin_1;3FJBMBW4/9qgVvuJGZwCzw==;KJ2kVdnG7rB3lDCYm3RmCsYw8wVsZJjdGmjEeVLojsSNGuQTwfAAjB8GU1u8pHcY", "title": "7F5hjIRMdAuqo0LXgb6uyA==;1MwYFAbmpvar4fyd3Ti6mCuEtUQUU/LoCC8Ue9CQ6To=", "text": "hdfh6zVcKmQQZ+czQs6WjQ==;8f+J8Ya+Y2Bi789k3UQLWQ=="}, {"location": "encryptcontent_plugin_1;Y5a04swtENMwdSoYajJUsw==;AwsYfZ+qV9SI0M54ED/EAT55Y3DgwV8gXl8shxqGUlk=", "title": "Eh6EvH6sQ51YzdqPVfOP4Q==;07BtWvkLujvz7hX3uRugFw==", "text": "F8PdtEKaKK1+OaIh8wkFkw==;Hv97dq5bm+fA1CN9p30ICeQy+1T+qwx5450tWrQoTndWxvHfWgOXpSc6Ydilv0o6cdRRenfF3zG1+/fSax19HjvhwF71Wfuj88lebHQIjYyjZLznRL7f4uVQZEYP46uTf6nER7tv9yYqHd8HzVnOAqaWVM0hyvVcGE2IDy6tPY/uonXo0xg1joHDY4WNq/vcOTi2CfIDHbQgO8FGTYU1mml+BeS9oKPah5JUoj1RrVYelHd5vYHFaKWoFg2S/N28EIPLU4el3K8avJHMbzrCarz7H9wW4ilSdlKmY29msZDoC8p7PwMHClq50kWJb9LrcneSdS+suun73SRiTbcv253qAY96gMzMS4wdJTy8rSJDXiki8dUYgrJG8laem+17cJJcOuVDC+Wu3ANsYx9bbQZGSibsuffmncouL1j5qjPWmQmQ7VnqYzdrR703Tset7eYlfe/pnsMITTKa25TkE12b+Ycrp8xyucYBDTg9CW2ea76AIcMVa1AqxlEru/Xy3fCd7aSTHJJM5D/9SMYCVrtuW4b10meanZKL01YDZnZ/s75KLNaaSEKoGy65CuzK2e7+9RIOUs1QhnCJdX8iHZpLYX1MxYHytjsNW9/O+VMsc+07Mu62n0C8kgAbF2nifWkLH4DfA4Bz3kSaVWQ32ewvnIVXnrI1UPIYUr2ThU/hn5al5ygQY8SwSQaAvd6qOP2msDRCNEnXbz3Lcx5PJ3JaT4Hz5pFO8jvTeBNT6XhVuBGx5qrq5RyLDCZG28KH9Qx86Z6jeVHPXB/PvFbZKdpgnJPs3qz4gttvXUAPHHeCu6nixmS6HnTakBPuLUO5BhC8A6NSs2vxW067CET2qLZT2sL1k+278IKYZgoAECugSp9D7YI9YhS4nEZW0HLmLXo4CpjxIKd6y/0DBQNDBIEuvc5y5IbBdf7pNxpsLFSvwgQbtF438Zs37m1LCpnsONz0nfnHv0kv4grlJNCeMhURBWAdFPwOQhXU6M8EWicnD49L7m3M1jv/VF6Eq1ZtQE63jowSoKYOv3OvnDp9eQi1slIfFpAIPCf2PsHuxLxCrI35ETplRg3QwR2F0Nx0RoLj7l/saJXu+XQSPOxAOMIMAtZ1kpFpjgRUv3YQEPw="}, {"location": "encryptcontent_plugin_1;pMgV4z4LSElaDrRdyUqYNg==;CvbbugwnUM6MzAcr2KJVJekW+MAtr/9BPLMiLB1dKoI=", "title": "gfPKOyTdtBPAGHM3pRkdWw==;SfGgsotRsK8icEkKmHuUPQ==", "text": "D2xdQ1SFrh75OW4caJCvqg==;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"}, {"location": "encryptcontent_plugin_1;c63jH9GHCN/4Hx/pjnBFzQ==;rMhj9PCj24JWP+Dk4Pe0IkUcA2xNu0EDOA5WVoYSs04=", "title": "3fIJ09uJWbfg00/8lIsiNw==;vH5jGc1+DaIORkpadgMx4g==", "text": "CHdH4YEkEG4tAwcalWpvlg==;/l20cM/lHv8OYjCMIzR3L1DB8WRyEgwHP8eksiQSIpZKUj7pEktKMRB3pUm6w1lMvZT5BTdGbAWCmv25XZWUwgmCttMioe/rafJ2G8G7T3Wf4DP8juyHb+QltXDA5M6HznsVvwFueevBh1iI/dkKrRU7tecMWpuOzPkevGtfhUcNyB1NOMz5JLhb9NoiXAoXS6/ht+X8iaubneXA0LsatQrHqZn5Cm87HQ18vF0jfMneOEH818x1K7osGyoUDMqfTx8t066DD4E0+HijQlZBVzKJFHAwDE8mPn4bEc5zAPKlsjTIPhefGBFM/f7BBDCG66/Kd0Oq4zZfT0hE6+Ntx/m7F/bpTTS58RMcKcvRMr6gWWmtAEJIBGLAPDQxDRWlbwgpcRsnV44OK30kx1xT+J6UM88Ua0bbJUlh3a7tjGzR/2QXUhMP8Cpy/tFClRiSuEoaXtQjpVpWwHfzlZqbqHugMCS270Zmuf5XXiz0L/EIn185nbncHmb7EVRSgjOUOI/rRuwLIKZkxEmmFxGBR9/mPWkDQGoIyffqcjdYHPyeCe0E0rHVTbgxe6c3MD3mYCA8FN80HZ6jlrtQ1gva744QqTQ3dBDwhKrVG+JUSLZ0EU1056wZ6yvJ2CQoz9GPrYafnd8HhOqKlKaJwtNwCKNz8VhzjygrwVYAF8KI3p1FJ24Ac88gzfVaWBopWa/Igtzww7NUNq8kXkGCpG9WTr0IseDjyTr7JGJnfSZQF5IDMrxRx7IqRAH6CQ6ArZ3Wxn9KE02vtVDYV/SJlPz5wY71vRPKxVj3u5lQQGHDBw4I7sfiocTPMn9VUxzvo3DOv4r80zmHgpryPrmHS+06Kfw1BrZ2tytXEMqFtRtB4jJ0oU+UEkKLhK4FDkra0riZ1ZxqgeSNu95Hdgva/9ocV6fSmi66xoOxnNBgd0w7orpGpV5anFKpNJm3+5d01ypJyAGNzAtTzwnCSL+nNFqxByO3I/E1v+MqcHR3RYGciYUMU3yFV8GbZtkiKZFIC+X8jopUU3q5beHIaVsuLDGf/Rn+pNraU6WZuDg85WWJxLRjNwCndA6ugJyy6RAD6jdpCHQpW2n1yFqR+vzUs/fFmrVfcuPSyxSUIb3LdmUSs62q3ECBG+gThCqFtCNQXP/YglU27o+vdEvBntF+qwgRGtGd1OZi9LNnS869SsZvbs31Tp1+lcWe+xNzfjyZ0WbrNZlVt/nWNQ9cJcG3lbUCVN32065bOfhNJVO+AigZ0ACINBkadBfoyyvK8AJdOQD1uaf8ayAch1bhcJIgG2bwkRllA7uv68iuxz7XJ2PP935plYP+sfRqn2l+hOlqhlAv8CeEsIPGATWPSnTtMOFC2mp6eGTBwPK+r0D8yof2pKQ58g9SlrK9f+eIlBwySlF5M7k2SxfINYF2GUqyKVmTrnl+iFe1lY6+qQ8HwPifQfKeHKv++cAoWt2knzVOxkOsXo7J6cF9MZKf8Lj0zxMWmCYyt+21TuGZxj+J2sKOGU1vCRekl6SZxqN5PfPUsHTa"}, {"location": "encryptcontent_plugin_1;eTGA/1bIUXpjUNJ2VHO1Dg==;ly8Oky88zi/2+5mvs5HztB2//dedYhzoTYyx2vYF+7M=", "title": "G+dzwPiObLcoZwAXITRSrw==;79XPcH7dL3gh8COTvppcOg==", "text": "HOcMhchQDPnSW7/hlACTpA==;+stT7nSoSbu2sXShLcUJVIixWWyXXVM34RMnWKm6ZPXncqy3dESJFjPSt3158PiRypmvLsH2gE2vi+MQEvOX2YzZVSoEDQPn0l5KohpUykfeH6V60N3qxkz+Fl29NhofkC/qavQ+7tkUpkeLEedDpqU8mH0GbDHZ54rC0fSTz6jOs3RHP8URIAjGIeGY7uhNyg7XD5nXCHQ4SxhQOHSHp/h9I/GQDMBSiPz0J0p7AyK2Dz9sawg+zltLSIRSSTk96j7JHEeob45lESdMI9Hi5phQsGA8xsvvfRLuvRPQI+K0HscJd5wY6kySO9OKPxyjHO40NyF5xDYNOSIRSD+HWWCNDDR75Q5jdrRzYV8tVSJ4/eYODTtTyGsYx4f588nmx3URGieO3iqozCf1JiMr9Kf2OfdZB9q94BM0dZ1bYtfNqm8vKO5nbPcgkf6keLPu9XO1RKY/LsSZo7R3VrlneqgxwaL+U4jOXqkkEkqG9JwvaHpReimpH5drh6PO100a7fZl67rED7IXt4yLuIchb57f+6RjCKA2vVTEt4F3xPkaK8MEhdnaQljThB7wjGHtaDYouQD6bYt5z+RYAMMViYuPqZp3KdCUYbEWbxzB69RixQ3DIfXNR2nMxZIa3xHh7npWPdptt1r0TYrmtAbm2Oboywcom9Vyh5ho2yxqrR6dvONh/I+GridboLQIu2RpzxOgNTAmwPEa/yZsD0s+jNLk7tMNViJR7Hm6Fnx19GA7wlTjJW8rl+Xvw49md8B8ZG8Cu3KeXvOiB2NqYlmepwxYSBpVkrrQl5FQjrTOMrvAzYzKNQjyHdG0te3x7Oek971G/AfFjzU4sHgRgYCl/oF91Lh3MGm9/M3Tli4rgiRlHM4r+PagRqfh2CbRb9cisTVezx9rPUf/huF5m8GzKsEViCjvpJCPAEIMWGtuIjLFEuTTmjXRS4sjZXdimQWW"}, {"location": "encryptcontent_plugin_1;ofujureE2vE9IBvWjI1lgQ==;ei8B/aIGQCvNclSPgi4UF40AijyOfZhbUsYMkhKKBpA=", "title": "npmSJivgGJwTqBREn3cHCA==;1p7QI+uuz7Jw0jT7+B2L3w==", "text": "8Sie0FL1lbAkhiTS95YgIg==;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"}, {"location": "encryptcontent_plugin_1;fIixqDvRzKYMJpog9b4v4A==;7qq9E81tdZQbX/ugVLhqV5u4I0PA/P1BdEoA9gfMEwQ=", "title": "U+eLy6rqVyAdNYJxxVvfWA==;WzsgvztW6oKM2CPPGVOfrA==", "text": "8B9sHTje/BaBlHOX4sGsTw==;UMmVNf4ZpfsukXosWA0mJA=="}, {"location": "encryptcontent_plugin_1;j1k5Oo27Lan+BhaFOsytPA==;M5f5iFAP4M17N5gci9U2/WL24dT/85Zg9VNQ30RNA5o=", "title": "Ps7SDUBS6pu+u7guXlfaDA==;OHDesvrORbHZu4jz4vVC+A==", "text": "qPTsnHopsmUlNZYe40aqbw==;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"}, {"location": "encryptcontent_plugin_1;TNLxtP9l1YsJ9cVlUbN2ww==;EdnatGw0Iy6bY6aV8KxDiIwutM+Gooqvu179FE5+aCk=", "title": "KlbNRCzeOJXX+LJktYahcg==;oeq7BU68v87gA+2awGuBQA==", "text": "NQ1SYh1R30SUbcsZ5qm2HQ==;0pvXFFoZJBVzVfrTUfOFswIpzSqaf4WZJ7xZLfCvpykeZ4P/sO3jk/YI9F7W/LaxiDvZEX1ZUv8jIdkvabqnyKrU4VVm0rspvM7rB8evx1utUNjKd+Y1gDZLIIptrq2FrT8xYRm9lYRMiF2X7dXU9wPu8f672SrUP80Vi5GCtXyiFoY2bUHBXvEeNxUuVVHauRVjlGmUZ/SyjDej20FoOu+ZZYm1vty8ROx62jAAZcaqNvvWcGDJ194oqzBcRqqCOySa3rwxE0k+1ItFyR/+D8TNjgA5EgtZmL1FTyu1wyEFjX7LDEfxMlw5c3ZkCzEdw2N9/l1I0bst9ghVJ8OMa2RJBm2eR359G047emR4rHPq/wUn2KAvV56Z9U1KHJCm+iTOLWwIcNXUtgva5TV3qiJftfF9dqStCNI+hpj+KTPcMe5uJ71I3g7bYh/k30ZOpCLf0Nknepul4Wez0MH3gwi6IvQPSOeliaCvemZwEqY="}, {"location": "ucase.html", "title": "Cas Pratique : Data Pipeline avec Pig et Hive", "text": ""}, {"location": "ucase.html#introduction", "title": "Introduction", "text": "<p>Les donn\u00e9es Clickstream constitue l'ensembles des informations qu'un utilisateur laisse lors de la visite d'un site web.</p> <p>Ces donn\u00e9es sont captur\u00e9es g\u00e9n\u00e9ralement par le serveur web et enregistr\u00e9es dans des fichiers semi-structur\u00e9s appel\u00e9s logs.</p> <p>Ces logs contiennent des informations comme l'adresse IP, le navigateur web, la date, le syst\u00e8me d'exploitation, etc.</p> <p>Le travail consiste \u00e0 automatiser les t\u00e2che d'ingestion, de nettoyage, de transformation des donn\u00e9es pour g\u00e9n\u00e9rer des donn\u00e9es exploitables dans la phase d'analyse.</p>"}, {"location": "ucase.html#sources_de_donnees_et_pipeline", "title": "Sources de donn\u00e9es et Pipeline", "text": "<p>Le pipeline souhait\u00e9 est repr\u00e9sent\u00e9 par la figure suivante :</p> <p></p> <p>Les fichiers sources sont regroup\u00e9s dans cette archive  :</p> <ol> <li>products.tsv : Format TSV avec ent\u00eate et 2 colonnes (url, category)</li> <li>users.tsv : Format TSV avec ent\u00eate et 3 colonnes (SWID, BIRTH_DT, GENDER_CD)</li> <li>log.tsv : Format TSV sans ent\u00eate et 179 colonnes</li> </ol> load_files.pig <p>T\u00e9l\u00e9charger le script load_files.pig  qui permet de parser ces fichiers.</p>"}, {"location": "ucase.html#travail_demande", "title": "Travail demand\u00e9", "text": "<ol> <li>Ajouter les transformations n\u00e9cessaires avec Pig pour obtenir la structure suivante : (user_id, age, gender, country, state, city, logdate, ip, product_category, url)</li> <li>R\u00e9pondre aux questions (avec Hive) :<ul> <li>Le top 5 des produits visit\u00e9s par coordonn\u00e9es g\u00e9ographiques de l'utilisateur.</li> <li>Le nombre de sessions par coordonn\u00e9es g\u00e9ographiques.</li> <li>Le nombre de sessions par genre, produit et coordonn\u00e9es g\u00e9ographiques.</li> </ul> </li> </ol>"}]}