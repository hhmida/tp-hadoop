{"config":{"lang":["en","fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"index.html","text":"D\u00e9buter avec Hadoop et MapReduce \u00b6 Objectifs \u00b6 G\u00e9rer et v\u00e9rifier les services de Hadoop Manipuler le syst\u00e8me de fichiers HDFS Comprendre et cr\u00e9er des programmes MapReduce simples Cr\u00e9er des programmes MapReduce \u00e0 plusieurs phases D\u00e9ployer un cluster Hadoop Ressources \u00b6 Outils Oracle Virtual Box V6.1 (ou VMware) Image de machine virtuelle Ubuntu 20.04 avec les outils pr\u00e9-install\u00e9s Fichiers de donn\u00e9es (datasets) shakespeare.txt Sources et r\u00e9f\u00e9rence Documentation Hadoop Big Data Analytics with Hadoop 3, Sridhar Alla, Packt Publishing, 2018. Pr\u00e9sentation de Hadoop \u00b6 R\u00f4le \u00b6 Hadoop est un framework pour le stockage et le traitement distribu\u00e9 de grands volumes de donn\u00e9es sur des clusters d'ordinateurs \u00e0 l'aide de mod\u00e8les de programmation simples. Il permet de passer de n\u0153uds uniques \u00e0 des milliers de machines, chacune offrant un calcul et un stockage locaux. Hadoop garantit une haute disponibilit\u00e9 en d\u00e9tectant et traitant les pannes au niveau de la couche application. Hadoop supporte la scalabilit\u00e9 horizontale et verticale. Il est la plateforme Big Data de r\u00e9f\u00e9rence. Historique et versions \u00b6 Les principales nouveaut\u00e9s des versions majeures par rapport aux versions pr\u00e9c\u00e9dentes sont : Version 2 : YARN : la gestion des ressources du cluster. Version 3 : Erasure Coding : tol\u00e9rance aux fautes par bloc de parit\u00e9. Haute disponibilit\u00e9 am\u00e9lior\u00e9e avec plusieurs NameNode secondaires (en standby) Comment obtenir Hadoop ? Hadoop est projet Open Source avec la licence Apache V2. Mais il existe aussi des distributions commerciales offertes par des fournisseurs avec des outils suppl\u00e9mentaires pour former une plateforme de Big Data Analytics. Historiquement, les leaders sont Cloudera, Hortonworks et MapR. Mais ces derni\u00e8res ann\u00e9es, plusieurs acquisitions et fusions ont \u00e9t\u00e9 effectu\u00e9es. Les principales alternatives actuelles pour obtenir Hadoop sont : Apache Hadoop : C'est la version Open Source. Plus difficile \u00e0 g\u00e9rer pour composer un \u00e9cosyst\u00e8me complet malgr\u00e9 que la majorit\u00e9 des composants sont Open Source aussi (Spark, Flink, Hive, Hbase, ...). Distribution Hadoop : o\u00f9 une pile de composants sont pr\u00e9-install\u00e9 avec des outils de gestion et d'administration int\u00e9gr\u00e9s (Ambari, Clouder Manager, ...). Parmi ces distributions on trouve : Cloudera : apr\u00e8s la fusion, en 2018, avec son concurrent direct Hortonworks il a h\u00e9rit\u00e9 de ces produits HDP (Hortonworks Data Platform) et HDF (Hortonworks Data Flow) sous forme de machines virtuelles (VMware, VirtualBox et Docker) et CDP (Cloudera Data Platform) en Cloud. Hewlett Packard Enterprise : qui h\u00e9rite de la plateforme MapR Data Platform apr\u00e8s son acquisition en 2018. Elle est rebaptis\u00e9e sous le nom HPE Ezmeral Data Fabric V6.2 (voir comment l'installer ici ). Elle est compatible avec Ubuntu, RedHat/CentOS et SUSE. Un version l\u00e9g\u00e8re pour le d\u00e9veloppement et test est aussi offerte sous la forme d'un container Docker ( Development Environment for HPE Ezmeral Data Fabric ). IBM Open Platform: IOP ou IBM BigInsights est la distribution d'IBM disponible en version de production ou de test ( Quick Start Edition ). \u00c9quipement d\u00e9di\u00e9 : Mat\u00e9riel optimis\u00e9 pour Hadoop comme : Dell, EMC, Teradata Appliance for Hadoop, HP, Oracle, ... Hadoop sur le Cloud: comme PaaS (Platform as a Service) tel que Amazon EMR, Microsoft HDInsight, Google Cloud Platform, Qubole, IBM BigInsights, ... Dans cet atelier, c'est Apache Hadoop qui sera install\u00e9 sur une distribution Linux (Ubuntu 20.04). Pour tester plus d'outils de l'\u00e9cosyst\u00e8me Hadoop, je vous recommance la distribution Cloudera Data Platform 3.0 ou 2.6. Composants du noyau Hadoop \u00b6 3 composants principaux sont au c\u0153ur de Hadoop : HDFS (Hadoop Distributed File System) \u00b6 C'est le syst\u00e8me de fichier primaire de Hadoop. Il permet le stockage de larges volumes de donn\u00e9es sur des unit\u00e9s de stockage assez basique et abordable. Les donn\u00e9es sont partitionn\u00e9es et r\u00e9pliqu\u00e9es pour garantir la fiabilit\u00e9 et un acc\u00e8s parall\u00e8le. Il op\u00e8re selon le mod\u00e8le ma\u00eetre-esclave form\u00e9 respectivement par les n\u0153uds NameNode et DataNode. MapReduce \u00b6 C'est la couche de traitement dans Hadoop. Elle traite des volumes importants de donn\u00e9es structur\u00e9es et non structur\u00e9es stock\u00e9es dans HDFS. MapReduce traite \u00e9galement une \u00e9norme quantit\u00e9 de donn\u00e9es en parall\u00e8le. Pour ce faire, il divise le travail en un ensemble de t\u00e2ches ind\u00e9pendantes selon le principe divisier et conqu\u00e9rir . MapReduce fonctionne en divisant le traitement en phases: Map et Reduce. YARN (Yet Another Resource Negotiator) \u00b6 Il s'occupe de la gestion et la surveillance des travaux. YARN permet plusieurs moteurs de traitement de donn\u00e9es tels que le streaming en temps r\u00e9el, le traitement par lots, etc. Le Resource Manager est le composant au niveau de la machine ma\u00eetre. Il g\u00e8re les ressources et planifie les applications s'ex\u00e9cutant sur YARN. Il a deux composants: Scheduler & Application Manager. Tandis que le Node Manager, au niveau du n\u0153ud, communique en permanence avec Resource Manager et assure l'ex\u00e9cution des t\u00e2ches. Pr\u00e9paration de la machine virtuelle \u00b6 En utilisant VirtualBox et \u00e0 partir du menu Fichier -> Importer un appareil virtuel, s\u00e9lectionner le fichier Big Data.ova fourni et choisissez l'emplacement de destination. Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te et 16G pour la taille du disque dynamique ainsi que 128M pour la m\u00e9moire vid\u00e9o. D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur iset . Le mot de passe est aussi iset .","title":"Introduction"},{"location":"index.html#debuter_avec_hadoop_et_mapreduce","text":"","title":"D\u00e9buter avec Hadoop et MapReduce"},{"location":"index.html#objectifs","text":"G\u00e9rer et v\u00e9rifier les services de Hadoop Manipuler le syst\u00e8me de fichiers HDFS Comprendre et cr\u00e9er des programmes MapReduce simples Cr\u00e9er des programmes MapReduce \u00e0 plusieurs phases D\u00e9ployer un cluster Hadoop","title":"Objectifs"},{"location":"index.html#ressources","text":"Outils Oracle Virtual Box V6.1 (ou VMware) Image de machine virtuelle Ubuntu 20.04 avec les outils pr\u00e9-install\u00e9s Fichiers de donn\u00e9es (datasets) shakespeare.txt Sources et r\u00e9f\u00e9rence Documentation Hadoop Big Data Analytics with Hadoop 3, Sridhar Alla, Packt Publishing, 2018.","title":"Ressources"},{"location":"index.html#presentation_de_hadoop","text":"","title":"Pr\u00e9sentation de Hadoop"},{"location":"index.html#role","text":"Hadoop est un framework pour le stockage et le traitement distribu\u00e9 de grands volumes de donn\u00e9es sur des clusters d'ordinateurs \u00e0 l'aide de mod\u00e8les de programmation simples. Il permet de passer de n\u0153uds uniques \u00e0 des milliers de machines, chacune offrant un calcul et un stockage locaux. Hadoop garantit une haute disponibilit\u00e9 en d\u00e9tectant et traitant les pannes au niveau de la couche application. Hadoop supporte la scalabilit\u00e9 horizontale et verticale. Il est la plateforme Big Data de r\u00e9f\u00e9rence.","title":"R\u00f4le"},{"location":"index.html#historique_et_versions","text":"Les principales nouveaut\u00e9s des versions majeures par rapport aux versions pr\u00e9c\u00e9dentes sont : Version 2 : YARN : la gestion des ressources du cluster. Version 3 : Erasure Coding : tol\u00e9rance aux fautes par bloc de parit\u00e9. Haute disponibilit\u00e9 am\u00e9lior\u00e9e avec plusieurs NameNode secondaires (en standby) Comment obtenir Hadoop ? Hadoop est projet Open Source avec la licence Apache V2. Mais il existe aussi des distributions commerciales offertes par des fournisseurs avec des outils suppl\u00e9mentaires pour former une plateforme de Big Data Analytics. Historiquement, les leaders sont Cloudera, Hortonworks et MapR. Mais ces derni\u00e8res ann\u00e9es, plusieurs acquisitions et fusions ont \u00e9t\u00e9 effectu\u00e9es. Les principales alternatives actuelles pour obtenir Hadoop sont : Apache Hadoop : C'est la version Open Source. Plus difficile \u00e0 g\u00e9rer pour composer un \u00e9cosyst\u00e8me complet malgr\u00e9 que la majorit\u00e9 des composants sont Open Source aussi (Spark, Flink, Hive, Hbase, ...). Distribution Hadoop : o\u00f9 une pile de composants sont pr\u00e9-install\u00e9 avec des outils de gestion et d'administration int\u00e9gr\u00e9s (Ambari, Clouder Manager, ...). Parmi ces distributions on trouve : Cloudera : apr\u00e8s la fusion, en 2018, avec son concurrent direct Hortonworks il a h\u00e9rit\u00e9 de ces produits HDP (Hortonworks Data Platform) et HDF (Hortonworks Data Flow) sous forme de machines virtuelles (VMware, VirtualBox et Docker) et CDP (Cloudera Data Platform) en Cloud. Hewlett Packard Enterprise : qui h\u00e9rite de la plateforme MapR Data Platform apr\u00e8s son acquisition en 2018. Elle est rebaptis\u00e9e sous le nom HPE Ezmeral Data Fabric V6.2 (voir comment l'installer ici ). Elle est compatible avec Ubuntu, RedHat/CentOS et SUSE. Un version l\u00e9g\u00e8re pour le d\u00e9veloppement et test est aussi offerte sous la forme d'un container Docker ( Development Environment for HPE Ezmeral Data Fabric ). IBM Open Platform: IOP ou IBM BigInsights est la distribution d'IBM disponible en version de production ou de test ( Quick Start Edition ). \u00c9quipement d\u00e9di\u00e9 : Mat\u00e9riel optimis\u00e9 pour Hadoop comme : Dell, EMC, Teradata Appliance for Hadoop, HP, Oracle, ... Hadoop sur le Cloud: comme PaaS (Platform as a Service) tel que Amazon EMR, Microsoft HDInsight, Google Cloud Platform, Qubole, IBM BigInsights, ... Dans cet atelier, c'est Apache Hadoop qui sera install\u00e9 sur une distribution Linux (Ubuntu 20.04). Pour tester plus d'outils de l'\u00e9cosyst\u00e8me Hadoop, je vous recommance la distribution Cloudera Data Platform 3.0 ou 2.6.","title":"Historique et versions"},{"location":"index.html#composants_du_noyau_hadoop","text":"3 composants principaux sont au c\u0153ur de Hadoop :","title":"Composants  du noyau Hadoop"},{"location":"index.html#hdfs_hadoop_distributed_file_system","text":"C'est le syst\u00e8me de fichier primaire de Hadoop. Il permet le stockage de larges volumes de donn\u00e9es sur des unit\u00e9s de stockage assez basique et abordable. Les donn\u00e9es sont partitionn\u00e9es et r\u00e9pliqu\u00e9es pour garantir la fiabilit\u00e9 et un acc\u00e8s parall\u00e8le. Il op\u00e8re selon le mod\u00e8le ma\u00eetre-esclave form\u00e9 respectivement par les n\u0153uds NameNode et DataNode.","title":"HDFS (Hadoop Distributed File System)"},{"location":"index.html#mapreduce","text":"C'est la couche de traitement dans Hadoop. Elle traite des volumes importants de donn\u00e9es structur\u00e9es et non structur\u00e9es stock\u00e9es dans HDFS. MapReduce traite \u00e9galement une \u00e9norme quantit\u00e9 de donn\u00e9es en parall\u00e8le. Pour ce faire, il divise le travail en un ensemble de t\u00e2ches ind\u00e9pendantes selon le principe divisier et conqu\u00e9rir . MapReduce fonctionne en divisant le traitement en phases: Map et Reduce.","title":"MapReduce"},{"location":"index.html#yarn_yet_another_resource_negotiator","text":"Il s'occupe de la gestion et la surveillance des travaux. YARN permet plusieurs moteurs de traitement de donn\u00e9es tels que le streaming en temps r\u00e9el, le traitement par lots, etc. Le Resource Manager est le composant au niveau de la machine ma\u00eetre. Il g\u00e8re les ressources et planifie les applications s'ex\u00e9cutant sur YARN. Il a deux composants: Scheduler & Application Manager. Tandis que le Node Manager, au niveau du n\u0153ud, communique en permanence avec Resource Manager et assure l'ex\u00e9cution des t\u00e2ches.","title":"YARN (Yet Another Resource Negotiator)"},{"location":"index.html#preparation_de_la_machine_virtuelle","text":"En utilisant VirtualBox et \u00e0 partir du menu Fichier -> Importer un appareil virtuel, s\u00e9lectionner le fichier Big Data.ova fourni et choisissez l'emplacement de destination. Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te et 16G pour la taille du disque dynamique ainsi que 128M pour la m\u00e9moire vid\u00e9o. D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur iset . Le mot de passe est aussi iset .","title":"Pr\u00e9paration de la machine virtuelle"},{"location":"cluster.html","text":"Hadoop en mode cluster \u00b6 Virtualisation avec Docker \u00b6 Utiliser plusieurs machines virtuelles requiert plus des ressources. Pour cela, nous allons utiliser des containers Docker. Le sc\u00e9nario consiste \u00e0 d\u00e9ployer 4 n\u0153uds hadoop \u00e0 partir d'une image Docker fournie [ici][]. Cette image est similaire \u00e0 celle utilis\u00e9e avec VirtuelBox : elle contient la verion 3.2.1 de hadoop, mrjob, python3 et jupyter. D\u00e9ploiement avec Docker \u00b6 Charger l'image docker Soit \u00e0 partir du fichier fourni : docker load < hadoop-3.2.1-docker.tar.gz Ou \u00e0 partir du Docker Hub : docker pull hhmida/hadoop:3.2.1 Cr\u00e9er un r\u00e9seau virtuel : docker network create --driver bridge hadoopnet Ex\u00e9cuter 4 instance de l'image avec les noms nodemaster node2, node3, et node4 docker run -d --network hadoopnet --name nodemaster -it -h nodemaster -p 8088 :8088 -p 9870 :9870 -p 9864 :9864 -p 19888 :19888 -p 8042 :8042 -p 8888 :8888 hadoop:3.2.1 docker run -dP --network hadoopnet --name node2 -it -h node2 hadoop:3.2.1 docker run -dP --network hadoopnet --name node3 -it -h node3 hadoop:3.2.1 docker run -dP --network hadoopnet --name node4 -it -h node4 hadoop:3.2.1 Image t\u00e9l\u00e9charg\u00e9e depuis Docker Hub Si vous avez t\u00e9l\u00e9charger l'image avec la commande docker pull alors remplacer hadoop:3.2.1 par hhmida/hadoop:3.2.1 . Formater le nodemaster : docker exec -u hadoop -it nodemaster hadoop/bin/hdfs namenode -format D\u00e9marrage du cluster \u00b6 D\u00e9marrer les containers docker start nodemaster node2 node3 node4 D\u00e9marrer les services hadoop Ex\u00e9cuter ces 2 commandes pour d\u00e9marrer les services \u00e0 partir du nodemaster : docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-dfs.sh docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-yarn.sh cr\u00e9er le dossier racine sur HDFS docker exec -u hadoop -it nodemaster /home/hadoop/hadoop/bin/hadoop fs -mkdir -p . D\u00e9marrer jupyter docker exec -u hadoop -d nodemaster jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = '' Arr\u00eat des n\u0153uds \u00b6 docker stop nodemaster node2 node3 node4 Red\u00e9marrage du cluster Pour red\u00e9marrer le cluster ex\u00e9cuter les \u00e9tapes de la section D\u00e9marrage du cluster . V\u00e9rification de l'\u00e9tat du cluster \u00b6 Avec la commande jps : Au niveau du nodemaster Au niveau de node2 et node3 Acc\u00e9der aux URLs suivantes pour v\u00e9rifier l'\u00e9tat et la configuration du cluster Cluster Hadoop : http://localhost:8088 HDFS : http://localhost:9870 Jupyter notebook : http://localhost:8888 L'interface du Resource Manager montre les n&339;uds : L'interface HDFS montre les n\u0153uds DataNodes actifs : Remarquer la colonne Last contact qui refl\u00e8te le dernier heartbeaz re\u00e7u (inf\u00e9rieur au timeout par d\u00e9faut : 3 Secondes). Maintenant, arr\u00eater le n\u0153ud node3 : docker stop node4 Apr\u00e8s 1000 Secondes le n\u0153ud est consid\u00e9r\u00e9 comme indisponible : Virtualisation avec machines virtuelles \u00b6 Cr\u00e9er un cluster avec la machine virtuelle est plus simple mais n\u00e9cessite plus de ressources. En effet, dans notre exemple et pour cr\u00e9er un cluster compos\u00e9 d'un ma\u00eetre et 2 workers, vous devez disposer de 8GO de RAM au minimum dont 6GO pour les machines virtuelles. Modifications sur la machine virtuelle originale \u00b6 Changer le nom de la machine dans le fichier /etc/hostname en nodemaster sudo echo \"nodemaster\" > /etc/hostname Modifier le fichier /etc/hosts : sudo nano /etc/hosts Puis \u00e9crire les lignes suivantes et enregistrer : 192 .168.100.10 nodemaster 192 .168.100.20 node2 192 .168.100.30 mode3 Modifier les fichiers de configuration de Hadoop Modifier le fichier /ur/local/hadoop/etc/hadoop/core-site.xml ainsi : <property> <name> fs.defaultFS </name> <value> hdfs://nodemaster:9000 </value> </property> Changer la replication dans /usr/local/hadoop/etc/hadoop/hdfs-site.xml <property> <name> dfs.replication </name> <value> 3 </value> </property> Ajouter cette propri\u00e9t\u00e9 dans /usr/local/hadoop/etc/hadoop/yarn-site.xml <property> <name> yarn.resourcemanager.hostname </name> <value> nodemaster </value> </property> Le fichier /usr/local/hadoop/etc/hadoop/workers contient les noms des machines du cluster, il sera utilis\u00e9 pour d\u00e9marrer les services depuis le n\u0153ud ma\u00eetre. nodemaster node2 node3 Workers nodemaster dans cette configuration est consid\u00e9r\u00e9 comme namenode et datanode \u00e0 la fois. Clonage de la machine virtuelle \u00b6 Cr\u00e9er 2 clones de la machine virtuelle originale qui vont \u00eatre respectivement node2 et node3. Chager leurs noms dans /etc/hostname respectivement en node2 et node3 Sur la machine node2 : sudo echo \"node2\" > /etc/hostname Sur la machine node3 : sudo echo \"node3\" > /etc/hostname Changer les adresses IP en 192.168.100.20 et 192.168.100.30 en suivant les \u00e9tapes suivantes (refaire les m\u00eames \u00e9tapes pour node3) : sudo nano /etc/netplan/00-installer-config.yaml Puis appliquer les modifications : sudo netplan apply Red\u00e9marrer les 3 machines et d\u00e9marrer les services depuis le nodemaster : start-dfs.sh start-yarn.sh V\u00e9rifier avec jps ou les URLs http://192.168.100.10:9870 et http://192.168.100.10:8088 Test de l'arr\u00eat d'un n\u0153ud : Voir la section pour l'exemple r\u00e9alis\u00e9 avec les containers Docker ( ici ). Test de Map Reduce sur le cluster \u00b6 Refaire l'exemple wordcout (voir ici ): Mettre le fichier shakespeare.txt sur HDFS : hadoop fs -put shakespeare.txt Lancer Jupyter jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = '' Se connecter \u00e0 http://192.168.100.10:8888 , cr\u00e9er le notebook et ajouter le code de l'exemple wordcount : Pendant l'ex\u00e9cution, vous pouvez visualiser l'\u00e9tat des ressources allou\u00e9es en cliquant sur Scheduler sur http://192.168.100.10:8088 Apr\u00e8s l'ex\u00e9cution r\u00e9ussie sur le cluster, cliquer sur Applications pour v\u00e9rifier l'\u00e9tat de l'application. Pour afficher le d\u00e9tail des t\u00e2ches, il est possible de voir le Job History. D\u00e9marrer le serveur job History et acc\u00e9der \u00e0 son interface web. D\u00e9marrer le serveur mapred jobhistoryserver start Acc\u00e8s \u00e0 l'interface web : http://192.168.100.10:19888 En cliquant sur le Job ID, le nombre de t\u00e2ches Map et Reduce est affich\u00e9 (dans ce cas 2 Maps et 1 Reduce) Et puis chacune des t\u00e2ches : La premi\u00e8re t\u00e2che Map La seconde t\u00e2che Map La t\u00e2che Reduce","title":"Hadoop en mode cluster"},{"location":"cluster.html#hadoop_en_mode_cluster","text":"","title":"Hadoop en mode cluster"},{"location":"cluster.html#virtualisation_avec_docker","text":"Utiliser plusieurs machines virtuelles requiert plus des ressources. Pour cela, nous allons utiliser des containers Docker. Le sc\u00e9nario consiste \u00e0 d\u00e9ployer 4 n\u0153uds hadoop \u00e0 partir d'une image Docker fournie [ici][]. Cette image est similaire \u00e0 celle utilis\u00e9e avec VirtuelBox : elle contient la verion 3.2.1 de hadoop, mrjob, python3 et jupyter.","title":"Virtualisation avec Docker"},{"location":"cluster.html#deploiement_avec_docker","text":"Charger l'image docker Soit \u00e0 partir du fichier fourni : docker load < hadoop-3.2.1-docker.tar.gz Ou \u00e0 partir du Docker Hub : docker pull hhmida/hadoop:3.2.1 Cr\u00e9er un r\u00e9seau virtuel : docker network create --driver bridge hadoopnet Ex\u00e9cuter 4 instance de l'image avec les noms nodemaster node2, node3, et node4 docker run -d --network hadoopnet --name nodemaster -it -h nodemaster -p 8088 :8088 -p 9870 :9870 -p 9864 :9864 -p 19888 :19888 -p 8042 :8042 -p 8888 :8888 hadoop:3.2.1 docker run -dP --network hadoopnet --name node2 -it -h node2 hadoop:3.2.1 docker run -dP --network hadoopnet --name node3 -it -h node3 hadoop:3.2.1 docker run -dP --network hadoopnet --name node4 -it -h node4 hadoop:3.2.1 Image t\u00e9l\u00e9charg\u00e9e depuis Docker Hub Si vous avez t\u00e9l\u00e9charger l'image avec la commande docker pull alors remplacer hadoop:3.2.1 par hhmida/hadoop:3.2.1 . Formater le nodemaster : docker exec -u hadoop -it nodemaster hadoop/bin/hdfs namenode -format","title":"D\u00e9ploiement avec Docker"},{"location":"cluster.html#demarrage_du_cluster","text":"D\u00e9marrer les containers docker start nodemaster node2 node3 node4 D\u00e9marrer les services hadoop Ex\u00e9cuter ces 2 commandes pour d\u00e9marrer les services \u00e0 partir du nodemaster : docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-dfs.sh docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-yarn.sh cr\u00e9er le dossier racine sur HDFS docker exec -u hadoop -it nodemaster /home/hadoop/hadoop/bin/hadoop fs -mkdir -p . D\u00e9marrer jupyter docker exec -u hadoop -d nodemaster jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = ''","title":"D\u00e9marrage du cluster"},{"location":"cluster.html#arret_des_nuds","text":"docker stop nodemaster node2 node3 node4 Red\u00e9marrage du cluster Pour red\u00e9marrer le cluster ex\u00e9cuter les \u00e9tapes de la section D\u00e9marrage du cluster .","title":"Arr\u00eat des n&#339;uds"},{"location":"cluster.html#verification_de_letat_du_cluster","text":"Avec la commande jps : Au niveau du nodemaster Au niveau de node2 et node3 Acc\u00e9der aux URLs suivantes pour v\u00e9rifier l'\u00e9tat et la configuration du cluster Cluster Hadoop : http://localhost:8088 HDFS : http://localhost:9870 Jupyter notebook : http://localhost:8888 L'interface du Resource Manager montre les n&339;uds : L'interface HDFS montre les n\u0153uds DataNodes actifs : Remarquer la colonne Last contact qui refl\u00e8te le dernier heartbeaz re\u00e7u (inf\u00e9rieur au timeout par d\u00e9faut : 3 Secondes). Maintenant, arr\u00eater le n\u0153ud node3 : docker stop node4 Apr\u00e8s 1000 Secondes le n\u0153ud est consid\u00e9r\u00e9 comme indisponible :","title":"V\u00e9rification de l'\u00e9tat du cluster"},{"location":"cluster.html#virtualisation_avec_machines_virtuelles","text":"Cr\u00e9er un cluster avec la machine virtuelle est plus simple mais n\u00e9cessite plus de ressources. En effet, dans notre exemple et pour cr\u00e9er un cluster compos\u00e9 d'un ma\u00eetre et 2 workers, vous devez disposer de 8GO de RAM au minimum dont 6GO pour les machines virtuelles.","title":"Virtualisation avec machines virtuelles"},{"location":"cluster.html#modifications_sur_la_machine_virtuelle_originale","text":"Changer le nom de la machine dans le fichier /etc/hostname en nodemaster sudo echo \"nodemaster\" > /etc/hostname Modifier le fichier /etc/hosts : sudo nano /etc/hosts Puis \u00e9crire les lignes suivantes et enregistrer : 192 .168.100.10 nodemaster 192 .168.100.20 node2 192 .168.100.30 mode3 Modifier les fichiers de configuration de Hadoop Modifier le fichier /ur/local/hadoop/etc/hadoop/core-site.xml ainsi : <property> <name> fs.defaultFS </name> <value> hdfs://nodemaster:9000 </value> </property> Changer la replication dans /usr/local/hadoop/etc/hadoop/hdfs-site.xml <property> <name> dfs.replication </name> <value> 3 </value> </property> Ajouter cette propri\u00e9t\u00e9 dans /usr/local/hadoop/etc/hadoop/yarn-site.xml <property> <name> yarn.resourcemanager.hostname </name> <value> nodemaster </value> </property> Le fichier /usr/local/hadoop/etc/hadoop/workers contient les noms des machines du cluster, il sera utilis\u00e9 pour d\u00e9marrer les services depuis le n\u0153ud ma\u00eetre. nodemaster node2 node3 Workers nodemaster dans cette configuration est consid\u00e9r\u00e9 comme namenode et datanode \u00e0 la fois.","title":"Modifications sur la machine virtuelle originale"},{"location":"cluster.html#clonage_de_la_machine_virtuelle","text":"Cr\u00e9er 2 clones de la machine virtuelle originale qui vont \u00eatre respectivement node2 et node3. Chager leurs noms dans /etc/hostname respectivement en node2 et node3 Sur la machine node2 : sudo echo \"node2\" > /etc/hostname Sur la machine node3 : sudo echo \"node3\" > /etc/hostname Changer les adresses IP en 192.168.100.20 et 192.168.100.30 en suivant les \u00e9tapes suivantes (refaire les m\u00eames \u00e9tapes pour node3) : sudo nano /etc/netplan/00-installer-config.yaml Puis appliquer les modifications : sudo netplan apply Red\u00e9marrer les 3 machines et d\u00e9marrer les services depuis le nodemaster : start-dfs.sh start-yarn.sh V\u00e9rifier avec jps ou les URLs http://192.168.100.10:9870 et http://192.168.100.10:8088 Test de l'arr\u00eat d'un n\u0153ud : Voir la section pour l'exemple r\u00e9alis\u00e9 avec les containers Docker ( ici ).","title":"Clonage de la machine virtuelle"},{"location":"cluster.html#test_de_map_reduce_sur_le_cluster","text":"Refaire l'exemple wordcout (voir ici ): Mettre le fichier shakespeare.txt sur HDFS : hadoop fs -put shakespeare.txt Lancer Jupyter jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = '' Se connecter \u00e0 http://192.168.100.10:8888 , cr\u00e9er le notebook et ajouter le code de l'exemple wordcount : Pendant l'ex\u00e9cution, vous pouvez visualiser l'\u00e9tat des ressources allou\u00e9es en cliquant sur Scheduler sur http://192.168.100.10:8088 Apr\u00e8s l'ex\u00e9cution r\u00e9ussie sur le cluster, cliquer sur Applications pour v\u00e9rifier l'\u00e9tat de l'application. Pour afficher le d\u00e9tail des t\u00e2ches, il est possible de voir le Job History. D\u00e9marrer le serveur job History et acc\u00e9der \u00e0 son interface web. D\u00e9marrer le serveur mapred jobhistoryserver start Acc\u00e8s \u00e0 l'interface web : http://192.168.100.10:19888 En cliquant sur le Job ID, le nombre de t\u00e2ches Map et Reduce est affich\u00e9 (dans ce cas 2 Maps et 1 Reduce) Et puis chacune des t\u00e2ches : La premi\u00e8re t\u00e2che Map La seconde t\u00e2che Map La t\u00e2che Reduce","title":"Test de Map Reduce sur le cluster"},{"location":"hdfs.html","text":"Manipulation de HDFS \u00b6 Service HDFS \u00b6 D\u00e9marrer/Arr\u00eater \u00b6 Service YARN D\u00e9marrage start-dfs.sh Arr\u00eat stop-dfs.sh Sous Linux l'affichage est comme suit : La commande jps permet de v\u00e9rifier que trois processus sont lanc\u00e9s : NameNode et DataNode et SecondaryNameNode. HDFS est maintenant accessible via l'interface web : http://localhost:9870 Cette interface permet d'afficher l'\u00e9tat de HDFS et ses diffrents datanodes. Il est possible d'explorer le contenu du syst\u00e8me de fichiers \u00e0 partir du menu Utilities puis Browse the file system . Formatage \u00b6 ```bash hdfs namenode -format ``` Attention L'op\u00e9ration de formatage supprime tous les fichiers. Elle est effectu\u00e9e lors de l'installation de Hadoop ou pour r\u00e9initialiser HDFS. Commandes HDFS \u00b6 Le syst\u00e8me HDFS est manipul\u00e9 \u00e0 travers des commandes inspir\u00e9es du syst\u00e8me Linux. La forme g\u00e9n\u00e9rale de ces commandes est comme suit : Format des commandes HDFS hadoop fs -nomCommande -options param1 ... Affichage du contenu d'un dossier \u00b6 ```bash hadoop fs -ls ``` Dossier par d\u00e9faut Pour les chemins relatifs HDFS utilise /user/nom_utilisateur comme racine ou nom_utilisateur est l'utilisateur connect\u00e9. Si le dossier /user/<nom utilisateur> n'est pas cr\u00e9\u00e9, cette commande provoque une erreur. Dans notre cas, c'est le dossier /user/iset qui doit \u00eatre cr\u00e9\u00e9. hadoop fs -mkdir -p . ou hadoop fs -mkdir -p /user/iset Cr\u00e9er des fichiers \u00b6 Cr\u00e9er un fichier test.txt hadoop fs -touch test.txt Upload de fichiers ou dossiers \u00b6 Upload de fichier Le fichier exemple.txt doit \u00eatre dans le dossier en cours. hadoop fs -put exemple.txt ou hadoop fs -copyFromLocal exeple.txt Supprimer des fichiers \u00b6 Supprimer fichier test.txt hadoop fs -rm test.txt Cr\u00e9er des dossier \u00b6 Cr\u00e9er un dossier data hadoop fs -mkdir data Supprimer des dossiers \u00b6 Supprimer le dossier data hadoop fs -rmdir data Copier/d\u00e9placer des fichiers ou des dossiers \u00b6 Copier le fichier test.txt hadoop fs -cp test.txt data/copie.txt D\u00e9placer le fichier copie.txt hadoop fs -mv data/copie.txt copie2.txt Exercice \u00b6 Sur HDFS, cr\u00e9er l'arborescence tunisie/petrole. Chercher et t\u00e9l\u00e9charger, sur http://data.industrie.gov.tn , les donn\u00e9es sur la production p\u00e9troli\u00e8re mensuelle par champ dans le format CSV. Placer le fichier dans le dossier petrole cr\u00e9\u00e9 dans la premi\u00e8re \u00e9tape. \u00c0 partir de l'interface web : Chercher la taille du bloc par d\u00e9faut. Copier sur HDFS un fichier plus grand que la taille du bloc et v\u00e9rifier le nombre de blocs de ce fichier.","title":"Manipulation de HDFS"},{"location":"hdfs.html#manipulation_de_hdfs","text":"","title":"Manipulation de HDFS"},{"location":"hdfs.html#service_hdfs","text":"","title":"Service HDFS"},{"location":"hdfs.html#demarrerarreter","text":"Service YARN D\u00e9marrage start-dfs.sh Arr\u00eat stop-dfs.sh Sous Linux l'affichage est comme suit : La commande jps permet de v\u00e9rifier que trois processus sont lanc\u00e9s : NameNode et DataNode et SecondaryNameNode. HDFS est maintenant accessible via l'interface web : http://localhost:9870 Cette interface permet d'afficher l'\u00e9tat de HDFS et ses diffrents datanodes. Il est possible d'explorer le contenu du syst\u00e8me de fichiers \u00e0 partir du menu Utilities puis Browse the file system .","title":"D\u00e9marrer/Arr\u00eater"},{"location":"hdfs.html#formatage","text":"```bash hdfs namenode -format ``` Attention L'op\u00e9ration de formatage supprime tous les fichiers. Elle est effectu\u00e9e lors de l'installation de Hadoop ou pour r\u00e9initialiser HDFS.","title":"Formatage"},{"location":"hdfs.html#commandes_hdfs","text":"Le syst\u00e8me HDFS est manipul\u00e9 \u00e0 travers des commandes inspir\u00e9es du syst\u00e8me Linux. La forme g\u00e9n\u00e9rale de ces commandes est comme suit : Format des commandes HDFS hadoop fs -nomCommande -options param1 ...","title":"Commandes HDFS"},{"location":"hdfs.html#affichage_du_contenu_dun_dossier","text":"```bash hadoop fs -ls ``` Dossier par d\u00e9faut Pour les chemins relatifs HDFS utilise /user/nom_utilisateur comme racine ou nom_utilisateur est l'utilisateur connect\u00e9. Si le dossier /user/<nom utilisateur> n'est pas cr\u00e9\u00e9, cette commande provoque une erreur. Dans notre cas, c'est le dossier /user/iset qui doit \u00eatre cr\u00e9\u00e9. hadoop fs -mkdir -p . ou hadoop fs -mkdir -p /user/iset","title":"Affichage du contenu d'un dossier"},{"location":"hdfs.html#creer_des_fichiers","text":"Cr\u00e9er un fichier test.txt hadoop fs -touch test.txt","title":"Cr\u00e9er des fichiers"},{"location":"hdfs.html#upload_de_fichiers_ou_dossiers","text":"Upload de fichier Le fichier exemple.txt doit \u00eatre dans le dossier en cours. hadoop fs -put exemple.txt ou hadoop fs -copyFromLocal exeple.txt","title":"Upload de fichiers ou dossiers"},{"location":"hdfs.html#supprimer_des_fichiers","text":"Supprimer fichier test.txt hadoop fs -rm test.txt","title":"Supprimer des fichiers"},{"location":"hdfs.html#creer_des_dossier","text":"Cr\u00e9er un dossier data hadoop fs -mkdir data","title":"Cr\u00e9er des dossier"},{"location":"hdfs.html#supprimer_des_dossiers","text":"Supprimer le dossier data hadoop fs -rmdir data","title":"Supprimer des dossiers"},{"location":"hdfs.html#copierdeplacer_des_fichiers_ou_des_dossiers","text":"Copier le fichier test.txt hadoop fs -cp test.txt data/copie.txt D\u00e9placer le fichier copie.txt hadoop fs -mv data/copie.txt copie2.txt","title":"Copier/d\u00e9placer des fichiers ou des dossiers"},{"location":"hdfs.html#exercice","text":"Sur HDFS, cr\u00e9er l'arborescence tunisie/petrole. Chercher et t\u00e9l\u00e9charger, sur http://data.industrie.gov.tn , les donn\u00e9es sur la production p\u00e9troli\u00e8re mensuelle par champ dans le format CSV. Placer le fichier dans le dossier petrole cr\u00e9\u00e9 dans la premi\u00e8re \u00e9tape. \u00c0 partir de l'interface web : Chercher la taille du bloc par d\u00e9faut. Copier sur HDFS un fichier plus grand que la taille du bloc et v\u00e9rifier le nombre de blocs de ce fichier.","title":"Exercice"},{"location":"mr.html","text":"MapReduce avec Python \u00b6 D\u00e9marrage/Arr\u00eat du service \u00b6 Service YARN D\u00e9marrage start-yarn.sh Arr\u00eat stop-yarn.sh La commande jps permet de v\u00e9rifier que deux processus sont lanc\u00e9s : ResourceManager et NodeManager. L'interface du Resource Manager est accessible depuis : http://localhost:8088 Cette interface permet de v\u00e9rifier l'\u00e9tat des ressources RAM et CPU du cluster, les applications (d\u00e9marr\u00e9es, termin\u00e9es, en cours ...) et les noeuds du cluster. D\u00e9marrage et arr\u00eat de Hadoop D\u00e9marrage start-dfs.sh start-yarn.sh Arr\u00eat stop-dfs.sh stop-yarn.sh D\u00e9marrer HDFS et YARN Ces scripts ne sont utilis\u00e9s que dans un environnement de test. start-all.sh stop-all.sh V\u00e9rification des processus serveur \u00b6 Pour voir les diff\u00e9rents processus serveurs, ex\u00e9cuter la commande jps . Le r\u00e9sultat est : Processus HDFS : NameNode, SecondaryNameNode, DataNode Processus YARN : ResourceManager, NodeManager Pr\u00e9parer l'environnement \u00b6 Pour les exemples et les exercices de cette section, vous avez besoin de Python, Jupyter et MRJOB. Installer les paquets et modules suivants : Cette \u00e9tape est \u00e0 ignorer si vous utilisez la machine virtuelle fournie. ```bash $ sudo apt-get install python python3-pip $ sudo pip3 install mrjob $ sudo pip3 install jupyter ``` Lancer Jupyter Notebook ```bash $ cd dossier_travail $ jupyter notebook --no-browser --ip=0.0.0.0 --NotebookApp.token='' ``` Pour acc\u00e9der depuis la machine h\u00f4te aller \u00e0 l'adresse : http://localhost:8888 Les exemples peuvent \u00eatre \u00e9crit avec un simple \u00e9diteur comme nano, vi ou notepad sous windows. Et les commandes peuvent \u00eatre lanc\u00e9es \u00e0 partir du shell. Premier Exemple : Word Count \u00b6 L'exemple classique pour illustrer le fonctionnement de MapReduce est celui de compter le nombre d'occurences d'un mot dans un fichier texte (le fichier shakespeare.txt .) Pr\u00e9parer les donn\u00e9es \u00b6 En utilisant WinSCP, envoyer le fichier shakespeare.txt sur la machine virtuelle Hadoop. Mettre le fichier dans le syst\u00e8me HDFS !#bash $ hadoop fs -put shakespeare.txt Mapper et Reducer \u00b6 L'objectif est de d\u00e9terminer le nombre d'occurences de chaque du fichier. Chaque ligne est compos\u00e9e de plusieurs mots (Le s\u00e9parateur est ' '). La transformation \u00e0 effectuer par chaque Mapper est de d\u00e9composer chaque ligne re\u00e7ue en mots est lui associer la valeur 1. Apr\u00e8s la phase de tri et de redistribution (sort and shuffle), chaque Reducer va recevoir des mots et une liste de '1' pour chaque occurence. Pour calculer le nombre d'occurence total, il suffit de faire la somme de ces '1'. Programmation avec MRJob \u00b6 MapReduce est \u00e9crit en Java mais accepte les autres langages de programmation via la technique de streaming via op\u00e9ration de lecture \u00e9criture sur les flux d'entr\u00e9e-sortie standard (print, read). La biblioth\u00e8que mrjob simplifie l'\u00e9criture de programmes MapReduce. Dans cet exemple, deux classes de la librairie mrjob sont utilis\u00e9es : MRJob et MRStep. Pour cr\u00e9er un Job MapReduce, il suffit de cr\u00e9er une classe qui h\u00e9rite de MRJob (ici la classe WordCount). Cr\u00e9er une fonction qui effectue la transformation du Mapper : la m\u00e9thode mapper_get_words . Cette m\u00e9thode re\u00e7oit 3 param\u00e8tres self comme toutes les m\u00e9thodes d'une classe, le deuxi\u00e8me est ignor\u00e9 pour ce mapper et le dernier c'est une ligne du fichier de donn\u00e9es. La fonction split d\u00e9compose la ligne en une liste de mots. Cr\u00e9er une fonction qui r\u00e9alise l'agr\u00e9gation du Reducer : la m\u00e9thode reducer_count_words . Elle re\u00e7oit le param\u00e8tre self, la cl\u00e9 utilis\u00e9e par le Mapper (donc un mot) et la liste des valeurs r\u00e9cupr\u00e9es des diff\u00e9rents mappers (les '1'). L'agr\u00e9gation effectu\u00e9e est une somme par la fonction sum. D\u00e9finier les \u00e9tapes ou les diff\u00e9rentes t\u00e2che \u00e0 effectuer dans une m\u00e9thode appel\u00e9e steps qui doit retouner une liste de MRStep. Pour chaque instance MRStep cr\u00e9\u00e9e, on sp\u00e9cifie le nom de la fonction mapper et reducer d\u00e9j\u00e0 d\u00e9finie. Dans cet exemple, il y a une seule phase MapReduce. Enfin l'appel de la m\u00e9thode run pour d\u00e9clencher L'ex\u00e9cution du Job MapReduce. Dans ce qui suit le contenu du Notebook Jupyter avec le code \u00e0 tester. Exercice \u00b6 Transformer l'exemple WordCount afin d'obtenir le r\u00e9sultat tri\u00e9 dans l'ordre d\u00e9croissant du nombre d'occurrences. (Hint : exploiter la phase sort and shuffle qui trie la cl\u00e9 selon l'ordre alphab\u00e9tique). Sur les donn\u00e9es de la production p\u00e9troli\u00e8re de la section pr\u00e9c\u00e9dente et en utilisant MapReduce : calculer la production annuelle de chaque champ. Trouver le champ avec la plus grande production par mois.","title":"MapReduce avec Python"},{"location":"mr.html#mapreduce_avec_python","text":"","title":"MapReduce avec Python"},{"location":"mr.html#demarragearret_du_service","text":"Service YARN D\u00e9marrage start-yarn.sh Arr\u00eat stop-yarn.sh La commande jps permet de v\u00e9rifier que deux processus sont lanc\u00e9s : ResourceManager et NodeManager. L'interface du Resource Manager est accessible depuis : http://localhost:8088 Cette interface permet de v\u00e9rifier l'\u00e9tat des ressources RAM et CPU du cluster, les applications (d\u00e9marr\u00e9es, termin\u00e9es, en cours ...) et les noeuds du cluster. D\u00e9marrage et arr\u00eat de Hadoop D\u00e9marrage start-dfs.sh start-yarn.sh Arr\u00eat stop-dfs.sh stop-yarn.sh D\u00e9marrer HDFS et YARN Ces scripts ne sont utilis\u00e9s que dans un environnement de test. start-all.sh stop-all.sh","title":"D\u00e9marrage/Arr\u00eat du service"},{"location":"mr.html#verification_des_processus_serveur","text":"Pour voir les diff\u00e9rents processus serveurs, ex\u00e9cuter la commande jps . Le r\u00e9sultat est : Processus HDFS : NameNode, SecondaryNameNode, DataNode Processus YARN : ResourceManager, NodeManager","title":"V\u00e9rification des processus serveur"},{"location":"mr.html#preparer_lenvironnement","text":"Pour les exemples et les exercices de cette section, vous avez besoin de Python, Jupyter et MRJOB. Installer les paquets et modules suivants : Cette \u00e9tape est \u00e0 ignorer si vous utilisez la machine virtuelle fournie. ```bash $ sudo apt-get install python python3-pip $ sudo pip3 install mrjob $ sudo pip3 install jupyter ``` Lancer Jupyter Notebook ```bash $ cd dossier_travail $ jupyter notebook --no-browser --ip=0.0.0.0 --NotebookApp.token='' ``` Pour acc\u00e9der depuis la machine h\u00f4te aller \u00e0 l'adresse : http://localhost:8888 Les exemples peuvent \u00eatre \u00e9crit avec un simple \u00e9diteur comme nano, vi ou notepad sous windows. Et les commandes peuvent \u00eatre lanc\u00e9es \u00e0 partir du shell.","title":"Pr\u00e9parer l'environnement"},{"location":"mr.html#premier_exemple_word_count","text":"L'exemple classique pour illustrer le fonctionnement de MapReduce est celui de compter le nombre d'occurences d'un mot dans un fichier texte (le fichier shakespeare.txt .)","title":"Premier Exemple : Word Count"},{"location":"mr.html#preparer_les_donnees","text":"En utilisant WinSCP, envoyer le fichier shakespeare.txt sur la machine virtuelle Hadoop. Mettre le fichier dans le syst\u00e8me HDFS !#bash $ hadoop fs -put shakespeare.txt","title":"Pr\u00e9parer les donn\u00e9es"},{"location":"mr.html#mapper_et_reducer","text":"L'objectif est de d\u00e9terminer le nombre d'occurences de chaque du fichier. Chaque ligne est compos\u00e9e de plusieurs mots (Le s\u00e9parateur est ' '). La transformation \u00e0 effectuer par chaque Mapper est de d\u00e9composer chaque ligne re\u00e7ue en mots est lui associer la valeur 1. Apr\u00e8s la phase de tri et de redistribution (sort and shuffle), chaque Reducer va recevoir des mots et une liste de '1' pour chaque occurence. Pour calculer le nombre d'occurence total, il suffit de faire la somme de ces '1'.","title":"Mapper et Reducer"},{"location":"mr.html#programmation_avec_mrjob","text":"MapReduce est \u00e9crit en Java mais accepte les autres langages de programmation via la technique de streaming via op\u00e9ration de lecture \u00e9criture sur les flux d'entr\u00e9e-sortie standard (print, read). La biblioth\u00e8que mrjob simplifie l'\u00e9criture de programmes MapReduce. Dans cet exemple, deux classes de la librairie mrjob sont utilis\u00e9es : MRJob et MRStep. Pour cr\u00e9er un Job MapReduce, il suffit de cr\u00e9er une classe qui h\u00e9rite de MRJob (ici la classe WordCount). Cr\u00e9er une fonction qui effectue la transformation du Mapper : la m\u00e9thode mapper_get_words . Cette m\u00e9thode re\u00e7oit 3 param\u00e8tres self comme toutes les m\u00e9thodes d'une classe, le deuxi\u00e8me est ignor\u00e9 pour ce mapper et le dernier c'est une ligne du fichier de donn\u00e9es. La fonction split d\u00e9compose la ligne en une liste de mots. Cr\u00e9er une fonction qui r\u00e9alise l'agr\u00e9gation du Reducer : la m\u00e9thode reducer_count_words . Elle re\u00e7oit le param\u00e8tre self, la cl\u00e9 utilis\u00e9e par le Mapper (donc un mot) et la liste des valeurs r\u00e9cupr\u00e9es des diff\u00e9rents mappers (les '1'). L'agr\u00e9gation effectu\u00e9e est une somme par la fonction sum. D\u00e9finier les \u00e9tapes ou les diff\u00e9rentes t\u00e2che \u00e0 effectuer dans une m\u00e9thode appel\u00e9e steps qui doit retouner une liste de MRStep. Pour chaque instance MRStep cr\u00e9\u00e9e, on sp\u00e9cifie le nom de la fonction mapper et reducer d\u00e9j\u00e0 d\u00e9finie. Dans cet exemple, il y a une seule phase MapReduce. Enfin l'appel de la m\u00e9thode run pour d\u00e9clencher L'ex\u00e9cution du Job MapReduce. Dans ce qui suit le contenu du Notebook Jupyter avec le code \u00e0 tester.","title":"Programmation avec MRJob"},{"location":"mr.html#exercice","text":"Transformer l'exemple WordCount afin d'obtenir le r\u00e9sultat tri\u00e9 dans l'ordre d\u00e9croissant du nombre d'occurrences. (Hint : exploiter la phase sort and shuffle qui trie la cl\u00e9 selon l'ordre alphab\u00e9tique). Sur les donn\u00e9es de la production p\u00e9troli\u00e8re de la section pr\u00e9c\u00e9dente et en utilisant MapReduce : calculer la production annuelle de chaque champ. Trouver le champ avec la plus grande production par mois.","title":"Exercice"}]}