{"config":{"indexing":"full","lang":["en","fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"index.html","text":"D\u00e9buter avec l'\u00e9cosyst\u00e8me Hadoop \u00b6 Objectifs \u00b6 G\u00e9rer et v\u00e9rifier les services de Hadoop Manipuler le syst\u00e8me de fichiers HDFS Comprendre et cr\u00e9er des programmes MapReduce simples Cr\u00e9er des programmes MapReduce \u00e0 plusieurs phases Transformer et Interroger les donn\u00e9es avec Pig et Hive D\u00e9ployer un cluster Hadoop Ressources \u00b6 Outils Oracle Virtual Box V6.1 (ou VMware) Image de machine virtuelle avec les outils pr\u00e9-install\u00e9s. C'est une image cr\u00e9\u00e9e par Pierre Nezric \u00e0 laquelle sont ajout\u00e9s jupyter et mrjob . Elle contient les outils suivants : Hadoop 2.7.3 Spark 2.1.1 Pig 0.15.0 Hive 1.2.1 HBase 1.1.9 Cassandra Elasticsearch et Kibana Zookeeper 3.4.6 Sources et r\u00e9f\u00e9rence Documentation Hadoop Big Data Analytics with Hadoop 3, Sridhar Alla, Packt Publishing, 2018. Pr\u00e9sentation de Hadoop \u00b6 R\u00f4le \u00b6 Hadoop est un framework pour le stockage et le traitement distribu\u00e9 de grands volumes de donn\u00e9es sur des clusters d'ordinateurs \u00e0 l'aide de mod\u00e8les de programmation simples. Il permet de passer de n\u0153uds uniques \u00e0 des milliers de machines, chacune offrant un calcul et un stockage locaux. Hadoop garantit une haute disponibilit\u00e9 en d\u00e9tectant et traitant les pannes au niveau de la couche application. Hadoop supporte la scalabilit\u00e9 horizontale et verticale. Il est la plateforme Big Data de r\u00e9f\u00e9rence. Historique et versions \u00b6 Les principales nouveaut\u00e9s des versions majeures par rapport aux versions pr\u00e9c\u00e9dentes sont : Version 2 : YARN : la gestion des ressources du cluster. Version 3 : Erasure Coding : tol\u00e9rance aux fautes par bloc de parit\u00e9. Haute disponibilit\u00e9 am\u00e9lior\u00e9e avec plusieurs NameNode secondaires (en standby) Comment obtenir Hadoop ? Hadoop est projet Open Source avec la licence Apache V2. Mais il existe aussi des distributions commerciales offertes par des fournisseurs avec des outils suppl\u00e9mentaires pour former une plateforme de Big Data Analytics. Historiquement, les leaders sont Cloudera, Hortonworks et MapR. Mais ces derni\u00e8res ann\u00e9es, plusieurs acquisitions et fusions ont \u00e9t\u00e9 effectu\u00e9es. Les principales alternatives actuelles pour obtenir Hadoop sont : Apache Hadoop : C'est la version Open Source. Plus difficile \u00e0 g\u00e9rer pour composer un \u00e9cosyst\u00e8me complet malgr\u00e9 que la majorit\u00e9 des composants sont Open Source aussi (Spark, Flink, Hive, Hbase, ...). Distribution Hadoop : o\u00f9 une pile de composants sont pr\u00e9-install\u00e9 avec des outils de gestion et d'administration int\u00e9gr\u00e9s (Ambari, Clouder Manager, ...). Parmi ces distributions on trouve : Cloudera : apr\u00e8s la fusion, en 2018, avec son concurrent direct Hortonworks il a h\u00e9rit\u00e9 de ces produits HDP (Hortonworks Data Platform) et HDF (Hortonworks Data Flow) sous forme de machines virtuelles (VMware, VirtualBox et Docker) et CDP (Cloudera Data Platform) en Cloud. Hewlett Packard Enterprise : qui h\u00e9rite de la plateforme MapR Data Platform apr\u00e8s son acquisition en 2018. Elle est rebaptis\u00e9e sous le nom HPE Ezmeral Data Fabric V6.2 (voir comment l'installer ici ). Elle est compatible avec Ubuntu, RedHat/CentOS et SUSE. Un version l\u00e9g\u00e8re pour le d\u00e9veloppement et test est aussi offerte sous la forme d'un container Docker ( Development Environment for HPE Ezmeral Data Fabric ). IBM Open Platform: IOP ou IBM BigInsights est la distribution d'IBM disponible en version de production ou de test ( Quick Start Edition ). \u00c9quipement d\u00e9di\u00e9 : Mat\u00e9riel optimis\u00e9 pour Hadoop comme : Dell, EMC, Teradata Appliance for Hadoop, HP, Oracle, ... Hadoop sur le Cloud: comme PaaS (Platform as a Service) tel que Amazon EMR, Microsoft HDInsight, Google Cloud Platform, Qubole, IBM BigInsights, ... Cloudera Data Platform Pour tester une liste plus \u00e9tendue d'outils de l'\u00e9cosyst\u00e8me Hadoop, je vous recommance la distribution Cloudera Data Platform 3.0 ou 2.6. Il faut lui pr\u00e9voir plus de ressources RAM et disque. Composants du noyau Hadoop \u00b6 3 composants principaux sont au c\u0153ur de Hadoop : HDFS (Hadoop Distributed File System) \u00b6 C'est le syst\u00e8me de fichier primaire de Hadoop. Il permet le stockage de larges volumes de donn\u00e9es sur des unit\u00e9s de stockage assez basique et abordable. Les donn\u00e9es sont partitionn\u00e9es et r\u00e9pliqu\u00e9es pour garantir la fiabilit\u00e9 et un acc\u00e8s parall\u00e8le. Il op\u00e8re selon le mod\u00e8le ma\u00eetre-esclave form\u00e9 respectivement par les n\u0153uds NameNode et DataNode. MapReduce \u00b6 C'est la couche de traitement dans Hadoop. Elle traite des volumes importants de donn\u00e9es structur\u00e9es et non structur\u00e9es stock\u00e9es dans HDFS. MapReduce traite \u00e9galement une \u00e9norme quantit\u00e9 de donn\u00e9es en parall\u00e8le. Pour ce faire, il divise le travail en un ensemble de t\u00e2ches ind\u00e9pendantes selon le principe divisier et conqu\u00e9rir . MapReduce fonctionne en divisant le traitement en phases: Map et Reduce. YARN (Yet Another Resource Negotiator) \u00b6 Il s'occupe de la gestion et la surveillance des travaux. YARN permet plusieurs moteurs de traitement de donn\u00e9es tels que le streaming en temps r\u00e9el, le traitement par lots, etc. Le Resource Manager est le composant au niveau de la machine ma\u00eetre. Il g\u00e8re les ressources et planifie les applications s'ex\u00e9cutant sur YARN. Il a deux composants: Scheduler & Application Manager. Tandis que le Node Manager, au niveau du n\u0153ud, communique en permanence avec Resource Manager et assure l'ex\u00e9cution des t\u00e2ches. Pr\u00e9paration de la machine virtuelle \u00b6 En utilisant VirtualBox et \u00e0 partir du menu Fichier -> Importer un appareil virtuel, s\u00e9lectionner le fichier BDTools.ova fourni et choisissez l'emplacement de destination. Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te. D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur uti . Le mot de passe =uti= . Par d\u00e9faut, les services actifs sont HDFS, YARN et HBase. La commande suivante permet de s\u00e9lectionner les services et activer/d\u00e9sactiver pour optimiser la gestion de la m\u00e9moire : sudo SelectService hadoop | spark | cassandra | elasticsearch Lancer le navigateur et aller sur la page http://localhost o\u00f9 l'\u00e9tat des services est affich\u00e9 :","title":"Introduction"},{"location":"index.html#debuter_avec_lecosysteme_hadoop","text":"","title":"D\u00e9buter avec l'\u00e9cosyst\u00e8me Hadoop"},{"location":"index.html#objectifs","text":"G\u00e9rer et v\u00e9rifier les services de Hadoop Manipuler le syst\u00e8me de fichiers HDFS Comprendre et cr\u00e9er des programmes MapReduce simples Cr\u00e9er des programmes MapReduce \u00e0 plusieurs phases Transformer et Interroger les donn\u00e9es avec Pig et Hive D\u00e9ployer un cluster Hadoop","title":"Objectifs"},{"location":"index.html#ressources","text":"Outils Oracle Virtual Box V6.1 (ou VMware) Image de machine virtuelle avec les outils pr\u00e9-install\u00e9s. C'est une image cr\u00e9\u00e9e par Pierre Nezric \u00e0 laquelle sont ajout\u00e9s jupyter et mrjob . Elle contient les outils suivants : Hadoop 2.7.3 Spark 2.1.1 Pig 0.15.0 Hive 1.2.1 HBase 1.1.9 Cassandra Elasticsearch et Kibana Zookeeper 3.4.6 Sources et r\u00e9f\u00e9rence Documentation Hadoop Big Data Analytics with Hadoop 3, Sridhar Alla, Packt Publishing, 2018.","title":"Ressources"},{"location":"index.html#presentation_de_hadoop","text":"","title":"Pr\u00e9sentation de Hadoop"},{"location":"index.html#role","text":"Hadoop est un framework pour le stockage et le traitement distribu\u00e9 de grands volumes de donn\u00e9es sur des clusters d'ordinateurs \u00e0 l'aide de mod\u00e8les de programmation simples. Il permet de passer de n\u0153uds uniques \u00e0 des milliers de machines, chacune offrant un calcul et un stockage locaux. Hadoop garantit une haute disponibilit\u00e9 en d\u00e9tectant et traitant les pannes au niveau de la couche application. Hadoop supporte la scalabilit\u00e9 horizontale et verticale. Il est la plateforme Big Data de r\u00e9f\u00e9rence.","title":"R\u00f4le"},{"location":"index.html#historique_et_versions","text":"Les principales nouveaut\u00e9s des versions majeures par rapport aux versions pr\u00e9c\u00e9dentes sont : Version 2 : YARN : la gestion des ressources du cluster. Version 3 : Erasure Coding : tol\u00e9rance aux fautes par bloc de parit\u00e9. Haute disponibilit\u00e9 am\u00e9lior\u00e9e avec plusieurs NameNode secondaires (en standby) Comment obtenir Hadoop ? Hadoop est projet Open Source avec la licence Apache V2. Mais il existe aussi des distributions commerciales offertes par des fournisseurs avec des outils suppl\u00e9mentaires pour former une plateforme de Big Data Analytics. Historiquement, les leaders sont Cloudera, Hortonworks et MapR. Mais ces derni\u00e8res ann\u00e9es, plusieurs acquisitions et fusions ont \u00e9t\u00e9 effectu\u00e9es. Les principales alternatives actuelles pour obtenir Hadoop sont : Apache Hadoop : C'est la version Open Source. Plus difficile \u00e0 g\u00e9rer pour composer un \u00e9cosyst\u00e8me complet malgr\u00e9 que la majorit\u00e9 des composants sont Open Source aussi (Spark, Flink, Hive, Hbase, ...). Distribution Hadoop : o\u00f9 une pile de composants sont pr\u00e9-install\u00e9 avec des outils de gestion et d'administration int\u00e9gr\u00e9s (Ambari, Clouder Manager, ...). Parmi ces distributions on trouve : Cloudera : apr\u00e8s la fusion, en 2018, avec son concurrent direct Hortonworks il a h\u00e9rit\u00e9 de ces produits HDP (Hortonworks Data Platform) et HDF (Hortonworks Data Flow) sous forme de machines virtuelles (VMware, VirtualBox et Docker) et CDP (Cloudera Data Platform) en Cloud. Hewlett Packard Enterprise : qui h\u00e9rite de la plateforme MapR Data Platform apr\u00e8s son acquisition en 2018. Elle est rebaptis\u00e9e sous le nom HPE Ezmeral Data Fabric V6.2 (voir comment l'installer ici ). Elle est compatible avec Ubuntu, RedHat/CentOS et SUSE. Un version l\u00e9g\u00e8re pour le d\u00e9veloppement et test est aussi offerte sous la forme d'un container Docker ( Development Environment for HPE Ezmeral Data Fabric ). IBM Open Platform: IOP ou IBM BigInsights est la distribution d'IBM disponible en version de production ou de test ( Quick Start Edition ). \u00c9quipement d\u00e9di\u00e9 : Mat\u00e9riel optimis\u00e9 pour Hadoop comme : Dell, EMC, Teradata Appliance for Hadoop, HP, Oracle, ... Hadoop sur le Cloud: comme PaaS (Platform as a Service) tel que Amazon EMR, Microsoft HDInsight, Google Cloud Platform, Qubole, IBM BigInsights, ... Cloudera Data Platform Pour tester une liste plus \u00e9tendue d'outils de l'\u00e9cosyst\u00e8me Hadoop, je vous recommance la distribution Cloudera Data Platform 3.0 ou 2.6. Il faut lui pr\u00e9voir plus de ressources RAM et disque.","title":"Historique et versions"},{"location":"index.html#composants_du_noyau_hadoop","text":"3 composants principaux sont au c\u0153ur de Hadoop :","title":"Composants  du noyau Hadoop"},{"location":"index.html#hdfs_hadoop_distributed_file_system","text":"C'est le syst\u00e8me de fichier primaire de Hadoop. Il permet le stockage de larges volumes de donn\u00e9es sur des unit\u00e9s de stockage assez basique et abordable. Les donn\u00e9es sont partitionn\u00e9es et r\u00e9pliqu\u00e9es pour garantir la fiabilit\u00e9 et un acc\u00e8s parall\u00e8le. Il op\u00e8re selon le mod\u00e8le ma\u00eetre-esclave form\u00e9 respectivement par les n\u0153uds NameNode et DataNode.","title":"HDFS (Hadoop Distributed File System)"},{"location":"index.html#mapreduce","text":"C'est la couche de traitement dans Hadoop. Elle traite des volumes importants de donn\u00e9es structur\u00e9es et non structur\u00e9es stock\u00e9es dans HDFS. MapReduce traite \u00e9galement une \u00e9norme quantit\u00e9 de donn\u00e9es en parall\u00e8le. Pour ce faire, il divise le travail en un ensemble de t\u00e2ches ind\u00e9pendantes selon le principe divisier et conqu\u00e9rir . MapReduce fonctionne en divisant le traitement en phases: Map et Reduce.","title":"MapReduce"},{"location":"index.html#yarn_yet_another_resource_negotiator","text":"Il s'occupe de la gestion et la surveillance des travaux. YARN permet plusieurs moteurs de traitement de donn\u00e9es tels que le streaming en temps r\u00e9el, le traitement par lots, etc. Le Resource Manager est le composant au niveau de la machine ma\u00eetre. Il g\u00e8re les ressources et planifie les applications s'ex\u00e9cutant sur YARN. Il a deux composants: Scheduler & Application Manager. Tandis que le Node Manager, au niveau du n\u0153ud, communique en permanence avec Resource Manager et assure l'ex\u00e9cution des t\u00e2ches.","title":"YARN (Yet Another Resource Negotiator)"},{"location":"index.html#preparation_de_la_machine_virtuelle","text":"En utilisant VirtualBox et \u00e0 partir du menu Fichier -> Importer un appareil virtuel, s\u00e9lectionner le fichier BDTools.ova fourni et choisissez l'emplacement de destination. Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te. D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur uti . Le mot de passe =uti= . Par d\u00e9faut, les services actifs sont HDFS, YARN et HBase. La commande suivante permet de s\u00e9lectionner les services et activer/d\u00e9sactiver pour optimiser la gestion de la m\u00e9moire : sudo SelectService hadoop | spark | cassandra | elasticsearch Lancer le navigateur et aller sur la page http://localhost o\u00f9 l'\u00e9tat des services est affich\u00e9 :","title":"Pr\u00e9paration de la machine virtuelle"},{"location":"cluster.html","text":"Hadoop en mode cluster \u00b6 Virtualisation avec Docker \u00b6 Utiliser plusieurs machines virtuelles requiert plus des ressources. Pour cela, nous allons utiliser des containers Docker. Le sc\u00e9nario consiste \u00e0 d\u00e9ployer 4 n\u0153uds hadoop \u00e0 partir d'une image Docker fournie ici . Cette image est similaire \u00e0 celle utilis\u00e9e avec VirtualBox : elle contient la verion 3.2.1 de hadoop, mrjob, python3 et jupyter. D\u00e9ploiement avec Docker \u00b6 Charger l'image docker Soit \u00e0 partir du fichier fourni : docker load < hadoop-3.2.1-docker.tar.gz Ou \u00e0 partir du Docker Hub : docker pull hhmida/hadoop:3.2.1 Cr\u00e9er un r\u00e9seau virtuel : docker network create --driver bridge hadoopnet Ex\u00e9cuter 4 instance de l'image avec les noms nodemaster node2, node3, et node4 docker run -d --network hadoopnet --name nodemaster -it -h nodemaster -p 8088 :8088 -p 9870 :9870 -p 9864 :9864 -p 19888 :19888 -p 8042 :8042 -p 8888 :8888 hadoop:3.2.1 docker run -dP --network hadoopnet --name node2 -it -h node2 hadoop:3.2.1 docker run -dP --network hadoopnet --name node3 -it -h node3 hadoop:3.2.1 docker run -dP --network hadoopnet --name node4 -it -h node4 hadoop:3.2.1 Image t\u00e9l\u00e9charg\u00e9e depuis Docker Hub Si vous avez t\u00e9l\u00e9charger l'image avec la commande docker pull alors remplacer hadoop:3.2.1 par hhmida/hadoop:3.2.1 . Formater le nodemaster : docker exec -u hadoop -it nodemaster hadoop/bin/hdfs namenode -format D\u00e9marrage du cluster \u00b6 D\u00e9marrer les containers docker start nodemaster node2 node3 node4 D\u00e9marrer les services hadoop Ex\u00e9cuter ces 2 commandes pour d\u00e9marrer les services \u00e0 partir du nodemaster : docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-dfs.sh docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-yarn.sh cr\u00e9er le dossier racine sur HDFS docker exec -u hadoop -it nodemaster /home/hadoop/hadoop/bin/hadoop fs -mkdir -p . D\u00e9marrer jupyter docker exec -u hadoop -d nodemaster jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = '' Arr\u00eat des n\u0153uds \u00b6 docker stop nodemaster node2 node3 node4 Red\u00e9marrage du cluster Pour red\u00e9marrer le cluster ex\u00e9cuter les \u00e9tapes de la section D\u00e9marrage du cluster . V\u00e9rification de l'\u00e9tat du cluster \u00b6 Avec la commande jps : Au niveau du nodemaster Au niveau de node2 et node3 Acc\u00e9der aux URLs suivantes pour v\u00e9rifier l'\u00e9tat et la configuration du cluster Cluster Hadoop : http://localhost:8088 HDFS : http://localhost:9870 Jupyter notebook : http://localhost:8888 L'interface du Resource Manager montre les n&339;uds : L'interface HDFS montre les n\u0153uds DataNodes actifs : Remarquer la colonne Last contact qui refl\u00e8te le dernier heartbeaz re\u00e7u (inf\u00e9rieur au timeout par d\u00e9faut : 3 Secondes). Maintenant, arr\u00eater le n\u0153ud node3 : docker stop node4 Apr\u00e8s 1000 Secondes le n\u0153ud est consid\u00e9r\u00e9 comme indisponible : Virtualisation avec machines virtuelles \u00b6 Cr\u00e9er un cluster avec la machine virtuelle est plus simple mais n\u00e9cessite plus de ressources. En effet, dans notre exemple et pour cr\u00e9er un cluster compos\u00e9 d'un ma\u00eetre et 2 workers, vous devez disposer de 8GO de RAM au minimum dont 6GO pour les machines virtuelles. Modifications sur la machine virtuelle originale \u00b6 Changer le nom de la machine dans le fichier /etc/hostname en nodemaster sudo echo \"nodemaster\" > /etc/hostname Modifier le fichier /etc/hosts : sudo nano /etc/hosts Puis \u00e9crire les lignes suivantes et enregistrer : 192 .168.100.10 nodemaster 192 .168.100.20 node2 192 .168.100.30 mode3 Modifier les fichiers de configuration de Hadoop Modifier le fichier /ur/local/hadoop/etc/hadoop/core-site.xml ainsi : <property> <name> fs.defaultFS </name> <value> hdfs://nodemaster:9000 </value> </property> Changer la replication dans /usr/local/hadoop/etc/hadoop/hdfs-site.xml <property> <name> dfs.replication </name> <value> 3 </value> </property> Ajouter cette propri\u00e9t\u00e9 dans /usr/local/hadoop/etc/hadoop/yarn-site.xml <property> <name> yarn.resourcemanager.hostname </name> <value> nodemaster </value> </property> Le fichier /usr/local/hadoop/etc/hadoop/workers contient les noms des machines du cluster, il sera utilis\u00e9 pour d\u00e9marrer les services depuis le n\u0153ud ma\u00eetre. nodemaster node2 node3 Workers nodemaster dans cette configuration est consid\u00e9r\u00e9 comme namenode et datanode \u00e0 la fois. Clonage de la machine virtuelle \u00b6 Cr\u00e9er 2 clones de la machine virtuelle originale qui vont \u00eatre respectivement node2 et node3. Chager leurs noms dans /etc/hostname respectivement en node2 et node3 Sur la machine node2 : sudo echo \"node2\" > /etc/hostname Sur la machine node3 : sudo echo \"node3\" > /etc/hostname Changer les adresses IP en 192.168.100.20 et 192.168.100.30 en suivant les \u00e9tapes suivantes (refaire les m\u00eames \u00e9tapes pour node3) : sudo nano /etc/netplan/00-installer-config.yaml Puis appliquer les modifications : sudo netplan apply Red\u00e9marrer les 3 machines et d\u00e9marrer les services depuis le nodemaster : start-dfs.sh start-yarn.sh V\u00e9rifier avec jps ou les URLs http://192.168.100.10:9870 et http://192.168.100.10:8088 Test de l'arr\u00eat d'un n\u0153ud : Voir la section pour l'exemple r\u00e9alis\u00e9 avec les containers Docker ( ici ). Test de Map Reduce sur le cluster \u00b6 Refaire l'exemple wordcout (voir ici ): Mettre le fichier shakespeare.txt sur HDFS : hadoop fs -put shakespeare.txt Lancer Jupyter jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = '' Se connecter \u00e0 http://192.168.100.10:8888 , cr\u00e9er le notebook et ajouter le code de l'exemple wordcount : Pendant l'ex\u00e9cution, vous pouvez visualiser l'\u00e9tat des ressources allou\u00e9es en cliquant sur Scheduler sur http://192.168.100.10:8088 Apr\u00e8s l'ex\u00e9cution r\u00e9ussie sur le cluster, cliquer sur Applications pour v\u00e9rifier l'\u00e9tat de l'application. Pour afficher le d\u00e9tail des t\u00e2ches, il est possible de voir le Job History. D\u00e9marrer le serveur job History et acc\u00e9der \u00e0 son interface web. D\u00e9marrer le serveur mapred jobhistoryserver start Acc\u00e8s \u00e0 l'interface web : http://192.168.100.10:19888 En cliquant sur le Job ID, le nombre de t\u00e2ches Map et Reduce est affich\u00e9 (dans ce cas 2 Maps et 1 Reduce) Et puis chacune des t\u00e2ches : La premi\u00e8re t\u00e2che Map La seconde t\u00e2che Map La t\u00e2che Reduce","title":"Hadoop en mode cluster"},{"location":"cluster.html#hadoop_en_mode_cluster","text":"","title":"Hadoop en mode cluster"},{"location":"cluster.html#virtualisation_avec_docker","text":"Utiliser plusieurs machines virtuelles requiert plus des ressources. Pour cela, nous allons utiliser des containers Docker. Le sc\u00e9nario consiste \u00e0 d\u00e9ployer 4 n\u0153uds hadoop \u00e0 partir d'une image Docker fournie ici . Cette image est similaire \u00e0 celle utilis\u00e9e avec VirtualBox : elle contient la verion 3.2.1 de hadoop, mrjob, python3 et jupyter.","title":"Virtualisation avec Docker"},{"location":"cluster.html#deploiement_avec_docker","text":"Charger l'image docker Soit \u00e0 partir du fichier fourni : docker load < hadoop-3.2.1-docker.tar.gz Ou \u00e0 partir du Docker Hub : docker pull hhmida/hadoop:3.2.1 Cr\u00e9er un r\u00e9seau virtuel : docker network create --driver bridge hadoopnet Ex\u00e9cuter 4 instance de l'image avec les noms nodemaster node2, node3, et node4 docker run -d --network hadoopnet --name nodemaster -it -h nodemaster -p 8088 :8088 -p 9870 :9870 -p 9864 :9864 -p 19888 :19888 -p 8042 :8042 -p 8888 :8888 hadoop:3.2.1 docker run -dP --network hadoopnet --name node2 -it -h node2 hadoop:3.2.1 docker run -dP --network hadoopnet --name node3 -it -h node3 hadoop:3.2.1 docker run -dP --network hadoopnet --name node4 -it -h node4 hadoop:3.2.1 Image t\u00e9l\u00e9charg\u00e9e depuis Docker Hub Si vous avez t\u00e9l\u00e9charger l'image avec la commande docker pull alors remplacer hadoop:3.2.1 par hhmida/hadoop:3.2.1 . Formater le nodemaster : docker exec -u hadoop -it nodemaster hadoop/bin/hdfs namenode -format","title":"D\u00e9ploiement avec Docker"},{"location":"cluster.html#demarrage_du_cluster","text":"D\u00e9marrer les containers docker start nodemaster node2 node3 node4 D\u00e9marrer les services hadoop Ex\u00e9cuter ces 2 commandes pour d\u00e9marrer les services \u00e0 partir du nodemaster : docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-dfs.sh docker exec -u hadoop -d nodemaster /home/hadoop/hadoop/sbin/start-yarn.sh cr\u00e9er le dossier racine sur HDFS docker exec -u hadoop -it nodemaster /home/hadoop/hadoop/bin/hadoop fs -mkdir -p . D\u00e9marrer jupyter docker exec -u hadoop -d nodemaster jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = ''","title":"D\u00e9marrage du cluster"},{"location":"cluster.html#arret_des_nuds","text":"docker stop nodemaster node2 node3 node4 Red\u00e9marrage du cluster Pour red\u00e9marrer le cluster ex\u00e9cuter les \u00e9tapes de la section D\u00e9marrage du cluster .","title":"Arr\u00eat des n\u0153uds"},{"location":"cluster.html#verification_de_letat_du_cluster","text":"Avec la commande jps : Au niveau du nodemaster Au niveau de node2 et node3 Acc\u00e9der aux URLs suivantes pour v\u00e9rifier l'\u00e9tat et la configuration du cluster Cluster Hadoop : http://localhost:8088 HDFS : http://localhost:9870 Jupyter notebook : http://localhost:8888 L'interface du Resource Manager montre les n&339;uds : L'interface HDFS montre les n\u0153uds DataNodes actifs : Remarquer la colonne Last contact qui refl\u00e8te le dernier heartbeaz re\u00e7u (inf\u00e9rieur au timeout par d\u00e9faut : 3 Secondes). Maintenant, arr\u00eater le n\u0153ud node3 : docker stop node4 Apr\u00e8s 1000 Secondes le n\u0153ud est consid\u00e9r\u00e9 comme indisponible :","title":"V\u00e9rification de l'\u00e9tat du cluster"},{"location":"cluster.html#virtualisation_avec_machines_virtuelles","text":"Cr\u00e9er un cluster avec la machine virtuelle est plus simple mais n\u00e9cessite plus de ressources. En effet, dans notre exemple et pour cr\u00e9er un cluster compos\u00e9 d'un ma\u00eetre et 2 workers, vous devez disposer de 8GO de RAM au minimum dont 6GO pour les machines virtuelles.","title":"Virtualisation avec machines virtuelles"},{"location":"cluster.html#modifications_sur_la_machine_virtuelle_originale","text":"Changer le nom de la machine dans le fichier /etc/hostname en nodemaster sudo echo \"nodemaster\" > /etc/hostname Modifier le fichier /etc/hosts : sudo nano /etc/hosts Puis \u00e9crire les lignes suivantes et enregistrer : 192 .168.100.10 nodemaster 192 .168.100.20 node2 192 .168.100.30 mode3 Modifier les fichiers de configuration de Hadoop Modifier le fichier /ur/local/hadoop/etc/hadoop/core-site.xml ainsi : <property> <name> fs.defaultFS </name> <value> hdfs://nodemaster:9000 </value> </property> Changer la replication dans /usr/local/hadoop/etc/hadoop/hdfs-site.xml <property> <name> dfs.replication </name> <value> 3 </value> </property> Ajouter cette propri\u00e9t\u00e9 dans /usr/local/hadoop/etc/hadoop/yarn-site.xml <property> <name> yarn.resourcemanager.hostname </name> <value> nodemaster </value> </property> Le fichier /usr/local/hadoop/etc/hadoop/workers contient les noms des machines du cluster, il sera utilis\u00e9 pour d\u00e9marrer les services depuis le n\u0153ud ma\u00eetre. nodemaster node2 node3 Workers nodemaster dans cette configuration est consid\u00e9r\u00e9 comme namenode et datanode \u00e0 la fois.","title":"Modifications sur la machine virtuelle originale"},{"location":"cluster.html#clonage_de_la_machine_virtuelle","text":"Cr\u00e9er 2 clones de la machine virtuelle originale qui vont \u00eatre respectivement node2 et node3. Chager leurs noms dans /etc/hostname respectivement en node2 et node3 Sur la machine node2 : sudo echo \"node2\" > /etc/hostname Sur la machine node3 : sudo echo \"node3\" > /etc/hostname Changer les adresses IP en 192.168.100.20 et 192.168.100.30 en suivant les \u00e9tapes suivantes (refaire les m\u00eames \u00e9tapes pour node3) : sudo nano /etc/netplan/00-installer-config.yaml Puis appliquer les modifications : sudo netplan apply Red\u00e9marrer les 3 machines et d\u00e9marrer les services depuis le nodemaster : start-dfs.sh start-yarn.sh V\u00e9rifier avec jps ou les URLs http://192.168.100.10:9870 et http://192.168.100.10:8088 Test de l'arr\u00eat d'un n\u0153ud : Voir la section pour l'exemple r\u00e9alis\u00e9 avec les containers Docker ( ici ).","title":"Clonage de la machine virtuelle"},{"location":"cluster.html#test_de_map_reduce_sur_le_cluster","text":"Refaire l'exemple wordcout (voir ici ): Mettre le fichier shakespeare.txt sur HDFS : hadoop fs -put shakespeare.txt Lancer Jupyter jupyter notebook --ip = 0 .0.0.0 --port = 8888 --notebook-dir = '/home/hadoop' --NotebookApp.token = '' --NotebookApp.password = '' Se connecter \u00e0 http://192.168.100.10:8888 , cr\u00e9er le notebook et ajouter le code de l'exemple wordcount : Pendant l'ex\u00e9cution, vous pouvez visualiser l'\u00e9tat des ressources allou\u00e9es en cliquant sur Scheduler sur http://192.168.100.10:8088 Apr\u00e8s l'ex\u00e9cution r\u00e9ussie sur le cluster, cliquer sur Applications pour v\u00e9rifier l'\u00e9tat de l'application. Pour afficher le d\u00e9tail des t\u00e2ches, il est possible de voir le Job History. D\u00e9marrer le serveur job History et acc\u00e9der \u00e0 son interface web. D\u00e9marrer le serveur mapred jobhistoryserver start Acc\u00e8s \u00e0 l'interface web : http://192.168.100.10:19888 En cliquant sur le Job ID, le nombre de t\u00e2ches Map et Reduce est affich\u00e9 (dans ce cas 2 Maps et 1 Reduce) Et puis chacune des t\u00e2ches : La premi\u00e8re t\u00e2che Map La seconde t\u00e2che Map La t\u00e2che Reduce","title":"Test de Map Reduce sur le cluster"},{"location":"hdfs.html","text":"Manipulation de HDFS \u00b6 Service HDFS \u00b6 D\u00e9marrer/Arr\u00eater \u00b6 Service HDFS Pour g\u00e9rer les services, il est recommand\u00e9 d'utiliser le script SelectService . Mais pour les g\u00e9rer individuellement il est possible d'utiliser les commandes suivantes : D\u00e9marrage sudo service hadoop-hdfs-namenode start sudo service hadoop-hdfs-datanode start Arr\u00eat sudo service hadoop-hdfs-namenode stop sudo service hadoop-hdfs-datanode stop La commande sudo jps permet de v\u00e9rifier que deux processus sont lanc\u00e9s : NameNode et DataNode. Avec la version 3, un processus SecondaryNameNode est aussi lanc\u00e9. HDFS est maintenant accessible via l'interface web : http://localhost:50070 Cette interface permet d'afficher l'\u00e9tat de HDFS et ses diffrents datanodes. Il est possible d'explorer le contenu du syst\u00e8me de fichiers \u00e0 partir du menu Utilities puis Browse the file system . Formatage \u00b6 hdfs namenode -format Attention L'op\u00e9ration de formatage supprime tous les fichiers. Elle est effectu\u00e9e lors de l'installation de Hadoop ou pour r\u00e9initialiser HDFS. Commandes HDFS \u00b6 Le syst\u00e8me HDFS est manipul\u00e9 \u00e0 travers des commandes inspir\u00e9es du syst\u00e8me Linux. La forme g\u00e9n\u00e9rale de ces commandes est comme suit : Format des commandes HDFS hadoop fs -nomCommande -options param1 ... Affichage du contenu d'un dossier \u00b6 hadoop fs -ls Dossier par d\u00e9faut Pour les chemins relatifs HDFS utilise /user/nom_utilisateur comme racine ou nom_utilisateur est l'utilisateur connect\u00e9. Si le dossier /user/<nom utilisateur> n'est pas cr\u00e9\u00e9, cette commande provoque une erreur. Dans notre cas, c'est le dossier /user/uti qui doit \u00eatre cr\u00e9\u00e9. hadoop fs -mkdir -p . ou hadoop fs -mkdir -p /user/uti Cr\u00e9er des fichiers \u00b6 Cr\u00e9er un fichier test.txt hadoop fs -touchz test.txt Upload de fichiers ou dossiers \u00b6 Upload de fichier Le fichier exemple.txt doit \u00eatre dans le dossier en cours. hadoop fs -put exemple.txt ou hadoop fs -copyFromLocal exeple.txt Supprimer des fichiers \u00b6 Supprimer fichier test.txt hadoop fs -rm test.txt Cr\u00e9er des dossier \u00b6 Cr\u00e9er un dossier data hadoop fs -mkdir data Supprimer des dossiers \u00b6 Supprimer le dossier data hadoop fs -rmdir data Copier/d\u00e9placer des fichiers ou des dossiers \u00b6 Copier le fichier test.txt hadoop fs -cp test.txt data/copie.txt D\u00e9placer le fichier copie.txt hadoop fs -mv data/copie.txt copie2.txt Exercice \u00b6 Sur HDFS, cr\u00e9er l'arborescence tunisie/petrole. Chercher et t\u00e9l\u00e9charger, sur http://data.industrie.gov.tn , les donn\u00e9es sur la production p\u00e9troli\u00e8re mensuelle par champ dans le format CSV. Placer le fichier dans le dossier petrole cr\u00e9\u00e9 dans la premi\u00e8re \u00e9tape. \u00c0 partir de l'interface web : Chercher la taille du bloc par d\u00e9faut. Copier sur HDFS un fichier plus grand que la taille du bloc et v\u00e9rifier le nombre de blocs de ce fichier.","title":"Manipulation de HDFS"},{"location":"hdfs.html#manipulation_de_hdfs","text":"","title":"Manipulation de HDFS"},{"location":"hdfs.html#service_hdfs","text":"","title":"Service HDFS"},{"location":"hdfs.html#demarrerarreter","text":"Service HDFS Pour g\u00e9rer les services, il est recommand\u00e9 d'utiliser le script SelectService . Mais pour les g\u00e9rer individuellement il est possible d'utiliser les commandes suivantes : D\u00e9marrage sudo service hadoop-hdfs-namenode start sudo service hadoop-hdfs-datanode start Arr\u00eat sudo service hadoop-hdfs-namenode stop sudo service hadoop-hdfs-datanode stop La commande sudo jps permet de v\u00e9rifier que deux processus sont lanc\u00e9s : NameNode et DataNode. Avec la version 3, un processus SecondaryNameNode est aussi lanc\u00e9. HDFS est maintenant accessible via l'interface web : http://localhost:50070 Cette interface permet d'afficher l'\u00e9tat de HDFS et ses diffrents datanodes. Il est possible d'explorer le contenu du syst\u00e8me de fichiers \u00e0 partir du menu Utilities puis Browse the file system .","title":"D\u00e9marrer/Arr\u00eater"},{"location":"hdfs.html#formatage","text":"hdfs namenode -format Attention L'op\u00e9ration de formatage supprime tous les fichiers. Elle est effectu\u00e9e lors de l'installation de Hadoop ou pour r\u00e9initialiser HDFS.","title":"Formatage"},{"location":"hdfs.html#commandes_hdfs","text":"Le syst\u00e8me HDFS est manipul\u00e9 \u00e0 travers des commandes inspir\u00e9es du syst\u00e8me Linux. La forme g\u00e9n\u00e9rale de ces commandes est comme suit : Format des commandes HDFS hadoop fs -nomCommande -options param1 ...","title":"Commandes HDFS"},{"location":"hdfs.html#affichage_du_contenu_dun_dossier","text":"hadoop fs -ls Dossier par d\u00e9faut Pour les chemins relatifs HDFS utilise /user/nom_utilisateur comme racine ou nom_utilisateur est l'utilisateur connect\u00e9. Si le dossier /user/<nom utilisateur> n'est pas cr\u00e9\u00e9, cette commande provoque une erreur. Dans notre cas, c'est le dossier /user/uti qui doit \u00eatre cr\u00e9\u00e9. hadoop fs -mkdir -p . ou hadoop fs -mkdir -p /user/uti","title":"Affichage du contenu d'un dossier"},{"location":"hdfs.html#creer_des_fichiers","text":"Cr\u00e9er un fichier test.txt hadoop fs -touchz test.txt","title":"Cr\u00e9er des fichiers"},{"location":"hdfs.html#upload_de_fichiers_ou_dossiers","text":"Upload de fichier Le fichier exemple.txt doit \u00eatre dans le dossier en cours. hadoop fs -put exemple.txt ou hadoop fs -copyFromLocal exeple.txt","title":"Upload de fichiers ou dossiers"},{"location":"hdfs.html#supprimer_des_fichiers","text":"Supprimer fichier test.txt hadoop fs -rm test.txt","title":"Supprimer des fichiers"},{"location":"hdfs.html#creer_des_dossier","text":"Cr\u00e9er un dossier data hadoop fs -mkdir data","title":"Cr\u00e9er des dossier"},{"location":"hdfs.html#supprimer_des_dossiers","text":"Supprimer le dossier data hadoop fs -rmdir data","title":"Supprimer des dossiers"},{"location":"hdfs.html#copierdeplacer_des_fichiers_ou_des_dossiers","text":"Copier le fichier test.txt hadoop fs -cp test.txt data/copie.txt D\u00e9placer le fichier copie.txt hadoop fs -mv data/copie.txt copie2.txt","title":"Copier/d\u00e9placer des fichiers ou des dossiers"},{"location":"hdfs.html#exercice","text":"Sur HDFS, cr\u00e9er l'arborescence tunisie/petrole. Chercher et t\u00e9l\u00e9charger, sur http://data.industrie.gov.tn , les donn\u00e9es sur la production p\u00e9troli\u00e8re mensuelle par champ dans le format CSV. Placer le fichier dans le dossier petrole cr\u00e9\u00e9 dans la premi\u00e8re \u00e9tape. \u00c0 partir de l'interface web : Chercher la taille du bloc par d\u00e9faut. Copier sur HDFS un fichier plus grand que la taille du bloc et v\u00e9rifier le nombre de blocs de ce fichier.","title":"Exercice"},{"location":"hive.html","text":"Hive \u00b6 Premier exemple : Wordcount \u00b6 Nous reprenons l'exemple de comptage de mots utilis\u00e9 dans l'atelier MapReduce pour le r\u00e9soudre avec Hive. wordcount en HiveQL 1 2 3 4 5 6 CREATE EXTERNAL TABLE lines ( line STRING ); -- (1) LOAD DATA INPATH '/user/uti/shakespeare.txt' OVERWRITE INTO TABLE lines ; CREATE TABLE words AS SELECT explode ( split ( line , ' ' )) AS word FROM lines ; CREATE TABLE word_counts AS SELECT word , count ( 1 ) AS nb FROM words GROUP BY word ORDER BY nb DESC ; EXTERNAL permet d'emp\u00eacher de supprimer le fichier shakespeare.txt original apr\u00e8s son chargement avec Hive. Suivre les \u00e9tapes suivantes pour ex\u00e9cuter le cide HiveQL : Lancer la machine virtuelle et acc\u00e9der \u00e0 Shell Hive (Beehive) \u00e0 partir du terminal hive Vous aurez alors l'invite hive : hive> Maintenant taper les commandes du code fourni. Services requis pour Hive Le shell Hive a besoin des services suivants : MySQL hive-metastore hive-server2 Utiliser la commande service nom_service start pour les d\u00e9marrer en cas de besoin. Les lignes du code permettent respectivement de : Cr\u00e9er une table externe lines avec une colonne unique line . Charger le contenu du fichier shakespeare.txt \u00e0 partir de HDFS dans la table pr\u00e9c\u00e9dente. V\u00e9rifier, apr\u00e8s cette commande, la cr\u00e9ation du fichier /user/hive/warehouse/lines/shakespeare.txt . Cr\u00e9er une deuxi\u00e8me table words pour stocker le r\u00e9sultat d'une requ\u00eate pour d\u00e9composer chaque ligne (line) du texte dans la table lines en mot avec les fonctions explode et split . Cr\u00e9er une table word_counts structur\u00e9e en 2 colonnes : word pour le mot et nb pour le nombre d'occurences obtenu par le r\u00e9sultat de la fonction count \u00e0 partir de la table words . Grouper par mot. Trier par nombre d'occurences d\u00e9croissant V\u00e9rifier la cr\u00e9ation de Jobs MapReduce pour les diff\u00e9rentes requ\u00eates SELECT sur http://localhost:8088 Un dossier word_counts est cr\u00e9\u00e9 dans /user/hive/warehouse o\u00f9 des fichiers (1 dans note cas 000000-0) contiennent les nombres d'occurences associ\u00e9s aux mots du fichier. Afficher le r\u00e9sultat : hadoop fs -cat /user/hive/warehouse/word_counts/000000-0 | more HiveQL \u00b6 Pr\u00e9parer les donn\u00e9es Dans cette section, nous allons utiliser les m\u00eames donn\u00e9es de la section Pig. Si les fichiers csv ne sont pas d\u00e9j\u00e0 t\u00e9l\u00e9charg\u00e9s alors proc\u00e9der aux \u00e9tapes suivantes : Cr\u00e9er un dossier demo : mkdir ~/demo T\u00e9l\u00e9charger les fichiers csv dans /home/uti/demo : employee.csv department.csv salary.csv Envoyer les donn\u00e9es sur HDFS hadoop fs -put /home/uti/demo LDD \u00b6 Cr\u00e9er une base de donn\u00e9es demo CREATE DATABASE demo ; S\u00e9lectionner une base de donn\u00e9es USE demo ; -- La base de donn\u00e9es par d\u00e9faut est DEFAULT Cr\u00e9er les tables create table employee ( id int , fname string , lname string , department_id int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ; create table department ( id int , dept_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = \",\" , \"quoteChar\" = \"\\\" \" ) ; create external table salary ( salary_id int , employee_id int , payment double , payment_date date ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/user/uti/salary' ; Contraintes d'int\u00e9grit\u00e9 Les contraintes primary key et foreign key ne sont support\u00e9es qu'\u00e0 partir de la version 2. Les contraintes unique , not null et check sont ajout\u00e9es depuis la version3. Ajouter une colonne \u00e0 une table CREATE TABLE test ( id int ); ALTER TABLE salary ADD COLUMNS ( name varchar , dob date ); Supprimer une table DROP TABLE test ; SHOW/DESCRIBE Afficher les objets disponibles Show Databases ; Show Tables ; Show Partitions < table > ; Show TblProperties < table > ; Show Create Table < table > ; Show Indexes on < table > ; Show Columns in < table > ; Show Functions ; Show transactions ; Afficher la strucuture ou les propri\u00e9t\u00e9s d'un objet Describe database ; Describe < table > ; Describe < table . column > ; LMD \u00b6 Charger des donn\u00e9es \u00e0 partir des fichiers CSV depuis HDFS LOAD DATA INPATH '/user/uti/demo/employee.csv' INTO TABlE employee ; LOAD DATA INPATH '/user/uti/demo/department.csv' INTO TABlE department ; LOAD DATA INPATH '/user/uti/demo/salary.csv' INTO TABlE salary ; Remarque V\u00e9rifier sur HDFS l'emplacement, le type et le format des fichiers de donn\u00e9es relatifs \u00e0 ces tables. Insertion de donn\u00e9es CREATE TABLE jours ( num int , nom string ); INSERT INTO jours VALUES ( 1 , 'Lundi' ); INSERT INTO jours VALUES ( 2 , 'Mardi' ); INSERT INTO jours VALUES ( 3 , 'Mercredi' ); MapReduce La commande d'insertion g\u00e9n\u00e8re un job MapReduce que vous pouvez consulter sur YARN. Mise \u00e0 jour et suppression Pour ex\u00e9cuter ces op\u00e9rations, quelques conditions doivent \u00eatre v\u00e9rifi\u00e9es : La table utilise le format =ORC= La table supporte le Bucketing Les transactions ACID sont activ\u00e9es sur Hive Essayer de mettre \u00e0 jour la table jours : UPDATE jours set nom = upper ( nom ) WHERE num = 1 ; Une erreur est alors affich\u00e9e. Pour pouvoir ex\u00e9cuter ces requ\u00eates, nous commen\u00e7ons par activer les transactions sous Hive. Ceci est possible avec l'une des deux m\u00e9thodes : Modifier le fichier =hive-site.xml= en ajoutant les lignes suivante et puis red\u00e9marrer Hive : <property> <name> hive.support.concurrency </name> <value> true </value> </property> <property> <name> hive.txn.manager </name> <value> org.apache.hadoop.hive.ql.lockmgr.DbTxnManager </value> </property> <property> <name> hive.enforce.bucketing </name> <value> true </value> </property> <property> <name> hive.exec.dynamic.partition.mode </name> <value> nostrict </value> </property> <property> <name> hive.compactor.initiator.on </name> <value> true </value> </property> <property> <name> hive.compactor.worker.threads </name> <value> 1 </value> </property> 2. \u00c0 partir du shell hive : SET hive.support.concurrency = true ; SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager ; # Pour Hive 2.0 SET hive.enforce.bucketing = true ; SET hive.exec.dynamic.partition.mode = nostrict ; # Pour hive metastore SET hive.compactor.initiator.on = true ; SET hive.compactor.worker.threads = 1 ; Ensuite cr\u00e9er une table supportant les transactions : CREATE TABLE employee_trans ( id int , name string , age int , gender string ) clustered by ( gender ) into 2 buckets STORED AS ORC TBLPROPERTIES ( 'transactional' = 'true' ); -- ins\u00e9rer des donn\u00e9es INSERT INTO employee_trans VALUES ( 1 , 'James' , 30 , 'M' ); INSERT INTO employee_trans VALUES ( 2 , 'Ann' , 40 , 'F' ); INSERT INTO employee_trans VALUES ( 3 , 'Jeff' , 41 , 'M' ); INSERT INTO employee_trans VALUES ( 4 , 'Jennifer' , 20 , 'F' ); -- Mise \u00e0 jour UPDATE employee_trans SET age = 45 WHERE id = 3 ; -- Suppression DELETE FROM employee_trans WHERE id = 4 ; -- V\u00e9rifier les donn\u00e9es SELECT * FROM employee_trans ; SELECT \u00b6 Format de la requ\u00eate : SELECT [ ALL | DISTINCT ] select_expr , select_expr , ... FROM table_reference [ WHERE where_condition ] [ GROUP BY col_list ] [ HAVING having_condition ] [ ORDER BY col_list ] [ LIMIT [ offset ,] rows ] Reprenons les m\u00eames exemples de la section Pig. Filtrage pour avoir les employ\u00e9s (sans doublons) ayant re\u00e7u un salaire compris entre 5000 et 7000 SELECT distinct employee_id FROM salary WHERE payment >= 5000 AND payment <= 7000 ; Nombre d'employ\u00e9s par d\u00e9partement et afficher les 3 plus grands d\u00e9partements SELECT department_id , count ( id ) AS nb_emp FROM employee GROUP BY department_id ORDER BY nb_emp DESC LIMIT 3 ; Jointure : trouver le nom du d\u00e9partement de chaque employ\u00e9 (afficher les 4 premiers employ\u00e9s) SELECT fname , lname , dept_name FROM employee JOIN department ON ( employee . department_id = department . id ) limit 4 ; Requ\u00eate imbriqu\u00e9e : Trouver le nombre moyen d'employ\u00e9s par d\u00e9partement SELECT AVG ( nb_emp ) FROM ( SELECT department_id , count ( id ) AS nb_emp FROM employee GROUP BY department_id ) q1 ; R\u00e9f\u00e9rences HiveQL \u00b6 Pour une r\u00e9f\u00e9rence compl\u00e8te du langage Pi Latin, aller sur la page de documentation ( ici ) Voici aussi, un m\u00e9mo du langage : Ouvrir dans un nouvel onglet Exercice \u00b6 R\u00e9soudre les questions 2 et 3 des exercices de la section MapReduce.","title":"Hive"},{"location":"hive.html#hive","text":"","title":"Hive"},{"location":"hive.html#premier_exemple_wordcount","text":"Nous reprenons l'exemple de comptage de mots utilis\u00e9 dans l'atelier MapReduce pour le r\u00e9soudre avec Hive. wordcount en HiveQL 1 2 3 4 5 6 CREATE EXTERNAL TABLE lines ( line STRING ); -- (1) LOAD DATA INPATH '/user/uti/shakespeare.txt' OVERWRITE INTO TABLE lines ; CREATE TABLE words AS SELECT explode ( split ( line , ' ' )) AS word FROM lines ; CREATE TABLE word_counts AS SELECT word , count ( 1 ) AS nb FROM words GROUP BY word ORDER BY nb DESC ; EXTERNAL permet d'emp\u00eacher de supprimer le fichier shakespeare.txt original apr\u00e8s son chargement avec Hive. Suivre les \u00e9tapes suivantes pour ex\u00e9cuter le cide HiveQL : Lancer la machine virtuelle et acc\u00e9der \u00e0 Shell Hive (Beehive) \u00e0 partir du terminal hive Vous aurez alors l'invite hive : hive> Maintenant taper les commandes du code fourni. Services requis pour Hive Le shell Hive a besoin des services suivants : MySQL hive-metastore hive-server2 Utiliser la commande service nom_service start pour les d\u00e9marrer en cas de besoin. Les lignes du code permettent respectivement de : Cr\u00e9er une table externe lines avec une colonne unique line . Charger le contenu du fichier shakespeare.txt \u00e0 partir de HDFS dans la table pr\u00e9c\u00e9dente. V\u00e9rifier, apr\u00e8s cette commande, la cr\u00e9ation du fichier /user/hive/warehouse/lines/shakespeare.txt . Cr\u00e9er une deuxi\u00e8me table words pour stocker le r\u00e9sultat d'une requ\u00eate pour d\u00e9composer chaque ligne (line) du texte dans la table lines en mot avec les fonctions explode et split . Cr\u00e9er une table word_counts structur\u00e9e en 2 colonnes : word pour le mot et nb pour le nombre d'occurences obtenu par le r\u00e9sultat de la fonction count \u00e0 partir de la table words . Grouper par mot. Trier par nombre d'occurences d\u00e9croissant V\u00e9rifier la cr\u00e9ation de Jobs MapReduce pour les diff\u00e9rentes requ\u00eates SELECT sur http://localhost:8088 Un dossier word_counts est cr\u00e9\u00e9 dans /user/hive/warehouse o\u00f9 des fichiers (1 dans note cas 000000-0) contiennent les nombres d'occurences associ\u00e9s aux mots du fichier. Afficher le r\u00e9sultat : hadoop fs -cat /user/hive/warehouse/word_counts/000000-0 | more","title":"Premier exemple : Wordcount"},{"location":"hive.html#hiveql","text":"Pr\u00e9parer les donn\u00e9es Dans cette section, nous allons utiliser les m\u00eames donn\u00e9es de la section Pig. Si les fichiers csv ne sont pas d\u00e9j\u00e0 t\u00e9l\u00e9charg\u00e9s alors proc\u00e9der aux \u00e9tapes suivantes : Cr\u00e9er un dossier demo : mkdir ~/demo T\u00e9l\u00e9charger les fichiers csv dans /home/uti/demo : employee.csv department.csv salary.csv Envoyer les donn\u00e9es sur HDFS hadoop fs -put /home/uti/demo","title":"HiveQL"},{"location":"hive.html#ldd","text":"Cr\u00e9er une base de donn\u00e9es demo CREATE DATABASE demo ; S\u00e9lectionner une base de donn\u00e9es USE demo ; -- La base de donn\u00e9es par d\u00e9faut est DEFAULT Cr\u00e9er les tables create table employee ( id int , fname string , lname string , department_id int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ; create table department ( id int , dept_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = \",\" , \"quoteChar\" = \"\\\" \" ) ; create external table salary ( salary_id int , employee_id int , payment double , payment_date date ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/user/uti/salary' ; Contraintes d'int\u00e9grit\u00e9 Les contraintes primary key et foreign key ne sont support\u00e9es qu'\u00e0 partir de la version 2. Les contraintes unique , not null et check sont ajout\u00e9es depuis la version3. Ajouter une colonne \u00e0 une table CREATE TABLE test ( id int ); ALTER TABLE salary ADD COLUMNS ( name varchar , dob date ); Supprimer une table DROP TABLE test ; SHOW/DESCRIBE Afficher les objets disponibles Show Databases ; Show Tables ; Show Partitions < table > ; Show TblProperties < table > ; Show Create Table < table > ; Show Indexes on < table > ; Show Columns in < table > ; Show Functions ; Show transactions ; Afficher la strucuture ou les propri\u00e9t\u00e9s d'un objet Describe database ; Describe < table > ; Describe < table . column > ;","title":"LDD"},{"location":"hive.html#lmd","text":"Charger des donn\u00e9es \u00e0 partir des fichiers CSV depuis HDFS LOAD DATA INPATH '/user/uti/demo/employee.csv' INTO TABlE employee ; LOAD DATA INPATH '/user/uti/demo/department.csv' INTO TABlE department ; LOAD DATA INPATH '/user/uti/demo/salary.csv' INTO TABlE salary ; Remarque V\u00e9rifier sur HDFS l'emplacement, le type et le format des fichiers de donn\u00e9es relatifs \u00e0 ces tables. Insertion de donn\u00e9es CREATE TABLE jours ( num int , nom string ); INSERT INTO jours VALUES ( 1 , 'Lundi' ); INSERT INTO jours VALUES ( 2 , 'Mardi' ); INSERT INTO jours VALUES ( 3 , 'Mercredi' ); MapReduce La commande d'insertion g\u00e9n\u00e8re un job MapReduce que vous pouvez consulter sur YARN. Mise \u00e0 jour et suppression Pour ex\u00e9cuter ces op\u00e9rations, quelques conditions doivent \u00eatre v\u00e9rifi\u00e9es : La table utilise le format =ORC= La table supporte le Bucketing Les transactions ACID sont activ\u00e9es sur Hive Essayer de mettre \u00e0 jour la table jours : UPDATE jours set nom = upper ( nom ) WHERE num = 1 ; Une erreur est alors affich\u00e9e. Pour pouvoir ex\u00e9cuter ces requ\u00eates, nous commen\u00e7ons par activer les transactions sous Hive. Ceci est possible avec l'une des deux m\u00e9thodes : Modifier le fichier =hive-site.xml= en ajoutant les lignes suivante et puis red\u00e9marrer Hive : <property> <name> hive.support.concurrency </name> <value> true </value> </property> <property> <name> hive.txn.manager </name> <value> org.apache.hadoop.hive.ql.lockmgr.DbTxnManager </value> </property> <property> <name> hive.enforce.bucketing </name> <value> true </value> </property> <property> <name> hive.exec.dynamic.partition.mode </name> <value> nostrict </value> </property> <property> <name> hive.compactor.initiator.on </name> <value> true </value> </property> <property> <name> hive.compactor.worker.threads </name> <value> 1 </value> </property> 2. \u00c0 partir du shell hive : SET hive.support.concurrency = true ; SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager ; # Pour Hive 2.0 SET hive.enforce.bucketing = true ; SET hive.exec.dynamic.partition.mode = nostrict ; # Pour hive metastore SET hive.compactor.initiator.on = true ; SET hive.compactor.worker.threads = 1 ; Ensuite cr\u00e9er une table supportant les transactions : CREATE TABLE employee_trans ( id int , name string , age int , gender string ) clustered by ( gender ) into 2 buckets STORED AS ORC TBLPROPERTIES ( 'transactional' = 'true' ); -- ins\u00e9rer des donn\u00e9es INSERT INTO employee_trans VALUES ( 1 , 'James' , 30 , 'M' ); INSERT INTO employee_trans VALUES ( 2 , 'Ann' , 40 , 'F' ); INSERT INTO employee_trans VALUES ( 3 , 'Jeff' , 41 , 'M' ); INSERT INTO employee_trans VALUES ( 4 , 'Jennifer' , 20 , 'F' ); -- Mise \u00e0 jour UPDATE employee_trans SET age = 45 WHERE id = 3 ; -- Suppression DELETE FROM employee_trans WHERE id = 4 ; -- V\u00e9rifier les donn\u00e9es SELECT * FROM employee_trans ;","title":"LMD"},{"location":"hive.html#select","text":"Format de la requ\u00eate : SELECT [ ALL | DISTINCT ] select_expr , select_expr , ... FROM table_reference [ WHERE where_condition ] [ GROUP BY col_list ] [ HAVING having_condition ] [ ORDER BY col_list ] [ LIMIT [ offset ,] rows ] Reprenons les m\u00eames exemples de la section Pig. Filtrage pour avoir les employ\u00e9s (sans doublons) ayant re\u00e7u un salaire compris entre 5000 et 7000 SELECT distinct employee_id FROM salary WHERE payment >= 5000 AND payment <= 7000 ; Nombre d'employ\u00e9s par d\u00e9partement et afficher les 3 plus grands d\u00e9partements SELECT department_id , count ( id ) AS nb_emp FROM employee GROUP BY department_id ORDER BY nb_emp DESC LIMIT 3 ; Jointure : trouver le nom du d\u00e9partement de chaque employ\u00e9 (afficher les 4 premiers employ\u00e9s) SELECT fname , lname , dept_name FROM employee JOIN department ON ( employee . department_id = department . id ) limit 4 ; Requ\u00eate imbriqu\u00e9e : Trouver le nombre moyen d'employ\u00e9s par d\u00e9partement SELECT AVG ( nb_emp ) FROM ( SELECT department_id , count ( id ) AS nb_emp FROM employee GROUP BY department_id ) q1 ;","title":"SELECT"},{"location":"hive.html#references_hiveql","text":"Pour une r\u00e9f\u00e9rence compl\u00e8te du langage Pi Latin, aller sur la page de documentation ( ici ) Voici aussi, un m\u00e9mo du langage : Ouvrir dans un nouvel onglet","title":"R\u00e9f\u00e9rences HiveQL"},{"location":"hive.html#exercice","text":"R\u00e9soudre les questions 2 et 3 des exercices de la section MapReduce.","title":"Exercice"},{"location":"mr.html","text":"MapReduce avec Python \u00b6 D\u00e9marrage/Arr\u00eat du service \u00b6 Service YARN D\u00e9marrage sudo service hadoop-yarn-resourcemanager start sudo service hadoop-yarn-nodemanager start Arr\u00eat sudo service hadoop-yarn-resourcemanager stop sudo service hadoop-yarn-nodemanager stop La commande jps permet de v\u00e9rifier que deux processus sont lanc\u00e9s : ResourceManager et NodeManager. L'interface du Resource Manager est accessible depuis : http://localhost:8088 Cette interface permet de v\u00e9rifier l'\u00e9tat des ressources RAM et CPU du cluster, les applications (d\u00e9marr\u00e9es, termin\u00e9es, en cours ...) et les noeuds du cluster. V\u00e9rification des processus serveur \u00b6 Pour voir les diff\u00e9rents processus serveurs, ex\u00e9cuter la commande jps . Le r\u00e9sultat est : Processus HDFS : NameNode, DataNode, JournalNode Processus YARN : ResourceManager, NodeManager, JobHistoryServer, ApplicationHistoryServer Pr\u00e9parer l'environnement \u00b6 Pour les exemples et les exercices de cette section, vous avez besoin de Python, Jupyter et MRJOB. Installer les paquets et modules suivants : Cette \u00e9tape est \u00e0 ignorer si vous utilisez la machine virtuelle fournie . $ sudo apt-get install python python3-pip $ sudo pip3 install mrjob $ sudo pip3 install jupyter Lancer Jupyter Notebook $ cd dossier_travail $ jupyter notebook --no-browser --ip = 0 .0.0.0 --NotebookApp.token = '' Pour acc\u00e9der depuis la machine h\u00f4te aller \u00e0 l'adresse : http://localhost:8888 Les exemples peuvent \u00eatre \u00e9crit avec un simple \u00e9diteur comme geany, nano, vi (ou encore notepad sous windows). Et les commandes peuvent \u00eatre lanc\u00e9es \u00e0 partir du shell. Premier Exemple : Word Count \u00b6 L'exemple classique pour illustrer le fonctionnement de MapReduce est celui de compter le nombre d'occurences d'un mot dans un fichier texte (le fichier shakespeare.txt .) Pr\u00e9parer les donn\u00e9es \u00b6 T\u00e9l\u00e9charger le fichier shakespeare.txt sur la machine virtuelle Hadoop. Mettre le fichier dans le syst\u00e8me HDFS $ hadoop fs -put shakespeare.txt Mapper et Reducer \u00b6 L'objectif est de d\u00e9terminer le nombre d'occurences de chaque mot du fichier. Chaque ligne est compos\u00e9e de plusieurs mots (Le s\u00e9parateur est ' '). La transformation \u00e0 effectuer par chaque Mapper est de d\u00e9composer chaque ligne re\u00e7ue en mots est lui associer la valeur 1. Apr\u00e8s la phase de tri et de redistribution (sort and shuffle), chaque Reducer va recevoir des mots et une liste de '1' pour chaque occurence. Pour calculer le nombre d'occurences total, il suffit de faire la somme de ces '1'. Programmation avec MRJob \u00b6 MapReduce est \u00e9crit en Java mais accepte les autres langages de programmation via la technique de streaming via op\u00e9ration de lecture \u00e9criture sur les flux d'entr\u00e9e-sortie standard (print, read). La biblioth\u00e8que mrjob simplifie l'\u00e9criture de programmes MapReduce. Dans cet exemple, deux classes de la librairie mrjob sont utilis\u00e9es : MRJob et MRStep. Pour cr\u00e9er un Job MapReduce, il suffit de cr\u00e9er une classe qui h\u00e9rite de MRJob (ici la classe WordCount). Cr\u00e9er une fonction qui effectue la transformation du Mapper : la m\u00e9thode mapper_get_words . Cette m\u00e9thode re\u00e7oit 3 param\u00e8tres self comme toutes les m\u00e9thodes d'une classe, le deuxi\u00e8me est ignor\u00e9 pour ce mapper et le dernier c'est une ligne du fichier de donn\u00e9es. La fonction split d\u00e9compose la ligne en une liste de mots. Cr\u00e9er une fonction qui r\u00e9alise l'agr\u00e9gation du Reducer : la m\u00e9thode reducer_count_words . Elle re\u00e7oit le param\u00e8tre self, la cl\u00e9 utilis\u00e9e par le Mapper (donc un mot) et la liste des valeurs r\u00e9cupr\u00e9es des diff\u00e9rents mappers (les '1'). L'agr\u00e9gation effectu\u00e9e est une somme par la fonction sum. D\u00e9finier les \u00e9tapes ou les diff\u00e9rentes t\u00e2che \u00e0 effectuer dans une m\u00e9thode appel\u00e9e steps qui doit retouner une liste de MRStep. Pour chaque instance MRStep cr\u00e9\u00e9e, on sp\u00e9cifie le nom de la fonction mapper et reducer d\u00e9j\u00e0 d\u00e9finie. Dans cet exemple, il y a une seule phase MapReduce. Enfin l'appel de la m\u00e9thode run pour d\u00e9clencher L'ex\u00e9cution du Job MapReduce. Dans ce qui suit le contenu du Notebook Jupyter avec le code \u00e0 tester. Streaming jar En cas d'erreur No hadoop streaming jar , donner le chemin vers ce jar avec la commande : ! python wordcount.py -r hadoop --hadoop-streaming-jar C:\\hadoop-env\\hadoop-3.2.1\\share\\hadoop\\tools\\lib\\hadoop-streaming-3.2.1.jar hdfs:///user/uti/shakespeare.txt Surveiller les jobs \u00b6 Pour afficher les jobs MapReduce de la session en cours, visiter la page http://localhost:8088 Quand le programme MapReduce de l'exercice pr\u00e9c\u00e9dent est termin\u00e9, il est indiqu\u00e9 dans la capture suivante : Pour afficher l'historique complet c'est \u00e0 partir d'ici : http://localhost:8081 Il est aussi possible d'utiliser les commandes yarn : - Pour afficher toutes les applications : yarn application -list -appStates ALL - Pour arr\u00eater une application : yarn application -kill <id> Exercice \u00b6 Transformer l'exemple WordCount afin d'obtenir le r\u00e9sultat tri\u00e9 dans l'ordre d\u00e9croissant du nombre d'occurrences. (Hint : exploiter la phase sort and shuffle qui trie la cl\u00e9 selon l'ordre alphab\u00e9tique). Sur les donn\u00e9es de la production p\u00e9troli\u00e8re de la section pr\u00e9c\u00e9dente et en utilisant MapReduce : Calculer la production annuelle de chaque champ. Trouver le champ avec la plus grande production par mois. En s'inspirant de l'exemple MapReduce et source json ( books.json ) ci-apr\u00e8s qui affiche les titres des livres ayant 700 pages, \u00e9crire un programme MapReduce qui calcule le nombre de livres pour chaque auteur. from mrjob.step import MRStep from mrjob.job import MRJob from mrjob.protocol import JSONValueProtocol class Exemple ( MRJob ): INPUT_PROTOCOL = JSONValueProtocol def steps ( self ): return [ MRStep ( mapper = self . fmap , reducer = self . freduce )] def fmap ( self , _ , book ): if book [ 'pageCount' ] > 700 : yield None ,( book [ 'title' ], book [ 'pageCount' ]) def freduce ( self , _ , v ): for l in v : yield l [ 0 ], l [ 1 ] if __name__ == '__main__' : Exemple . run ()","title":"MapReduce avec Python"},{"location":"mr.html#mapreduce_avec_python","text":"","title":"MapReduce avec Python"},{"location":"mr.html#demarragearret_du_service","text":"Service YARN D\u00e9marrage sudo service hadoop-yarn-resourcemanager start sudo service hadoop-yarn-nodemanager start Arr\u00eat sudo service hadoop-yarn-resourcemanager stop sudo service hadoop-yarn-nodemanager stop La commande jps permet de v\u00e9rifier que deux processus sont lanc\u00e9s : ResourceManager et NodeManager. L'interface du Resource Manager est accessible depuis : http://localhost:8088 Cette interface permet de v\u00e9rifier l'\u00e9tat des ressources RAM et CPU du cluster, les applications (d\u00e9marr\u00e9es, termin\u00e9es, en cours ...) et les noeuds du cluster.","title":"D\u00e9marrage/Arr\u00eat du service"},{"location":"mr.html#verification_des_processus_serveur","text":"Pour voir les diff\u00e9rents processus serveurs, ex\u00e9cuter la commande jps . Le r\u00e9sultat est : Processus HDFS : NameNode, DataNode, JournalNode Processus YARN : ResourceManager, NodeManager, JobHistoryServer, ApplicationHistoryServer","title":"V\u00e9rification des processus serveur"},{"location":"mr.html#preparer_lenvironnement","text":"Pour les exemples et les exercices de cette section, vous avez besoin de Python, Jupyter et MRJOB. Installer les paquets et modules suivants : Cette \u00e9tape est \u00e0 ignorer si vous utilisez la machine virtuelle fournie . $ sudo apt-get install python python3-pip $ sudo pip3 install mrjob $ sudo pip3 install jupyter Lancer Jupyter Notebook $ cd dossier_travail $ jupyter notebook --no-browser --ip = 0 .0.0.0 --NotebookApp.token = '' Pour acc\u00e9der depuis la machine h\u00f4te aller \u00e0 l'adresse : http://localhost:8888 Les exemples peuvent \u00eatre \u00e9crit avec un simple \u00e9diteur comme geany, nano, vi (ou encore notepad sous windows). Et les commandes peuvent \u00eatre lanc\u00e9es \u00e0 partir du shell.","title":"Pr\u00e9parer l'environnement"},{"location":"mr.html#premier_exemple_word_count","text":"L'exemple classique pour illustrer le fonctionnement de MapReduce est celui de compter le nombre d'occurences d'un mot dans un fichier texte (le fichier shakespeare.txt .)","title":"Premier Exemple : Word Count"},{"location":"mr.html#preparer_les_donnees","text":"T\u00e9l\u00e9charger le fichier shakespeare.txt sur la machine virtuelle Hadoop. Mettre le fichier dans le syst\u00e8me HDFS $ hadoop fs -put shakespeare.txt","title":"Pr\u00e9parer les donn\u00e9es"},{"location":"mr.html#mapper_et_reducer","text":"L'objectif est de d\u00e9terminer le nombre d'occurences de chaque mot du fichier. Chaque ligne est compos\u00e9e de plusieurs mots (Le s\u00e9parateur est ' '). La transformation \u00e0 effectuer par chaque Mapper est de d\u00e9composer chaque ligne re\u00e7ue en mots est lui associer la valeur 1. Apr\u00e8s la phase de tri et de redistribution (sort and shuffle), chaque Reducer va recevoir des mots et une liste de '1' pour chaque occurence. Pour calculer le nombre d'occurences total, il suffit de faire la somme de ces '1'.","title":"Mapper et Reducer"},{"location":"mr.html#programmation_avec_mrjob","text":"MapReduce est \u00e9crit en Java mais accepte les autres langages de programmation via la technique de streaming via op\u00e9ration de lecture \u00e9criture sur les flux d'entr\u00e9e-sortie standard (print, read). La biblioth\u00e8que mrjob simplifie l'\u00e9criture de programmes MapReduce. Dans cet exemple, deux classes de la librairie mrjob sont utilis\u00e9es : MRJob et MRStep. Pour cr\u00e9er un Job MapReduce, il suffit de cr\u00e9er une classe qui h\u00e9rite de MRJob (ici la classe WordCount). Cr\u00e9er une fonction qui effectue la transformation du Mapper : la m\u00e9thode mapper_get_words . Cette m\u00e9thode re\u00e7oit 3 param\u00e8tres self comme toutes les m\u00e9thodes d'une classe, le deuxi\u00e8me est ignor\u00e9 pour ce mapper et le dernier c'est une ligne du fichier de donn\u00e9es. La fonction split d\u00e9compose la ligne en une liste de mots. Cr\u00e9er une fonction qui r\u00e9alise l'agr\u00e9gation du Reducer : la m\u00e9thode reducer_count_words . Elle re\u00e7oit le param\u00e8tre self, la cl\u00e9 utilis\u00e9e par le Mapper (donc un mot) et la liste des valeurs r\u00e9cupr\u00e9es des diff\u00e9rents mappers (les '1'). L'agr\u00e9gation effectu\u00e9e est une somme par la fonction sum. D\u00e9finier les \u00e9tapes ou les diff\u00e9rentes t\u00e2che \u00e0 effectuer dans une m\u00e9thode appel\u00e9e steps qui doit retouner une liste de MRStep. Pour chaque instance MRStep cr\u00e9\u00e9e, on sp\u00e9cifie le nom de la fonction mapper et reducer d\u00e9j\u00e0 d\u00e9finie. Dans cet exemple, il y a une seule phase MapReduce. Enfin l'appel de la m\u00e9thode run pour d\u00e9clencher L'ex\u00e9cution du Job MapReduce. Dans ce qui suit le contenu du Notebook Jupyter avec le code \u00e0 tester. Streaming jar En cas d'erreur No hadoop streaming jar , donner le chemin vers ce jar avec la commande : ! python wordcount.py -r hadoop --hadoop-streaming-jar C:\\hadoop-env\\hadoop-3.2.1\\share\\hadoop\\tools\\lib\\hadoop-streaming-3.2.1.jar hdfs:///user/uti/shakespeare.txt","title":"Programmation avec MRJob"},{"location":"mr.html#surveiller_les_jobs","text":"Pour afficher les jobs MapReduce de la session en cours, visiter la page http://localhost:8088 Quand le programme MapReduce de l'exercice pr\u00e9c\u00e9dent est termin\u00e9, il est indiqu\u00e9 dans la capture suivante : Pour afficher l'historique complet c'est \u00e0 partir d'ici : http://localhost:8081 Il est aussi possible d'utiliser les commandes yarn : - Pour afficher toutes les applications : yarn application -list -appStates ALL - Pour arr\u00eater une application : yarn application -kill <id>","title":"Surveiller les jobs"},{"location":"mr.html#exercice","text":"Transformer l'exemple WordCount afin d'obtenir le r\u00e9sultat tri\u00e9 dans l'ordre d\u00e9croissant du nombre d'occurrences. (Hint : exploiter la phase sort and shuffle qui trie la cl\u00e9 selon l'ordre alphab\u00e9tique). Sur les donn\u00e9es de la production p\u00e9troli\u00e8re de la section pr\u00e9c\u00e9dente et en utilisant MapReduce : Calculer la production annuelle de chaque champ. Trouver le champ avec la plus grande production par mois. En s'inspirant de l'exemple MapReduce et source json ( books.json ) ci-apr\u00e8s qui affiche les titres des livres ayant 700 pages, \u00e9crire un programme MapReduce qui calcule le nombre de livres pour chaque auteur. from mrjob.step import MRStep from mrjob.job import MRJob from mrjob.protocol import JSONValueProtocol class Exemple ( MRJob ): INPUT_PROTOCOL = JSONValueProtocol def steps ( self ): return [ MRStep ( mapper = self . fmap , reducer = self . freduce )] def fmap ( self , _ , book ): if book [ 'pageCount' ] > 700 : yield None ,( book [ 'title' ], book [ 'pageCount' ]) def freduce ( self , _ , v ): for l in v : yield l [ 0 ], l [ 1 ] if __name__ == '__main__' : Exemple . run ()","title":"Exercice"},{"location":"pig.html","text":"Pig \u00b6 Premier exemple : Wordcount \u00b6 Nous reprenons l'exemple de comptage de mots utilis\u00e9 dans l'atelier MapReduce pour le r\u00e9soudre avec le langage Pig Latin. Structure d'un script Pig Un script Pig comporte 3 phases : 1. Le chargement des donn\u00e9es avec LOAD 2. Une s\u00e9rie de transformations 3. L'affichage ou la sauvegarde des r\u00e9sultats Lancer la machine virtuelle et acc\u00e9der \u00e0 Pig en mode interactif \u00e0 partir du terminal pig -x mapreduce Vous serez alors redirig\u00e9 vers le shell grunt : grunt> Charger le fichier shakespeare.txt lines = LOAD './shakespeare.txt' AS ( line: chararray ); Ce qui charge le contenu du fichier \u00e0 partir de HDFS sous la forme de bag de chararray dans la variable lines. D\u00e9composer chaque ligne en mots words = FOREACH lines GENERATE FLATTEN ( TOKENIZE ( line )) AS word; TOKENIZE : D\u00e9couper la ligne en bag de mots FLATTEN : Transformer le bag en des mots individuels Cr\u00e9er un groupe pour chaque mot word_groups = GROUP words BY word; Compter les \u00e9l\u00e9ments de chaque groupe word_count = FOREACH word_groups GENERATE COUNT ( words ) AS count , group ; Trier par le nombre d'occurences ordered_word_count = ORDER word_count BY count DESC ; Sauvegarder sur HDFS STORE ordered_word_count INTO './word_count_result' ; Apr\u00e8s l'envoi de cette instruction, Pig cr\u00e9e un job MapReduce. V\u00e9rifier la progression sur http://localhost:8088 Un dossier word_count_result est cr\u00e9\u00e9 o\u00f9 des fichiers part-r-???? contiennent les nombres d'occurences associ\u00e9s aux mots du fichier. Afficher le r\u00e9sultat hadoop fs -cat word_count_result/part-r-* | more Vous pouvez aussi utiliser l'interface web de HDFS pour afficher le r\u00e9sultat dans le dossier 'word_count_result' Ex\u00e9cution d'un script \u00b6 Pour ex\u00e9cuter l'exemple pr\u00e9c\u00e9dent : Cr\u00e9er un fichier wordcount.pig avec un \u00e9diteur de texte (geany) avec le code suivant : lines = LOAD './shakespeare.txt' AS ( line: chararray ); words = FOREACH lines GENERATE FLATTEN ( TOKENIZE ( line )) AS word; word_groups = GROUP words BY word; word_count = FOREACH word_groups GENERATE COUNT ( words ) AS count , group ; ordered_word_count = ORDER word_count BY count DESC ; STORE ordered_word_count INTO './word_count_result' ; Ex\u00e9cuter le script avec la commande suivante pig -x mapreduce wordcount.pig Mode local Avant de lancer un script pig, il est recommand\u00e9 de le tester en local. Ceci est possible avec la commande suivante : pig -x local wordcount.pig Attention : dans ce mode d'ex\u00e9cution les chemins sont interpr\u00e9t\u00e9s dans le syst\u00e8me local et non HDFS. Instructions Pig par l'exemple \u00b6 Pr\u00e9parer les donn\u00e9es Cr\u00e9er un dossier demo : mkdir ~/demo T\u00e9l\u00e9charger les fichiers csv dans /home/uti/demo : employee.csv department.csv salary.csv Envoyer les donn\u00e9es sur HDFS hadoop fs -put /home/uti/demo Charger des donn\u00e9es sans sch\u00e9ma \u00e0 partir d'un fichier CSV avec ',' comme s\u00e9parateur. Puis limiter le nombre de tuples \u00e0 10 e1 = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ); e1_sample = limit e1 10 ; dump e1_sample; Charger avec un sch\u00e9ma emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( eid: chararray , fname: chararray , lname: chararray , department: chararray ); emp_sample = limit emp 10 ; dump emp_sample; Projection emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( eid: chararray , fname: chararray , lname: chararray , department: chararray ); emp_proj = FOREACH emp GENERATE $1, lname; emp_sample = limit emp_proj 10 ; dump emp_sample; Filtrage pour avoir les employ\u00e9s (sans doublons) ayant re\u00e7u un salaire compris entre 5000 et 7000 sal = LOAD '/user/uti/demo/salary.csv' USING PigStorage ( ',' ) as ( salary_id: chararray , employ_id: chararray , payment: double , p_date: datetime ); above_5k_7k = DISTINCT ( FILTER sal BY payment >= 5000 AND payment <= 7000 ); emp_sample = limit emp 10 ; dump emp_sample; Nombre d'employ\u00e9s par d\u00e9partement et afficher les 3 plus grands d\u00e9partements emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( eid: chararray , fname: chararray , lname: chararray , department: chararray ); emp_group_dep = group emp by department; -- ou emp_group_dep = group emp by $3; nbemp_dep = ORDER ( FOREACH emp_group_dep GENERATE group AS dep, COUNT ( eid ) AS nb_emp ) BY nb_emp DESC ; top_dep = limit nbemp_dep 3 ; dump top_dep; Jointure : trouver le nom du d\u00e9partement de chaque employ\u00e9 emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( emp_id: chararray , fname: chararray , lname: chararray , dept_id: chararray ); dep = LOAD '/user/uti/demo/department.csv' USING PigStorage ( ',' ) as ( dept_id: chararray , dept_name: chararray ); dep_emp = LIMIT ( FOREACH ( JOIN emp by ( dept_id ) , dep by ( dept_id )) GENERATE fname, lname, dept_name ) 3 ; DUMP dep_emp; Test et performances \u00b6 DESCRIBE Elle permet d'afficher le sch\u00e9ma d'une relation. Exemple grunt> lines = LOAD './shakespeare.txt' AS ( line: chararray ); grunt> words = FOREACH lines GENERATE FLATTEN ( TOKENIZE ( line )) AS word; grunt> word_groups = GROUP words BY word; grunt> word_count = FOREACH word_groups GENERATE COUNT ( words ) AS count , group ; grunt> DESCRIBE lines; lines: { line: chararray } grunt> DECRIBE word_groups; word_groups: { group : chararray , words: { word: chararray }} grunt> DESCRIBEvword_count; word_count: { count : long , group : chararray } ILLUSTRATE Cette commande Pig permet de montrer les diff\u00e9rentes transformations effectu\u00e9es sur les donn\u00e9es. ILLUSTRATE relation|-script < nom_script> Exemple : (source http://pig.apache.org ) grunt> cat visits.txt Amy yahoo.com 19990421 Fred harvard.edu 19991104 Amy cnn.com 20070218 Frank nba.com 20070305 Fred berkeley.edu 20071204 Fred stanford.edu 20071206 grunt> cat visits.pig visits = LOAD 'visits.txt' AS ( user, url, timestamp ); recent_visits = FILTER visits BY timestamp >= '20071201' ; historical_visits = FILTER visits BY timestamp <= '20000101' ; DUMP recent_visits; DUMP historical_visits; STORE recent_visits INTO 'recent' ; STORE historical_visits INTO 'historical' ; grunt> exec visits.pig ( Fred,berkeley.edu,20071204 ) ( Fred,stanford.edu,20071206 ) ( Amy,yahoo.com,19990421 ) ( Fred,harvard.edu,19991104 ) grunt> illustrate - script visits.pig ------------------------------------------------------------------------ | visits | user: bytearray | url: bytearray | timestamp: bytearray | ------------------------------------------------------------------------ | | Amy | yahoo.com | 19990421 | | | Fred | stanford.edu | 20071206 | ------------------------------------------------------------------------ ------------------------------------------------------------------------------- | recent_visits | user: bytearray | url: bytearray | timestamp: bytearray | ------------------------------------------------------------------------------- | | Fred | stanford.edu | 20071206 | ------------------------------------------------------------------------------- --------------------------------------------------------------------------------------- | Store : recent_visits | user: bytearray | url: bytearray | timestamp: bytearray | --------------------------------------------------------------------------------------- | | Fred | stanford.edu | 20071206 | --------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- | historical_visits | user: bytearray | url: bytearray | timestamp: bytearray | ----------------------------------------------------------------------------------- | | Amy | yahoo.com | 19990421 | ----------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------- | Store : historical_visits | user: bytearray | url: bytearray | timestamp: bytearray | ------------------------------------------------------------------------------------------- | | Amy | yahoo.com | 19990421 | ------------------------------------------------------------------------------------------- EXPLAIN Affiche les plans d'ex\u00e9cution logique, physique et les phases MapReduce. EXPLAIN relation|-script < nom_script> [ - out chemin] [ - dot|-xml] Exemple grunt> EXPLAIN - script wordcount.pig - out explain_wordcount.txt Le fichier explain_wordcount.txt obtenu contient : #----------------------------------------------- # New Logical Plan: #----------------------------------------------- ordered_word_count: ( Name: LOStore Schema: count#112:long,group#109:chararray ) | | ---ordered_word_count: ( Name: LOSort Schema: count#112:long,group#109:chararray ) | | | count: ( Name: Project Type: long Uid: 112 Input: 0 Column: 0 ) | | ---word_count: ( Name: LOForEach Schema: count#112:long,group#109:chararray ) | | | ( Name: LOGenerate [ false,false ] Schema: count#112:long,group#109:chararray ) ColumnPrune:OutputUids =[ 112 , 109 ] ColumnPrune:InputUids =[ 109 , 110 ] | | | | | ( Name: UserFunc ( org.apache.pig.builtin.COUNT ) Type: long Uid: 112 ) | | | | | | ---words: ( Name: Project Type: bag Uid: 110 Input: 0 Column: ( * )) | | | | | group: ( Name: Project Type: chararray Uid: 109 Input: 1 Column: ( * )) | | | | ---words: ( Name: LOInnerLoad [ 1 ] Schema: word#109:chararray ) | | | | --- ( Name: LOInnerLoad [ 0 ] Schema: group#109:chararray ) | | ---word_groups: ( Name: LOCogroup Schema: group#109:chararray,words#110:bag { #114:tuple(word#109:chararray)}) | | | word: ( Name: Project Type: chararray Uid: 109 Input: 0 Column: 0 ) | | ---words: ( Name: LOForEach Schema: word#118:chararray ) | | | ( Name: LOGenerate [ true ] Schema: word#118:chararray ) | | | | | ( Name: UserFunc ( org.apache.pig.builtin.TOKENIZE ) Type: bag Uid: 116 ) | | | | | | --- ( Name: Cast Type: chararray Uid: 98 ) | | | | | | ---line: ( Name: Project Type: bytearray Uid: 98 Input: 0 Column: ( * )) | | | | --- ( Name: LOInnerLoad [ 0 ] Schema: line#98:bytearray ) | | ---lines: ( Name: LOLoad Schema: line#98:bytearray ) RequiredFields:null #----------------------------------------------- # Physical Plan: #----------------------------------------------- ordered_word_count: Store ( hdfs://master.cluster.virt:8020/user/uti/word_count_result:org.apache.pig.builtin.PigStorage ) - scope-18 | | ---ordered_word_count: POSort [ bag ]() - scope-17 | | | Project [ long ][ 0 ] - scope-16 | | ---word_count: New For Each ( false,false )[ bag ] - scope-15 | | | POUserFunc ( org.apache.pig.builtin.COUNT )[ long ] - scope-11 | | | | ---Project [ bag ][ 1 ] - scope-10 | | | Project [ chararray ][ 0 ] - scope-13 | | ---word_groups: Package ( Packager )[ tuple ]{ chararray } - scope-7 | | ---word_groups: Global Rearrange [ tuple ] - scope-6 | | ---word_groups: Local Rearrange [ tuple ]{ chararray }( false ) - scope-8 | | | Project [ chararray ][ 0 ] - scope-9 | | ---words: New For Each ( true )[ bag ] - scope-5 | | | POUserFunc ( org.apache.pig.builtin.TOKENIZE )[ bag ] - scope-3 | | | | ---Cast [ chararray ] - scope-2 | | | | ---Project [ bytearray ][ 0 ] - scope-1 | | ---lines: Load ( hdfs://master.cluster.virt:8020/user/uti/shakespeare.txt:org.apache.pig.builtin.PigStorage ) - scope-0 #-------------------------------------------------- # Map Reduce Plan #-------------------------------------------------- MapReduce node scope-19 Map Plan word_groups: Local Rearrange [ tuple ]{ chararray }( false ) - scope-53 | | | Project [ chararray ][ 0 ] - scope-55 | | ---word_count: New For Each ( false,false )[ bag ] - scope-42 | | | Project [ chararray ][ 0 ] - scope-43 | | | POUserFunc ( org.apache.pig.builtin.COUNT $Initial )[ tuple ] - scope-44 | | | | ---Project [ bag ][ 1 ] - scope-45 | | ---Pre Combiner Local Rearrange [ tuple ]{ Unknown } - scope-56 | | ---words: New For Each ( true )[ bag ] - scope-5 | | | POUserFunc ( org.apache.pig.builtin.TOKENIZE )[ bag ] - scope-3 | | | | ---Cast [ chararray ] - scope-2 | | | | ---Project [ bytearray ][ 0 ] - scope-1 | | ---lines: Load ( hdfs://master.cluster.virt:8020/user/uti/shakespeare.txt:org.apache.pig.builtin.PigStorage ) - scope-0-------- Combine Plan word_groups: Local Rearrange [ tuple ]{ chararray }( false ) - scope-57 | | | Project [ chararray ][ 0 ] - scope-59 | | ---word_count: New For Each ( false,false )[ bag ] - scope-46 | | | Project [ chararray ][ 0 ] - scope-47 | | | POUserFunc ( org.apache.pig.builtin.COUNT $Intermediate )[ tuple ] - scope-48 | | | | ---Project [ bag ][ 1 ] - scope-49 | | ---word_groups: Package ( CombinerPackager )[ tuple ]{ chararray } - scope-52-------- Reduce Plan Store ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.io.InterStorage ) - scope-20 | | ---word_count: New For Each ( false,false )[ bag ] - scope-15 | | | POUserFunc ( org.apache.pig.builtin.COUNT $Final )[ long ] - scope-11 | | | | ---Project [ bag ][ 1 ] - scope-50 | | | Project [ chararray ][ 0 ] - scope-13 | | ---word_groups: Package ( CombinerPackager )[ tuple ]{ chararray } - scope-7-------- Global sort: false ---------------- MapReduce node scope-22 Map Plan ordered_word_count: Local Rearrange [ tuple ]{ tuple }( false ) - scope-26 | | | Constant ( all ) - scope-25 | | ---New For Each ( false )[ tuple ] - scope-24 | | | Project [ long ][ 0 ] - scope-23 | | ---Load ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.builtin.RandomSampleLoader ( 'org.apache.pig.impl.io.InterStorage' , '100' )) - scope-21-------- Reduce Plan Store ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp569443691:org.apache.pig.impl.io.InterStorage ) - scope-35 | | ---New For Each ( false )[ tuple ] - scope-34 | | | POUserFunc ( org.apache.pig.impl.builtin.FindQuantiles )[ tuple ] - scope-33 | | | | ---Project [ tuple ][ * ] - scope-32 | | ---New For Each ( false,false )[ tuple ] - scope-31 | | | Constant ( -1 ) - scope-30 | | | Project [ bag ][ 1 ] - scope-28 | | ---Package ( Packager )[ tuple ]{ chararray } - scope-27-------- Global sort: false Secondary sort: true ---------------- MapReduce node scope-37 Map Plan ordered_word_count: Local Rearrange [ tuple ]{ long }( false ) - scope-38 | | | Project [ long ][ 0 ] - scope-16 | | ---Load ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.io.InterStorage ) - scope-36-------- Reduce Plan ordered_word_count: Store ( hdfs://master.cluster.virt:8020/user/uti/word_count_result:org.apache.pig.builtin.PigStorage ) - scope-18 | | ---New For Each ( true )[ tuple ] - scope-41 | | | Project [ bag ][ 1 ] - scope-40 | | ---Package ( LitePackager )[ tuple ]{ long } - scope-39-------- Global sort: true Quantile file: hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp569443691 ---------------- Pour avoir un graphe d'ex\u00e9cution : grunt> EXPLAIN - script wordcount.pig - out explain_wordcount.dot - dot Puis convertir le fichier .dot en png : apt install graphviz -y dot -Tpng -out explain_wordcount.png explain_wordcount.dot Ce qui donne : R\u00e9f\u00e9rences Pig Latin \u00b6 Pour une r\u00e9f\u00e9rence compl\u00e8te du langage Pig Latin, aller sur la page de documentation ( ici ) Voici aussi, un m\u00e9mo du langage : Ouvrir dans un nouvel onglet Exercice \u00b6 R\u00e9soudre les questions 2 et 3 des exercices de la section MapReduce.","title":"Pig"},{"location":"pig.html#pig","text":"","title":"Pig"},{"location":"pig.html#premier_exemple_wordcount","text":"Nous reprenons l'exemple de comptage de mots utilis\u00e9 dans l'atelier MapReduce pour le r\u00e9soudre avec le langage Pig Latin. Structure d'un script Pig Un script Pig comporte 3 phases : 1. Le chargement des donn\u00e9es avec LOAD 2. Une s\u00e9rie de transformations 3. L'affichage ou la sauvegarde des r\u00e9sultats Lancer la machine virtuelle et acc\u00e9der \u00e0 Pig en mode interactif \u00e0 partir du terminal pig -x mapreduce Vous serez alors redirig\u00e9 vers le shell grunt : grunt> Charger le fichier shakespeare.txt lines = LOAD './shakespeare.txt' AS ( line: chararray ); Ce qui charge le contenu du fichier \u00e0 partir de HDFS sous la forme de bag de chararray dans la variable lines. D\u00e9composer chaque ligne en mots words = FOREACH lines GENERATE FLATTEN ( TOKENIZE ( line )) AS word; TOKENIZE : D\u00e9couper la ligne en bag de mots FLATTEN : Transformer le bag en des mots individuels Cr\u00e9er un groupe pour chaque mot word_groups = GROUP words BY word; Compter les \u00e9l\u00e9ments de chaque groupe word_count = FOREACH word_groups GENERATE COUNT ( words ) AS count , group ; Trier par le nombre d'occurences ordered_word_count = ORDER word_count BY count DESC ; Sauvegarder sur HDFS STORE ordered_word_count INTO './word_count_result' ; Apr\u00e8s l'envoi de cette instruction, Pig cr\u00e9e un job MapReduce. V\u00e9rifier la progression sur http://localhost:8088 Un dossier word_count_result est cr\u00e9\u00e9 o\u00f9 des fichiers part-r-???? contiennent les nombres d'occurences associ\u00e9s aux mots du fichier. Afficher le r\u00e9sultat hadoop fs -cat word_count_result/part-r-* | more Vous pouvez aussi utiliser l'interface web de HDFS pour afficher le r\u00e9sultat dans le dossier 'word_count_result'","title":"Premier exemple : Wordcount"},{"location":"pig.html#execution_dun_script","text":"Pour ex\u00e9cuter l'exemple pr\u00e9c\u00e9dent : Cr\u00e9er un fichier wordcount.pig avec un \u00e9diteur de texte (geany) avec le code suivant : lines = LOAD './shakespeare.txt' AS ( line: chararray ); words = FOREACH lines GENERATE FLATTEN ( TOKENIZE ( line )) AS word; word_groups = GROUP words BY word; word_count = FOREACH word_groups GENERATE COUNT ( words ) AS count , group ; ordered_word_count = ORDER word_count BY count DESC ; STORE ordered_word_count INTO './word_count_result' ; Ex\u00e9cuter le script avec la commande suivante pig -x mapreduce wordcount.pig Mode local Avant de lancer un script pig, il est recommand\u00e9 de le tester en local. Ceci est possible avec la commande suivante : pig -x local wordcount.pig Attention : dans ce mode d'ex\u00e9cution les chemins sont interpr\u00e9t\u00e9s dans le syst\u00e8me local et non HDFS.","title":"Ex\u00e9cution d'un script"},{"location":"pig.html#instructions_pig_par_lexemple","text":"Pr\u00e9parer les donn\u00e9es Cr\u00e9er un dossier demo : mkdir ~/demo T\u00e9l\u00e9charger les fichiers csv dans /home/uti/demo : employee.csv department.csv salary.csv Envoyer les donn\u00e9es sur HDFS hadoop fs -put /home/uti/demo Charger des donn\u00e9es sans sch\u00e9ma \u00e0 partir d'un fichier CSV avec ',' comme s\u00e9parateur. Puis limiter le nombre de tuples \u00e0 10 e1 = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ); e1_sample = limit e1 10 ; dump e1_sample; Charger avec un sch\u00e9ma emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( eid: chararray , fname: chararray , lname: chararray , department: chararray ); emp_sample = limit emp 10 ; dump emp_sample; Projection emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( eid: chararray , fname: chararray , lname: chararray , department: chararray ); emp_proj = FOREACH emp GENERATE $1, lname; emp_sample = limit emp_proj 10 ; dump emp_sample; Filtrage pour avoir les employ\u00e9s (sans doublons) ayant re\u00e7u un salaire compris entre 5000 et 7000 sal = LOAD '/user/uti/demo/salary.csv' USING PigStorage ( ',' ) as ( salary_id: chararray , employ_id: chararray , payment: double , p_date: datetime ); above_5k_7k = DISTINCT ( FILTER sal BY payment >= 5000 AND payment <= 7000 ); emp_sample = limit emp 10 ; dump emp_sample; Nombre d'employ\u00e9s par d\u00e9partement et afficher les 3 plus grands d\u00e9partements emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( eid: chararray , fname: chararray , lname: chararray , department: chararray ); emp_group_dep = group emp by department; -- ou emp_group_dep = group emp by $3; nbemp_dep = ORDER ( FOREACH emp_group_dep GENERATE group AS dep, COUNT ( eid ) AS nb_emp ) BY nb_emp DESC ; top_dep = limit nbemp_dep 3 ; dump top_dep; Jointure : trouver le nom du d\u00e9partement de chaque employ\u00e9 emp = LOAD '/user/uti/demo/employee.csv' USING PigStorage ( ',' ) as ( emp_id: chararray , fname: chararray , lname: chararray , dept_id: chararray ); dep = LOAD '/user/uti/demo/department.csv' USING PigStorage ( ',' ) as ( dept_id: chararray , dept_name: chararray ); dep_emp = LIMIT ( FOREACH ( JOIN emp by ( dept_id ) , dep by ( dept_id )) GENERATE fname, lname, dept_name ) 3 ; DUMP dep_emp;","title":"Instructions Pig par l'exemple"},{"location":"pig.html#test_et_performances","text":"DESCRIBE Elle permet d'afficher le sch\u00e9ma d'une relation. Exemple grunt> lines = LOAD './shakespeare.txt' AS ( line: chararray ); grunt> words = FOREACH lines GENERATE FLATTEN ( TOKENIZE ( line )) AS word; grunt> word_groups = GROUP words BY word; grunt> word_count = FOREACH word_groups GENERATE COUNT ( words ) AS count , group ; grunt> DESCRIBE lines; lines: { line: chararray } grunt> DECRIBE word_groups; word_groups: { group : chararray , words: { word: chararray }} grunt> DESCRIBEvword_count; word_count: { count : long , group : chararray } ILLUSTRATE Cette commande Pig permet de montrer les diff\u00e9rentes transformations effectu\u00e9es sur les donn\u00e9es. ILLUSTRATE relation|-script < nom_script> Exemple : (source http://pig.apache.org ) grunt> cat visits.txt Amy yahoo.com 19990421 Fred harvard.edu 19991104 Amy cnn.com 20070218 Frank nba.com 20070305 Fred berkeley.edu 20071204 Fred stanford.edu 20071206 grunt> cat visits.pig visits = LOAD 'visits.txt' AS ( user, url, timestamp ); recent_visits = FILTER visits BY timestamp >= '20071201' ; historical_visits = FILTER visits BY timestamp <= '20000101' ; DUMP recent_visits; DUMP historical_visits; STORE recent_visits INTO 'recent' ; STORE historical_visits INTO 'historical' ; grunt> exec visits.pig ( Fred,berkeley.edu,20071204 ) ( Fred,stanford.edu,20071206 ) ( Amy,yahoo.com,19990421 ) ( Fred,harvard.edu,19991104 ) grunt> illustrate - script visits.pig ------------------------------------------------------------------------ | visits | user: bytearray | url: bytearray | timestamp: bytearray | ------------------------------------------------------------------------ | | Amy | yahoo.com | 19990421 | | | Fred | stanford.edu | 20071206 | ------------------------------------------------------------------------ ------------------------------------------------------------------------------- | recent_visits | user: bytearray | url: bytearray | timestamp: bytearray | ------------------------------------------------------------------------------- | | Fred | stanford.edu | 20071206 | ------------------------------------------------------------------------------- --------------------------------------------------------------------------------------- | Store : recent_visits | user: bytearray | url: bytearray | timestamp: bytearray | --------------------------------------------------------------------------------------- | | Fred | stanford.edu | 20071206 | --------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- | historical_visits | user: bytearray | url: bytearray | timestamp: bytearray | ----------------------------------------------------------------------------------- | | Amy | yahoo.com | 19990421 | ----------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------- | Store : historical_visits | user: bytearray | url: bytearray | timestamp: bytearray | ------------------------------------------------------------------------------------------- | | Amy | yahoo.com | 19990421 | ------------------------------------------------------------------------------------------- EXPLAIN Affiche les plans d'ex\u00e9cution logique, physique et les phases MapReduce. EXPLAIN relation|-script < nom_script> [ - out chemin] [ - dot|-xml] Exemple grunt> EXPLAIN - script wordcount.pig - out explain_wordcount.txt Le fichier explain_wordcount.txt obtenu contient : #----------------------------------------------- # New Logical Plan: #----------------------------------------------- ordered_word_count: ( Name: LOStore Schema: count#112:long,group#109:chararray ) | | ---ordered_word_count: ( Name: LOSort Schema: count#112:long,group#109:chararray ) | | | count: ( Name: Project Type: long Uid: 112 Input: 0 Column: 0 ) | | ---word_count: ( Name: LOForEach Schema: count#112:long,group#109:chararray ) | | | ( Name: LOGenerate [ false,false ] Schema: count#112:long,group#109:chararray ) ColumnPrune:OutputUids =[ 112 , 109 ] ColumnPrune:InputUids =[ 109 , 110 ] | | | | | ( Name: UserFunc ( org.apache.pig.builtin.COUNT ) Type: long Uid: 112 ) | | | | | | ---words: ( Name: Project Type: bag Uid: 110 Input: 0 Column: ( * )) | | | | | group: ( Name: Project Type: chararray Uid: 109 Input: 1 Column: ( * )) | | | | ---words: ( Name: LOInnerLoad [ 1 ] Schema: word#109:chararray ) | | | | --- ( Name: LOInnerLoad [ 0 ] Schema: group#109:chararray ) | | ---word_groups: ( Name: LOCogroup Schema: group#109:chararray,words#110:bag { #114:tuple(word#109:chararray)}) | | | word: ( Name: Project Type: chararray Uid: 109 Input: 0 Column: 0 ) | | ---words: ( Name: LOForEach Schema: word#118:chararray ) | | | ( Name: LOGenerate [ true ] Schema: word#118:chararray ) | | | | | ( Name: UserFunc ( org.apache.pig.builtin.TOKENIZE ) Type: bag Uid: 116 ) | | | | | | --- ( Name: Cast Type: chararray Uid: 98 ) | | | | | | ---line: ( Name: Project Type: bytearray Uid: 98 Input: 0 Column: ( * )) | | | | --- ( Name: LOInnerLoad [ 0 ] Schema: line#98:bytearray ) | | ---lines: ( Name: LOLoad Schema: line#98:bytearray ) RequiredFields:null #----------------------------------------------- # Physical Plan: #----------------------------------------------- ordered_word_count: Store ( hdfs://master.cluster.virt:8020/user/uti/word_count_result:org.apache.pig.builtin.PigStorage ) - scope-18 | | ---ordered_word_count: POSort [ bag ]() - scope-17 | | | Project [ long ][ 0 ] - scope-16 | | ---word_count: New For Each ( false,false )[ bag ] - scope-15 | | | POUserFunc ( org.apache.pig.builtin.COUNT )[ long ] - scope-11 | | | | ---Project [ bag ][ 1 ] - scope-10 | | | Project [ chararray ][ 0 ] - scope-13 | | ---word_groups: Package ( Packager )[ tuple ]{ chararray } - scope-7 | | ---word_groups: Global Rearrange [ tuple ] - scope-6 | | ---word_groups: Local Rearrange [ tuple ]{ chararray }( false ) - scope-8 | | | Project [ chararray ][ 0 ] - scope-9 | | ---words: New For Each ( true )[ bag ] - scope-5 | | | POUserFunc ( org.apache.pig.builtin.TOKENIZE )[ bag ] - scope-3 | | | | ---Cast [ chararray ] - scope-2 | | | | ---Project [ bytearray ][ 0 ] - scope-1 | | ---lines: Load ( hdfs://master.cluster.virt:8020/user/uti/shakespeare.txt:org.apache.pig.builtin.PigStorage ) - scope-0 #-------------------------------------------------- # Map Reduce Plan #-------------------------------------------------- MapReduce node scope-19 Map Plan word_groups: Local Rearrange [ tuple ]{ chararray }( false ) - scope-53 | | | Project [ chararray ][ 0 ] - scope-55 | | ---word_count: New For Each ( false,false )[ bag ] - scope-42 | | | Project [ chararray ][ 0 ] - scope-43 | | | POUserFunc ( org.apache.pig.builtin.COUNT $Initial )[ tuple ] - scope-44 | | | | ---Project [ bag ][ 1 ] - scope-45 | | ---Pre Combiner Local Rearrange [ tuple ]{ Unknown } - scope-56 | | ---words: New For Each ( true )[ bag ] - scope-5 | | | POUserFunc ( org.apache.pig.builtin.TOKENIZE )[ bag ] - scope-3 | | | | ---Cast [ chararray ] - scope-2 | | | | ---Project [ bytearray ][ 0 ] - scope-1 | | ---lines: Load ( hdfs://master.cluster.virt:8020/user/uti/shakespeare.txt:org.apache.pig.builtin.PigStorage ) - scope-0-------- Combine Plan word_groups: Local Rearrange [ tuple ]{ chararray }( false ) - scope-57 | | | Project [ chararray ][ 0 ] - scope-59 | | ---word_count: New For Each ( false,false )[ bag ] - scope-46 | | | Project [ chararray ][ 0 ] - scope-47 | | | POUserFunc ( org.apache.pig.builtin.COUNT $Intermediate )[ tuple ] - scope-48 | | | | ---Project [ bag ][ 1 ] - scope-49 | | ---word_groups: Package ( CombinerPackager )[ tuple ]{ chararray } - scope-52-------- Reduce Plan Store ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.io.InterStorage ) - scope-20 | | ---word_count: New For Each ( false,false )[ bag ] - scope-15 | | | POUserFunc ( org.apache.pig.builtin.COUNT $Final )[ long ] - scope-11 | | | | ---Project [ bag ][ 1 ] - scope-50 | | | Project [ chararray ][ 0 ] - scope-13 | | ---word_groups: Package ( CombinerPackager )[ tuple ]{ chararray } - scope-7-------- Global sort: false ---------------- MapReduce node scope-22 Map Plan ordered_word_count: Local Rearrange [ tuple ]{ tuple }( false ) - scope-26 | | | Constant ( all ) - scope-25 | | ---New For Each ( false )[ tuple ] - scope-24 | | | Project [ long ][ 0 ] - scope-23 | | ---Load ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.builtin.RandomSampleLoader ( 'org.apache.pig.impl.io.InterStorage' , '100' )) - scope-21-------- Reduce Plan Store ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp569443691:org.apache.pig.impl.io.InterStorage ) - scope-35 | | ---New For Each ( false )[ tuple ] - scope-34 | | | POUserFunc ( org.apache.pig.impl.builtin.FindQuantiles )[ tuple ] - scope-33 | | | | ---Project [ tuple ][ * ] - scope-32 | | ---New For Each ( false,false )[ tuple ] - scope-31 | | | Constant ( -1 ) - scope-30 | | | Project [ bag ][ 1 ] - scope-28 | | ---Package ( Packager )[ tuple ]{ chararray } - scope-27-------- Global sort: false Secondary sort: true ---------------- MapReduce node scope-37 Map Plan ordered_word_count: Local Rearrange [ tuple ]{ long }( false ) - scope-38 | | | Project [ long ][ 0 ] - scope-16 | | ---Load ( hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp-1292167423:org.apache.pig.impl.io.InterStorage ) - scope-36-------- Reduce Plan ordered_word_count: Store ( hdfs://master.cluster.virt:8020/user/uti/word_count_result:org.apache.pig.builtin.PigStorage ) - scope-18 | | ---New For Each ( true )[ tuple ] - scope-41 | | | Project [ bag ][ 1 ] - scope-40 | | ---Package ( LitePackager )[ tuple ]{ long } - scope-39-------- Global sort: true Quantile file: hdfs://master.cluster.virt:8020/tmp/temp-483208883/tmp569443691 ---------------- Pour avoir un graphe d'ex\u00e9cution : grunt> EXPLAIN - script wordcount.pig - out explain_wordcount.dot - dot Puis convertir le fichier .dot en png : apt install graphviz -y dot -Tpng -out explain_wordcount.png explain_wordcount.dot Ce qui donne :","title":"Test et performances"},{"location":"pig.html#references_pig_latin","text":"Pour une r\u00e9f\u00e9rence compl\u00e8te du langage Pig Latin, aller sur la page de documentation ( ici ) Voici aussi, un m\u00e9mo du langage : Ouvrir dans un nouvel onglet","title":"R\u00e9f\u00e9rences Pig Latin"},{"location":"pig.html#exercice","text":"R\u00e9soudre les questions 2 et 3 des exercices de la section MapReduce.","title":"Exercice"},{"location":"ucase.html","text":"Cas Pratique : Data Pipeline avec Pig et Hive \u00b6 Introduction \u00b6 Les donn\u00e9es Clickstream constitue l'ensembles des informations qu'un utilisateur laisse lors de la visite d'un site web. Ces donn\u00e9es sont captur\u00e9es g\u00e9n\u00e9ralement par le serveur web et enregistr\u00e9es dans des fichiers semi-structur\u00e9s appel\u00e9s logs . Ces logs contiennent des informations comme l'adresse IP, le navigateur web, la date, le syst\u00e8me d'exploitation, etc. Le travail consiste \u00e0 automatiser les t\u00e2che d'ingestion, de nettoyage, de transformation des donn\u00e9es pour g\u00e9n\u00e9rer des donn\u00e9es exploitables dans la phase d'analyse. Sources de donn\u00e9es et Pipeline \u00b6 Le pipeline souhait\u00e9 est repr\u00e9sent\u00e9 par la figure suivante : Les fichiers sources sont regroup\u00e9s dans cette archive : products.tsv : Format TSV avec ent\u00eate et 2 colonnes (url, category) users.tsv : Format TSV avec ent\u00eate et 3 colonnes (SWID, BIRTH_DT, GENDER_CD) log.tsv : Format TSV sans ent\u00eate et 179 colonnes load_files.pig T\u00e9l\u00e9charger le script load_files.pig qui permet de parser ces fichier. Travail demand\u00e9 \u00b6 Ajouter les transformations n\u00e9cessaires avec Pig pour obtenir la structure suivante : (user_id, age, gender, country, state, city, logdate, ip, product_category, url) R\u00e9pondre aux questions : Le top 5 des produits visit\u00e9s par coordonn\u00e9es g\u00e9ographiques de l'utilisateur Le nombre de sessions par coordonn\u00e9es g\u00e9ographiques Le nombre de sessions par genre, produit et coordonn\u00e9es g\u00e9ographiques","title":"Cas Pratique"},{"location":"ucase.html#cas_pratique_data_pipeline_avec_pig_et_hive","text":"","title":"Cas Pratique : Data Pipeline avec Pig et Hive"},{"location":"ucase.html#introduction","text":"Les donn\u00e9es Clickstream constitue l'ensembles des informations qu'un utilisateur laisse lors de la visite d'un site web. Ces donn\u00e9es sont captur\u00e9es g\u00e9n\u00e9ralement par le serveur web et enregistr\u00e9es dans des fichiers semi-structur\u00e9s appel\u00e9s logs . Ces logs contiennent des informations comme l'adresse IP, le navigateur web, la date, le syst\u00e8me d'exploitation, etc. Le travail consiste \u00e0 automatiser les t\u00e2che d'ingestion, de nettoyage, de transformation des donn\u00e9es pour g\u00e9n\u00e9rer des donn\u00e9es exploitables dans la phase d'analyse.","title":"Introduction"},{"location":"ucase.html#sources_de_donnees_et_pipeline","text":"Le pipeline souhait\u00e9 est repr\u00e9sent\u00e9 par la figure suivante : Les fichiers sources sont regroup\u00e9s dans cette archive : products.tsv : Format TSV avec ent\u00eate et 2 colonnes (url, category) users.tsv : Format TSV avec ent\u00eate et 3 colonnes (SWID, BIRTH_DT, GENDER_CD) log.tsv : Format TSV sans ent\u00eate et 179 colonnes load_files.pig T\u00e9l\u00e9charger le script load_files.pig qui permet de parser ces fichier.","title":"Sources de donn\u00e9es et Pipeline"},{"location":"ucase.html#travail_demande","text":"Ajouter les transformations n\u00e9cessaires avec Pig pour obtenir la structure suivante : (user_id, age, gender, country, state, city, logdate, ip, product_category, url) R\u00e9pondre aux questions : Le top 5 des produits visit\u00e9s par coordonn\u00e9es g\u00e9ographiques de l'utilisateur Le nombre de sessions par coordonn\u00e9es g\u00e9ographiques Le nombre de sessions par genre, produit et coordonn\u00e9es g\u00e9ographiques","title":"Travail demand\u00e9"}]}